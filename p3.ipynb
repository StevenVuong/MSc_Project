{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/p3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcuiN5_8q_e4",
        "colab_type": "text"
      },
      "source": [
        "**This notebook aims to bring together the df and loaded files to implement them in a Deep Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRYoomKprOlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pydicom\n",
        "import pickle\n",
        "from deepbrain import Extractor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nibabel as nb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGUaJQYz0hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from keras.layers import LeakyReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcp0O27gq7rM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "809a9f49-4f4c-4274-9fc7-239b7703280d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where the data is\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNO8nIA0rr_J",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataframe and Pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpCBM2asrNEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_df = pd.read_pickle('processed_patient_df.pkl')\n",
        "\n",
        "with open('total_slices_all.pkl', 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USadebeguPrR",
        "colab_type": "text"
      },
      "source": [
        "**Split data into training and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIcEIgKcuU1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2c91174-5a3a-49ad-a9e9-5a61b43368b9"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  # convert to 'keras friendly outputs'\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values\n",
        "\n",
        "y_values = get_y_values(total_slices_info)\n",
        "\n",
        "# print distribution of the array\n",
        "num_controls = np.unique(y_values, return_counts=True)[1][0]\n",
        "num_pd = np.unique(y_values, return_counts=True)[1][1]\n",
        "\n",
        "pct_controls = (num_controls/ np.sum([num_controls, num_pd])) * 100\n",
        "pct_pd = (num_pd / np.sum([num_controls, num_pd])) * 100\n",
        "print (\"%s%% Control and %s%% PD\" % (pct_controls, pct_pd))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.0% Control and 50.0% PD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auG6d1LMxD-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46ff2f5c-4cdf-4f98-b21f-485d45e16738"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(total_slices, y_values, test_size=0.2, shuffle=True)\n",
        "\n",
        "np.shape(X_train)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 200, 200, 160, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1wiMiOZz3CM",
        "colab_type": "text"
      },
      "source": [
        "**Model Architecture**\n",
        "Could potentially split data into chunks of 50 post processing (randomly sample 50 at a time and save that for our data set), train model with one epoch per chunk and save in batches, save, continue etc.. Then once that is done then validate on one chunk that is left out. Saving different parameters in our deep learning framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWY7sk9HztXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution3D(filters=8, kernel_size=2, strides=1, padding='same', input_shape=(200,200,160,1))) # or should activation be linear?\n",
        "model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Convolution3D(filters=16, kernel_size=2, strides=1, padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.01)) \n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Convolution3D(filters=32, kernel_size=3, strides=1, padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Convolution3D(filters=64, kernel_size=3, strides=1, padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Convolution3D(filters=128, kernel_size=4, strides=1, padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Convolution3D(filters=256, kernel_size=4, strides=1, padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVedtkSg1i4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "48a6b556-9bdb-49a0-e9d6-b4aefae50691"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_7 (Conv3D)            (None, 200, 200, 160, 8)  72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 200, 200, 160, 8)  0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3 (None, 100, 100, 80, 8)   0         \n",
            "_________________________________________________________________\n",
            "conv3d_8 (Conv3D)            (None, 100, 100, 80, 16)  1040      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 100, 100, 80, 16)  0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3 (None, 50, 50, 40, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_9 (Conv3D)            (None, 50, 50, 40, 32)    13856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 50, 50, 40, 32)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3 (None, 25, 25, 20, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_10 (Conv3D)           (None, 25, 25, 20, 64)    55360     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 25, 25, 20, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling (None, 12, 12, 10, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_11 (Conv3D)           (None, 12, 12, 10, 128)   524416    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 12, 12, 10, 128)   0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_11 (MaxPooling (None, 6, 6, 5, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_12 (Conv3D)           (None, 6, 6, 5, 256)      2097408   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 6, 6, 5, 256)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_12 (MaxPooling (None, 3, 3, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               2359808   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 5,052,986\n",
            "Trainable params: 5,052,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jxY5Wi_KYm4",
        "colab_type": "text"
      },
      "source": [
        "**RUNNING THE MODEL!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rCFD8VS1kMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/MinhazPalasara/keras/blob/master/examples/shapes_3d_cnn.py\n",
        "model.fit(x=X_train, y=y_train, batch_size=1, epochs=3, verbose=1,\n",
        "          validation_data=(X_test, y_test), shuffle=True)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mq8dH7INDFB",
        "colab_type": "text"
      },
      "source": [
        "**Put into one super class which we can load our data (overwrite), fit and continue**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlFkbFuKcER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "num_epochs = 3\n",
        "batch_size = 10\n",
        "\n",
        "batches_path = os.getcwd() + '/stored_batches'\n",
        "\n",
        "for batch_pkl in os.listdir(batches_path):\n",
        "  batch_pkl_path = batches_path + '/' + batch_pkl\n",
        "  \n",
        "  # load up the batch\n",
        "  with open(batch_pkl_path, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)\n",
        "    \n",
        "  # get the corresponding y-values\n",
        "  y_values = get_y_values(total_slices_info)\n",
        "  \n",
        "  # split into training and test set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(total_slices, y_values, test_size=0.2, shuffle=True)\n",
        "  \n",
        "  # fit to our existing model\n",
        "  model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, # can modify batch size and epochs\n",
        "          validation_data=(X_test, y_test), shuffle=True)\n",
        "\n",
        "# creates a file and saves it (architecture, weights, optimizer)\n",
        "model.save('my_model.h5', overwrite=True)  # creates a HDF5 file 'my_model.h5'\n",
        "# model = load_model('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAjNAKjuOIp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/p3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcuiN5_8q_e4",
        "colab_type": "text"
      },
      "source": [
        "**This notebook aims to bring together the df and loaded files to implement them in a Deep Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM87-rK6n03g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install talos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRYoomKprOlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nibabel as nb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGUaJQYz0hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports for keras model\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "# imports for automated parameter tuning\n",
        "import talos as ta\n",
        "from talos.metrics.keras_metrics import fmeasure_acc\n",
        "from talos import live"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcp0O27gq7rM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6801cf13-b34b-41c3-ecd7-d95da43d618b"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where the data is\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNO8nIA0rr_J",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataframe and Pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpCBM2asrNEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_df = pd.read_pickle('processed_patient_df.pkl')\n",
        "\n",
        "with open('total_slices_all.pkl', 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USadebeguPrR",
        "colab_type": "text"
      },
      "source": [
        "**Split data into training and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIcEIgKcuU1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e68ebc6-24ff-4f79-ea9b-6571b71102a7"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  # convert to 'keras friendly outputs'\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values\n",
        "\n",
        "y_values = get_y_values(total_slices_info)\n",
        "\n",
        "# print distribution of the array\n",
        "num_controls = np.unique(y_values, return_counts=True)[1][0]\n",
        "num_pd = np.unique(y_values, return_counts=True)[1][1]\n",
        "\n",
        "pct_controls = (num_controls/ np.sum([num_controls, num_pd])) * 100\n",
        "pct_pd = (num_pd / np.sum([num_controls, num_pd])) * 100\n",
        "print (\"%s%% Control and %s%% PD\" % (pct_controls, pct_pd))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.0% Control and 50.0% PD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auG6d1LMxD-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15a5273d-3dce-4d05-9dcd-ca7eb3e66047"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(total_slices, y_values, test_size=0.2, shuffle=True)\n",
        "\n",
        "np.shape(y_test) # how many y-outputs"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1wiMiOZz3CM",
        "colab_type": "text"
      },
      "source": [
        "**Model Architecture**\n",
        "Could potentially split data into chunks of 50 post processing (randomly sample 50 at a time and save that for our data set), train model with one epoch per chunk and save in batches, save, continue etc.. Then once that is done then validate on one chunk that is left out. Saving different parameters in our deep learning framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZ2ihEFhjkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile our model\n",
        "# change architecture / layers /parameters, increase filters then decrease again\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution3D(filters=8, kernel_size=2, padding='same', input_shape=(200,200,160,1))) # or should activation be linear?\n",
        "model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "model.add(MaxPooling3D(pool_size=2)) # pool_size=2\n",
        "\n",
        "model.add(Convolution3D(filters=16, kernel_size=2))\n",
        "model.add(LeakyReLU(alpha=0.01)) \n",
        "model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "model.add(Convolution3D(filters=32, kernel_size=3))\n",
        "model.add(LeakyReLU())\n",
        "model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "model.add(Convolution3D(filters=64, kernel_size=3))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "model.add(Convolution3D(filters=128, kernel_size=2))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "model.add(Convolution3D(filters=256, kernel_size=2))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVedtkSg1i4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "4ae7e98d-053d-445f-8edc-22dd04d15fc2"
      },
      "source": [
        "# investigate what possible 'good' combinations are first..\n",
        "model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_73 (Conv3D)           (None, 200, 200, 160, 8)  72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)   (None, 200, 200, 160, 8)  0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_70 (MaxPooling (None, 100, 100, 80, 8)   0         \n",
            "_________________________________________________________________\n",
            "conv3d_74 (Conv3D)           (None, 99, 99, 79, 16)    1040      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_77 (LeakyReLU)   (None, 99, 99, 79, 16)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_71 (MaxPooling (None, 49, 49, 39, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_75 (Conv3D)           (None, 47, 47, 37, 32)    13856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_78 (LeakyReLU)   (None, 47, 47, 37, 32)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_72 (MaxPooling (None, 23, 23, 18, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_76 (Conv3D)           (None, 21, 21, 16, 64)    55360     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_79 (LeakyReLU)   (None, 21, 21, 16, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_73 (MaxPooling (None, 10, 10, 8, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_77 (Conv3D)           (None, 9, 9, 7, 128)      65664     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_80 (LeakyReLU)   (None, 9, 9, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_74 (MaxPooling (None, 4, 4, 3, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_78 (Conv3D)           (None, 3, 3, 2, 256)      262400    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_81 (LeakyReLU)   (None, 3, 3, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_75 (MaxPooling (None, 1, 1, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_82 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 531,002\n",
            "Trainable params: 531,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jxY5Wi_KYm4",
        "colab_type": "text"
      },
      "source": [
        "**RUNNING THE MODEL!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rCFD8VS1kMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "f23bdc17-0d93-40d7-c5b0-435d03315d45"
      },
      "source": [
        "model.fit(x=X_train, y=y_train, batch_size=5, epochs=3, verbose=1,\n",
        "          validation_split=0.2, shuffle=True)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 64 samples, validate on 17 samples\n",
            "Epoch 1/3\n",
            "64/64 [==============================] - 15s 231ms/step - loss: 0.2362 - acc: 0.9063 - val_loss: 0.1131 - val_acc: 1.0000\n",
            "Epoch 2/3\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0904 - acc: 1.0000 - val_loss: 0.1627 - val_acc: 0.9412\n",
            "Epoch 3/3\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0784 - acc: 0.9844 - val_loss: 0.0873 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f98a07cfbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyqKXWl3NCRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7X-HuBJ3Vs7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "436ac8ad-ec4a-4fd4-c9ea-55030aca1646"
      },
      "source": [
        "score = model.evaluate(X_test[:20], y_test[:20], verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r20/20 [==============================] - 2s 93ms/step\n",
            "Test score: 0.10864386707544327\n",
            "Test accuracy: 0.949999988079071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0b-HANx4nei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model.h5', overwrite=True)  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axei2GR6Ixew",
        "colab_type": "text"
      },
      "source": [
        "Next Steps:\n",
        "-  Add age and gender to FC layers\n",
        "-  Add Normalisation\n",
        "-  Add Bias and Kernel Regularisation (coefficient 0.001)\n",
        "-  Add Dropout layers to last two layers (Keep coefficient of 0.45 and 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mq8dH7INDFB",
        "colab_type": "text"
      },
      "source": [
        "**Put into one super class which we can load our data (overwrite), fit and continue**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlFkbFuKcER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "num_epochs = 3\n",
        "batch_size = 10\n",
        "\n",
        "batches_path = os.getcwd() + '/stored_batches'\n",
        "\n",
        "for batch_pkl in os.listdir(batches_path):\n",
        "  batch_pkl_path = batches_path + '/' + batch_pkl\n",
        "  \n",
        "  # load up the batch\n",
        "  with open(batch_pkl_path, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)\n",
        "    \n",
        "  # get the corresponding y-values\n",
        "  y_values = get_y_values(total_slices_info)\n",
        "  \n",
        "  # split into training and test set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(total_slices, y_values, test_size=0.2, shuffle=True)\n",
        "  \n",
        "  # fit to our existing model\n",
        "  model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, # can modify batch size and epochs\n",
        "          validation_data=(X_test, y_test), shuffle=True)\n",
        "\n",
        "# creates a file and saves it (architecture, weights, optimizer)\n",
        "model.save('my_model.h5', overwrite=True)  # creates a HDF5 file 'my_model.h5'\n",
        "# model = load_model('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nusjRQF8homO",
        "colab_type": "text"
      },
      "source": [
        "**Attempt at auto tuning the parameters to our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elSn7SqseJeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def many_layers_architecture(x_train, y_train, x_val, y_val, params):\n",
        "  '''This model has many layers of smaller kernel sizes which we can adjust'''\n",
        "  \n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c1_filter'], kernel_size=params['k1_size']\n",
        "                          , padding='same', input_shape=(200,200,160,1)))\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c2_filter'], kernel_size=params['k1_size']))\n",
        "  model.add(LeakyReLU(alpha=0.01)) \n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c3_filter'], kernel_size=params['k2_size']))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c4_filter'], kernel_size=params['k2_size']))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c5_filter'], kernel_size=params['k3_size']))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Convolution3D(filters=params['c6_filter'], kernel_size=params['k3_size']))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=1))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']\n",
        "  \n",
        "  # https://github.com/MinhazPalasara/keras/blob/master/examples/shapes_3d_cnn.py\n",
        "  history = model.fit(x=X_train, y=y_train, batch_size=10, epochs=10, verbose=1,\n",
        "          validation_data=(X_test, y_test), shuffle=True)\n",
        "  \n",
        "  return history, model\n",
        "\n",
        "p = {'c1_filter':[4,8],\n",
        "    'c2_filter':[6,16],\n",
        "    'c3_filter':[16,32],\n",
        "    'c4_filter':[32,64],\n",
        "    'c5_filter':[64,128],\n",
        "    'c6_filter':[128,256],\n",
        "    'd1_filter':[512,1024],\n",
        "    'k1_size':[2,3],\n",
        "    'k2_size':[3,4],\n",
        "    'k3_size':[2]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWY7sk9HztXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "1a406f42-b736-48c9-f4f6-3fc086574990"
      },
      "source": [
        "# https://github.com/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20on%20Keras%20with%20Breast%20Cancer%20Data.ipynb\n",
        "def breast_cancer_model(x_train, y_train, x_val, y_val, params):\n",
        "    '''This model has many layers of smaller kernel sizes which we can adjust'''\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
        "                    activation=params['activation'],\n",
        "                    kernel_initializer=params['kernel_initializer']))\n",
        "    \n",
        "    model.add(Dropout(params['dropout']))\n",
        "\n",
        "    model.add(Dense(1, activation=params['last_activation'],\n",
        "                    kernel_initializer=params['kernel_initializer']))\n",
        "    \n",
        "    model.compile(loss=params['losses'],\n",
        "                  optimizer=params['optimizer'](),\n",
        "                  metrics=['acc', fmeasure_acc])\n",
        "    \n",
        "    history = model.fit(x_train, y_train, \n",
        "                        validation_data=[x_val, y_val],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        callbacks=[live()],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0)\n",
        "\n",
        "    return history, model\n",
        "  \n",
        "# then we can go ahead and set the parameter space\n",
        "p = {'first_neuron':[9,10,11],\n",
        "     'hidden_layers':[0, 1, 2],\n",
        "     'batch_size': [30],\n",
        "     'epochs': [100],\n",
        "     'dropout': [0],\n",
        "     'kernel_initializer': ['uniform','normal'],\n",
        "     'optimizer': [Nadam, Adam],\n",
        "     'losses': [binary_crossentropy],\n",
        "     'activation':[relu, elu],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-56c24962b7e5>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    model.add(MaxPooling3D(pool_size=2, strides=))\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAjNAKjuOIp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
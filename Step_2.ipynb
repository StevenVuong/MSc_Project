{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/Step_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyHDTsckJza",
        "colab_type": "text"
      },
      "source": [
        "**Processing should be done in this notebook, training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoF1f31WkApY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad33e344-8115-457c-cbbe-71ef5e697736"
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# other imports to handle files\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dn-QWq5kHsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "463c0674-fb04-4a27-b80f-3ba79c15dde2"
      },
      "source": [
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where we will be working\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project/all_mprage_grappa')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rgruIswlvMv",
        "colab_type": "text"
      },
      "source": [
        "**Load our batches and build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gATeg_-pkWlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets our y-values and converts to keras, one hot encoded outputs\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8Pgw98mWgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_model():\n",
        "\n",
        "  # compile our model\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Convolution3D(filters=16, kernel_size=3, padding='same', input_shape=(160,160,160,1), strides=1))# ,kernel_regularizer=regularizers.l2(0.0005))) # padding on first one only?\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  model.add(BatchNormalization()) \n",
        "  # could user he_norm kernel initializer?\n",
        "  model.add(MaxPooling3D(pool_size=2, strides=2)) # pool_size=2\n",
        "\n",
        "  model.add(Convolution3D(filters=32, kernel_size=3, padding='same', strides=1))# ,kernel_regularizer=regularizers.l2(0.0005))) # padding on first one only?\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  model.add(BatchNormalization()) \n",
        "  # could user he_norm kernel initializer?\n",
        "  model.add(MaxPooling3D(pool_size=3, strides=2)) # pool_size=2\n",
        "  \n",
        "  model.add(Convolution3D(filters=64, kernel_size=3, strides=1, padding='same')) #,kernel_regularizer=regularizers.l2(0.0005)))\n",
        "  model.add(LeakyReLU(alpha=0.01)) \n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=3, strides=2))\n",
        "\n",
        "  model.add(Convolution3D(filters=128, kernel_size=3, strides=1, padding='same'))# ,kernel_regularizer=regularizers.l2(0.0005)))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=3, strides=2))\n",
        "  \n",
        "  model.add(Convolution3D(filters=256, kernel_size=3, strides=1, padding='same'))# ,kernel_regularizer=regularizers.l2(0.0005)))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=3, strides=2))\n",
        " \n",
        "  model.add(Convolution3D(filters=512, kernel_size=3, strides=1, padding='same'))# ,kernel_regularizer=regularizers.l2(0.0005)))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=3, strides=2))\n",
        "  \n",
        "  model.add(Dropout(0.30))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "   \n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Dropout(0.35)) # add dropout to prevent overfitting\n",
        "\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['categorical_accuracy']) # metrics=['categorical_accuracy']\n",
        "\n",
        "  # experiment with literally everything?... Random Search with optimisers\n",
        "            \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xVngjzAmZ1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        },
        "outputId": "3b30bbd6-e32e-4fd7-f89d-3b72d7724a70"
      },
      "source": [
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_13 (Conv3D)           (None, 160, 160, 160, 16) 448       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 160, 160, 160, 16) 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 160, 160, 160, 16) 64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_13 (MaxPooling (None, 80, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_14 (Conv3D)           (None, 80, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 80, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 80, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_14 (MaxPooling (None, 39, 39, 39, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_15 (Conv3D)           (None, 39, 39, 39, 64)    55360     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 39, 39, 39, 64)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 39, 39, 39, 64)    256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_15 (MaxPooling (None, 19, 19, 19, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_16 (Conv3D)           (None, 19, 19, 19, 128)   221312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 19, 19, 19, 128)   0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 19, 19, 19, 128)   512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_16 (MaxPooling (None, 9, 9, 9, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_17 (Conv3D)           (None, 9, 9, 9, 256)      884992    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 9, 9, 9, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 9, 9, 9, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_17 (MaxPooling (None, 4, 4, 4, 256)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 4, 4, 4, 512)      3539456   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 4, 4, 4, 512)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 4, 4, 4, 512)      2048      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_18 (MaxPooling (None, 1, 1, 1, 512)      0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1, 1, 1, 512)      0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 4,983,138\n",
            "Trainable params: 4,981,122\n",
            "Non-trainable params: 2,016\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6xTykx-mdMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(hist):\n",
        "  ''' Function to get our metrics from history and score as inputs'''\n",
        "\n",
        "  # actually obtain our metrics\n",
        "  val_loss = hist.history['val_loss'][0]\n",
        "  val_acc = hist.history['val_categorical_accuracy'][0]\n",
        "  train_loss = hist.history['loss'][0]\n",
        "  train_acc = hist.history['categorical_accuracy'][0]\n",
        "\n",
        "  # put everything into one array\n",
        "  return [val_loss, val_acc, train_loss, train_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxngidoAmawg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6065
        },
        "outputId": "d6c7d4bc-c1b8-43a5-a210-1f8db871f3a0"
      },
      "source": [
        "ppath = 'processed_brains_aug' # set this early on\n",
        "nname = '18_aug_v'\n",
        "# train on all, perhaps have different conditions for when we reach our last one\n",
        "total_slices_train = os.listdir(ppath)[:3] # train on first 3, test on last one\n",
        "\n",
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "\n",
        "# Load Model (If not this, then must initialise)\n",
        "# model = load_model('my_model_1.h5')\n",
        "\n",
        "num_loopz = 1 # number of repeats we want\n",
        "num_iterations = 30 # number of times we want to loop it\n",
        "\n",
        "for kk in range(num_loopz):\n",
        "  \n",
        "  file_name = (nname+str(kk)) ###filename, what we are changing\n",
        "  for iteration in range(num_iterations):\n",
        "\n",
        "    # load and fit our model for our instances\n",
        "    for tsf in total_slices_train:\n",
        "      pkl_path = ppath+'/'+tsf\n",
        "\n",
        "      # load pickle file\n",
        "      with open(pkl_path, 'rb') as f:\n",
        "        total_slices, total_slices_info = pickle.load(f)\n",
        "\n",
        "      # convert to numpy array\n",
        "      total_slices = np.array(total_slices)\n",
        "      # process y-values\n",
        "      y_values = get_y_values(total_slices_info)\n",
        "\n",
        "      # Run our model with validation of 15%\n",
        "      hist = model.fit(x=total_slices, y=y_values, batch_size=1, epochs=1, verbose=1, shuffle=True,validation_split=0.15,)\n",
        "      # get metrics\n",
        "      metrics = get_metrics(hist)\n",
        "\n",
        "      # write to csv (want to append instead of overwrite)\n",
        "      with open('stored_metrics/metrics'+file_name+'.csv', 'a') as csvFile:\n",
        "          writer = csv.writer(csvFile)\n",
        "          writer.writerow(metrics)\n",
        "      csvFile.close()\n",
        "\n",
        "      print (\"Iteration: %d, batch %s\" % (iteration, tsf[-5]))\n",
        "\n",
        "  # save the model as reference, incase we need the brain heatmap\n",
        "  model.save('stored_models/model'+file_name+'.h5', overwrite=True)  # saves as a hd5 file"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 19s 221ms/step - loss: 1.5212 - categorical_accuracy: 0.6235 - val_loss: 0.9574 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 0, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 201ms/step - loss: 1.7712 - categorical_accuracy: 0.5176 - val_loss: 1.0692 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 0, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 203ms/step - loss: 1.6262 - categorical_accuracy: 0.5647 - val_loss: 0.9890 - val_categorical_accuracy: 0.2667\n",
            "Iteration: 0, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 18s 207ms/step - loss: 1.2876 - categorical_accuracy: 0.6353 - val_loss: 0.6598 - val_categorical_accuracy: 0.8667\n",
            "Iteration: 1, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 18s 209ms/step - loss: 0.7741 - categorical_accuracy: 0.7294 - val_loss: 0.9998 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 1, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 18s 206ms/step - loss: 1.2198 - categorical_accuracy: 0.5529 - val_loss: 1.8931 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 1, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.1343 - categorical_accuracy: 0.5294 - val_loss: 0.4955 - val_categorical_accuracy: 0.8667\n",
            "Iteration: 2, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.7742 - categorical_accuracy: 0.7059 - val_loss: 1.4576 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 2, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.1087 - categorical_accuracy: 0.5647 - val_loss: 1.4319 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 2, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.1457 - categorical_accuracy: 0.6471 - val_loss: 0.7114 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 3, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.8055 - categorical_accuracy: 0.6706 - val_loss: 1.1573 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 3, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.0270 - categorical_accuracy: 0.6118 - val_loss: 1.0904 - val_categorical_accuracy: 0.3333\n",
            "Iteration: 3, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.8624 - categorical_accuracy: 0.6941 - val_loss: 0.6213 - val_categorical_accuracy: 0.6000\n",
            "Iteration: 4, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.6277 - categorical_accuracy: 0.7412 - val_loss: 1.0076 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 4, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.7986 - categorical_accuracy: 0.6588 - val_loss: 2.0726 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 4, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.9079 - categorical_accuracy: 0.6235 - val_loss: 0.7220 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 5, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.5254 - categorical_accuracy: 0.7529 - val_loss: 1.2237 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 5, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.9692 - categorical_accuracy: 0.6235 - val_loss: 1.7033 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 5, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.9756 - categorical_accuracy: 0.5765 - val_loss: 0.7893 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 6, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.5359 - categorical_accuracy: 0.8118 - val_loss: 1.2993 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 6, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.9349 - categorical_accuracy: 0.6118 - val_loss: 1.8472 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 6, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.6653 - categorical_accuracy: 0.7412 - val_loss: 0.6632 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 7, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.5396 - categorical_accuracy: 0.7765 - val_loss: 1.1032 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 7, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.7284 - categorical_accuracy: 0.6471 - val_loss: 1.3918 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 7, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.5312 - categorical_accuracy: 0.7412 - val_loss: 0.6253 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 8, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.3256 - categorical_accuracy: 0.8824 - val_loss: 1.5670 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 8, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.6194 - categorical_accuracy: 0.7529 - val_loss: 1.1101 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 8, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.4544 - categorical_accuracy: 0.8000 - val_loss: 1.0668 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 9, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.3182 - categorical_accuracy: 0.8471 - val_loss: 1.1944 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 9, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.5282 - categorical_accuracy: 0.7647 - val_loss: 1.4530 - val_categorical_accuracy: 0.2667\n",
            "Iteration: 9, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.6481 - categorical_accuracy: 0.7294 - val_loss: 1.2955 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 10, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.3394 - categorical_accuracy: 0.9059 - val_loss: 1.0591 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 10, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.3331 - categorical_accuracy: 0.8588 - val_loss: 1.6506 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 10, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.4170 - categorical_accuracy: 0.7647 - val_loss: 0.5421 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 11, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.2416 - categorical_accuracy: 0.8941 - val_loss: 1.1305 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 11, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.2063 - categorical_accuracy: 0.8941 - val_loss: 2.8372 - val_categorical_accuracy: 0.2000\n",
            "Iteration: 11, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.3295 - categorical_accuracy: 0.8588 - val_loss: 0.7129 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 12, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.1535 - categorical_accuracy: 0.9412 - val_loss: 1.4204 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 12, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.1800 - categorical_accuracy: 0.9412 - val_loss: 1.6534 - val_categorical_accuracy: 0.3333\n",
            "Iteration: 12, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.1124 - categorical_accuracy: 0.9647 - val_loss: 0.6652 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 13, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0764 - categorical_accuracy: 0.9765 - val_loss: 1.1678 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 13, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0741 - categorical_accuracy: 0.9647 - val_loss: 1.5622 - val_categorical_accuracy: 0.3333\n",
            "Iteration: 13, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0294 - categorical_accuracy: 0.9882 - val_loss: 1.4778 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 14, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0076 - categorical_accuracy: 1.0000 - val_loss: 1.5607 - val_categorical_accuracy: 0.6000\n",
            "Iteration: 14, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0098 - categorical_accuracy: 1.0000 - val_loss: 2.1848 - val_categorical_accuracy: 0.3333\n",
            "Iteration: 14, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0175 - categorical_accuracy: 0.9882 - val_loss: 0.7974 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 15, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0313 - categorical_accuracy: 0.9882 - val_loss: 1.3110 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 15, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0574 - categorical_accuracy: 0.9882 - val_loss: 1.1679 - val_categorical_accuracy: 0.6667\n",
            "Iteration: 15, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0250 - categorical_accuracy: 0.9882 - val_loss: 1.0565 - val_categorical_accuracy: 0.6000\n",
            "Iteration: 16, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8647 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 16, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0105 - categorical_accuracy: 1.0000 - val_loss: 1.7017 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 16, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0037 - categorical_accuracy: 1.0000 - val_loss: 1.2761 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 17, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0050 - categorical_accuracy: 1.0000 - val_loss: 1.7660 - val_categorical_accuracy: 0.6000\n",
            "Iteration: 17, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 2.1624 - val_categorical_accuracy: 0.3333\n",
            "Iteration: 17, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 1.4766 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 18, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 9.8114e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3800 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 18, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 5.8319e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4866 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 18, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0078 - categorical_accuracy: 1.0000 - val_loss: 0.4819 - val_categorical_accuracy: 0.7333\n",
            "Iteration: 19, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 2.4377 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 19, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 7.7317e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7543 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 19, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 1.2387 - val_categorical_accuracy: 0.6000\n",
            "Iteration: 20, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 2.4119 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 20, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 8.9411e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7050 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 20, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 5.1028e-04 - categorical_accuracy: 1.0000 - val_loss: 1.8352 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 21, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 2.9468e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2193 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 21, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 4.4429e-04 - categorical_accuracy: 1.0000 - val_loss: 1.9103 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 21, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 4.3974e-04 - categorical_accuracy: 1.0000 - val_loss: 1.8648 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 22, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 2.2013e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3909 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 22, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 7.3244e-05 - categorical_accuracy: 1.0000 - val_loss: 1.9813 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 22, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 3.4183e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7331 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 23, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.5266e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4958 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 23, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 2.5514e-04 - categorical_accuracy: 1.0000 - val_loss: 1.8798 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 23, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.3882e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7273 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 24, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 3.7331e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5347 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 24, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 1.8345e-04 - categorical_accuracy: 1.0000 - val_loss: 1.8378 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 24, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 9.4234e-05 - categorical_accuracy: 1.0000 - val_loss: 1.7110 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 25, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.6263e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5209 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 25, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 9.2789e-05 - categorical_accuracy: 1.0000 - val_loss: 1.9204 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 25, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 2.7767e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7870 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 26, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 3.4310e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3256 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 26, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 2.3397e-04 - categorical_accuracy: 1.0000 - val_loss: 1.9838 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 26, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 3.3731e-04 - categorical_accuracy: 1.0000 - val_loss: 1.6054 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 27, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 1.0423e-04 - categorical_accuracy: 1.0000 - val_loss: 2.7134 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 27, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 203ms/step - loss: 1.5269e-04 - categorical_accuracy: 1.0000 - val_loss: 2.0260 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 27, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 1.5061e-04 - categorical_accuracy: 1.0000 - val_loss: 2.0766 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 28, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 5.1674e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5871 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 28, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 8.6672e-05 - categorical_accuracy: 1.0000 - val_loss: 2.0596 - val_categorical_accuracy: 0.4667\n",
            "Iteration: 28, batch 2\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 5.0024e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2486 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 29, batch 0\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 204ms/step - loss: 2.8118e-04 - categorical_accuracy: 1.0000 - val_loss: 2.8174 - val_categorical_accuracy: 0.5333\n",
            "Iteration: 29, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 17s 205ms/step - loss: 1.8734e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2492 - val_categorical_accuracy: 0.4000\n",
            "Iteration: 29, batch 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpFB9HkBSQ6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
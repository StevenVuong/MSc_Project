{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p3_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/p3_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcQOFU0NrCvd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqsFNk0abH5",
        "colab_type": "text"
      },
      "source": [
        "**This notebook aims to modularise some of the input functions in P3 to suit the databse**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ejr96oJaYyX",
        "colab_type": "code",
        "colab": {},
        "outputId": "22d27183-b93a-48bc-e141-f24c3fa7f797"
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcV-vanjaa8p",
        "colab_type": "code",
        "colab": {},
        "outputId": "cc9f8f20-29ca-447a-dbbb-c019d80154a9"
      },
      "source": [
        "# mount google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where the data is\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln4-nPp3bBzp",
        "colab_type": "text"
      },
      "source": [
        "**Load our dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwpIQNqFa2H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_df = pd.read_pickle('processed_patient_df_TRY2.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rbFYCN1bExv",
        "colab_type": "code",
        "colab": {},
        "outputId": "a618d74a-01e9-4fd6-8b99-2475717bbbad"
      },
      "source": [
        "# establish control and pd df's\n",
        "df_control = patient_df[patient_df.Group == 0] \n",
        "df_pd = patient_df[patient_df.Group == 1] \n",
        "\n",
        "patient_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ilYDMpHb1hG",
        "colab_type": "text"
      },
      "source": [
        "**Load our file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSLclggbwHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for each set of training data in our pickle\n",
        "# load\n",
        "# process\n",
        "# train our model in this\n",
        "# 100 Test, 15% of 600 Validation, 85% of 600 Training. 700 Slices in total\n",
        "# Save our model, tweak and evaluate etc.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BajrkVMoeRFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets our y-values and converts to keras, one hot encoded outputs\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMILzi2f25i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_model():\n",
        "\n",
        "  # compile our model\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Convolution3D(filters=8, kernel_size=2, padding='same', input_shape=(200,200,160,1)))\n",
        "            #,kernel_regularizer=regularizers.l2(0.05),use_bias = True)) # padding on first one only?\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  model.add(MaxPooling3D(pool_size=2)) # pool_size=2\n",
        "  ## model.add(BatchNormalization(momentum=0.99)) # something to consider next level, moving mean and variance\n",
        "\n",
        "  model.add(Convolution3D(filters=16, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01)) \n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=32, kernel_size=3))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=64, kernel_size=3))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=128, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=256, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  # model.add(Dropout(0.45)) # add dropout to prevent overfitting\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  # model.add(Dropout(0.2))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']\n",
        "\n",
        "  # experiment with literally everything?... Random Search with optimisers\n",
        "            \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9amGVRb4Ct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "99aaf2ec-68a6-4453-f809-4eac9c2b91f0"
      },
      "source": [
        "# train on all, perhaps have different conditions for when we reach our last one\n",
        "total_slices_train = os.listdir('stored_batches')\n",
        "\n",
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "\n",
        "# load and fit our model for our instances\n",
        "for tsf in total_slices_train:\n",
        "  pkl_path = 'stored_batches/'+tsf\n",
        "  \n",
        "  # load pickle file\n",
        "  with open(pkl_path, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)\n",
        "    \n",
        "  # convert to numpy array\n",
        "  total_slices = np.array(total_slices)\n",
        "    \n",
        "  # process y-values\n",
        "  y_values = get_y_values(total_slices_info)\n",
        "  \n",
        "  # fit our model ## can play with batch size\n",
        "  model.fit(x=total_slices, y=y_values, batch_size=1, epochs=1, verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 99 samples, validate on 11 samples\n",
            "Epoch 1/1\n",
            "99/99 [==============================] - 20s 204ms/step - loss: 5.5355 - acc: 0.6566 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Train on 91 samples, validate on 11 samples\n",
            "Epoch 1/1\n",
            "91/91 [==============================] - 11s 121ms/step - loss: 8.1476 - acc: 0.4945 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Train on 90 samples, validate on 10 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 10s 113ms/step - loss: 6.0891 - acc: 0.6222 - val_loss: 6.4472 - val_acc: 0.6000\n",
            "Train on 95 samples, validate on 11 samples\n",
            "Epoch 1/1\n",
            "95/95 [==============================] - 11s 115ms/step - loss: 4.2416 - acc: 0.7368 - val_loss: 4.3958 - val_acc: 0.7273\n",
            "Train on 90 samples, validate on 10 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 10s 114ms/step - loss: 4.6563 - acc: 0.7111 - val_loss: 6.4472 - val_acc: 0.6000\n",
            "Train on 91 samples, validate on 11 samples\n",
            "Epoch 1/1\n",
            "91/91 [==============================] - 10s 114ms/step - loss: 5.1365 - acc: 0.6813 - val_loss: 4.3958 - val_acc: 0.7273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpfcrwFmDLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to use in future\n",
        "model.save('my_model_1.h5', overwrite=True)  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kaCTq4wXrCnX",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# load and run a saved model\n",
        "model = load_model('my_model_1.h5') # such a beastly model here.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "THCJ_J-WrCa0",
        "colab": {}
      },
      "source": [
        "# Now load the test file and run the prediction against it!\n",
        "total_slices_test = os.listdir('stored_batches')[6]\n",
        "pkl_path_test = 'stored_batches/'+total_slices_test\n",
        "\n",
        "# load pickle file\n",
        "with open(pkl_path_test, 'rb') as f:\n",
        "  total_slices, total_slices_info = pickle.load(f)\n",
        "  \n",
        "# convert to numpy array\n",
        "total_slices_test = np.array(total_slices)\n",
        "\n",
        "# process y-values\n",
        "y_true_test = get_y_values(total_slices_info)\n",
        "\n",
        "# split into 10, predict and build y-values as we go along\n",
        "total_slices_test_chunks = np.array_split(total_slices_test, 10)\n",
        "\n",
        "y_predictions = []\n",
        "# run test chunks and get predictions.. Doesn't work because goddamn everything is 1\n",
        "for test_chunk in total_slices_test_chunks:\n",
        "  \n",
        "  # make a prediction\n",
        "  y_chunk_pred = model.predict_classes(test_chunk)\n",
        "  y_predictions.append(y_chunk_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gLd_5eaqIEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate and turn to keras friendly output\n",
        "y_predictions = np.concatenate(y_predictions)\n",
        "y_predictions = np.array(to_categorical(y_predictions, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xutxY6CjrCV6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObnnagLfmZsg",
        "colab_type": "code",
        "colab": {},
        "outputId": "472d3d04-ea30-43b7-a7b5-2bb6b201e495"
      },
      "source": [
        "# get accuracy score\n",
        "accuracy_score(y_true_test, y_predictions, normalize=True, sample_weight=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CbAMKwHhsXS",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae94f029-d791-4ab4-d0cc-bd1759847dba"
      },
      "source": [
        "print (\"Accuracy Score: %s\" % accura\n",
        "cy_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
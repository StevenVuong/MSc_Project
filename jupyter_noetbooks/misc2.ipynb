{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "misc2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/misc2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzBN1AhkB8-",
        "colab_type": "text"
      },
      "source": [
        "**Goal: Build a joint deeplearning model which trains on ages and genders, to classify that of the fifth batch.** \n",
        "**Then ensemble this with deep learning prediction outputs of our best model to get the final classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HffPUoiMkAb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# other imports to handle files\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU, Input, ReLU, concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJGlXqK7j8vD",
        "colab_type": "code",
        "outputId": "06519bd6-49d6-42c5-c1da-5567a587b23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where we will be working\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project/')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRqVyvgomjT",
        "colab_type": "text"
      },
      "source": [
        "**Run through our batches and train our model after building it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrA5vdtKpcqi",
        "colab_type": "code",
        "outputId": "0d927135-da07-4653-d4bb-a62129881143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## Loop through all our total slices and accumulate all the total slices info\n",
        "training_batch_f = os.listdir('all_mprage_grappa/processed_brains_aug')[:3]\n",
        "print (training_batch_f)\n",
        "\n",
        "# build ultima total slices info\n",
        "tsi_ultima = []\n",
        "\n",
        "# load the pickle (train with 0 to 6)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) # stored_batches/total_slices_batch5\n",
        "    \n",
        "    tsi_ultima.extend(total_slices_info)\n",
        "    \n",
        "print (np.shape(tsi_ultima))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8sh1rCrmME",
        "colab_type": "code",
        "outputId": "1b976da3-48d8-4a0c-ddf4-1607df96e765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(tsi_ultima) # use this to train"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE9O2hmGkBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classification(total_slices_info):\n",
        "    '''get information of patient from total slices info'''\n",
        "    classif = [s[2] for s in total_slices_info]\n",
        "    classif = np.array(to_categorical(classif, 2))\n",
        "    \n",
        "    # now do the same for the sex, 'F' is 0, 'M' is 1\n",
        "    sex = [s[1] for s in total_slices_info]\n",
        "    for i in range(len(sex)):\n",
        "        if sex[i] == 'F':\n",
        "            sex[i] = 0\n",
        "        if sex[i] == 'M':\n",
        "            sex[i] = 1\n",
        "    sex = np.array(to_categorical(sex, 2))\n",
        "    \n",
        "    # finally for age, one hot encode ages 0 to 100 (101 classes then)\n",
        "    ages = [s[3] for s in total_slices_info]\n",
        "    ages = np.array(to_categorical(ages, 101))\n",
        "    \n",
        "    return classif, sex, ages\n",
        "\n",
        "y_train, sex_train, ages_train = get_classification(tsi_ultima)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGfmbPdP52xt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9418b96-a162-43d8-ac91-ef772399a12c"
      },
      "source": [
        "def larger_info_dataset(df_path):\n",
        "  '''Get the info from any given dataframe'''\n",
        "  ## Get the classification of the y-values, sex and ages from a dataframe instead\n",
        "  big_boi_df = pd.read_csv(df_path)\n",
        "\n",
        "  # remove duplicate subject ids\n",
        "  big_boi_df = big_boi_df.drop_duplicates(subset='Subject', keep='first')\n",
        "\n",
        "  # map groups, control to 0, pd to 1. Also do this for sex, M is 1, F is 0\n",
        "  big_boi_df = big_boi_df.replace({'Group': {'Control': 0, 'PD': 1}, 'Sex': {'M':1, 'F':0}})\n",
        "\n",
        "  # split into training and test sets\n",
        "  train_df, test_df = train_test_split(big_boi_df, test_size=0.10)\n",
        "\n",
        "  print (\"Train Size: %d, Test Size: %d\" % (len(train_df), len(test_df)))\n",
        "\n",
        "  # get the values for train\n",
        "  ages_train = np.array(to_categorical(train_df['Age'].values,101))\n",
        "  sex_train = np.array(to_categorical(train_df['Sex'].values, 2))\n",
        "  y_train = np.array(to_categorical(train_df['Group'].values, 2))\n",
        "\n",
        "  # get the values for test\n",
        "  ages_test = np.array(to_categorical(test_df['Age'].values,101))\n",
        "  sex_test = np.array(to_categorical(test_df['Sex'].values, 2))\n",
        "  y_test = np.array(to_categorical(test_df['Group'].values, 2))\n",
        "  \n",
        "  return [ages_train, sex_train, y_train, ages_test, sex_test, y_test]\n",
        "\n",
        "ages_train, sex_train, y_train, ages_test, sex_test, y_test = larger_info_dataset('all_mprage_grappa/Control_PD_6_21_2019.csv')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Size: 536, Test Size: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWqqxB8ypBgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "    '''Create our MLP network'''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "    model.add(Dense(2, activation=\"relu\"))\n",
        " \n",
        "    # check to see if the regression node should be added\n",
        "    if regress:\n",
        "        model.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "    # return our model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGyH7DU5NOwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(hist):\n",
        "  ''' Function to get our metrics from history and score as inputs'''\n",
        "\n",
        "  # actually obtain our metrics\n",
        "  val_loss = hist.history['val_loss'][0]\n",
        "  val_acc = hist.history['val_categorical_accuracy'][0]\n",
        "  train_loss = hist.history['loss'][0]\n",
        "  train_acc = hist.history['categorical_accuracy'][0]\n",
        "\n",
        "  # put everything into one array\n",
        "  return [val_loss, val_acc, train_loss, train_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqqiKz9VSlrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_binary(cat_array):\n",
        "  '''Function to convert categorical back to binary values'''\n",
        "  binary_output_array = []\n",
        "  for i in range(len(cat_array)):\n",
        "    binary_output_array.append(np.argmax(cat_array[i]))\n",
        "    \n",
        "  binary_output_array = np.array(binary_output_array)\n",
        "  return binary_output_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OS2QxNNpHaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creds: https://www.puzzlr.org/the-keras-functional-api-five-simple-examples/\n",
        "# Create MLP models\n",
        "mlp_sex = create_mlp(sex_train.shape[1], regress=False)\n",
        "mlp_age = create_mlp(ages_train.shape[1], regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input = concatenate([mlp_sex.output, mlp_age.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(8, activation=\"relu\")(combined_input)\n",
        "x = Dense(2, activation=\"sigmoid\")(x)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "as_model = Model(inputs=[mlp_sex.input, mlp_age.input], outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wNgZ89trZc",
        "colab_type": "code",
        "outputId": "b27d3f5b-0c88-4f15-ca6d-47f35b28bda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16850
        }
      },
      "source": [
        "# compile our model\n",
        "as_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "history = as_model.fit([sex_train, ages_train], y_train, validation_split=0.1, epochs=500, batch_size=150)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n",
            "Train on 482 samples, validate on 54 samples\n",
            "Epoch 1/500\n",
            "482/482 [==============================] - 2s 4ms/step - loss: 0.7269 - categorical_accuracy: 0.3216 - val_loss: 0.7218 - val_categorical_accuracy: 0.3148\n",
            "Epoch 2/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.7211 - categorical_accuracy: 0.3216 - val_loss: 0.7163 - val_categorical_accuracy: 0.3148\n",
            "Epoch 3/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.7157 - categorical_accuracy: 0.3216 - val_loss: 0.7111 - val_categorical_accuracy: 0.3148\n",
            "Epoch 4/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.7106 - categorical_accuracy: 0.3216 - val_loss: 0.7065 - val_categorical_accuracy: 0.3148\n",
            "Epoch 5/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.7058 - categorical_accuracy: 0.3216 - val_loss: 0.7022 - val_categorical_accuracy: 0.3148\n",
            "Epoch 6/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.7015 - categorical_accuracy: 0.3216 - val_loss: 0.6982 - val_categorical_accuracy: 0.3148\n",
            "Epoch 7/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6975 - categorical_accuracy: 0.3216 - val_loss: 0.6945 - val_categorical_accuracy: 0.3148\n",
            "Epoch 8/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6938 - categorical_accuracy: 0.4772 - val_loss: 0.6910 - val_categorical_accuracy: 0.7037\n",
            "Epoch 9/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6907 - categorical_accuracy: 0.6577 - val_loss: 0.6881 - val_categorical_accuracy: 0.6667\n",
            "Epoch 10/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6880 - categorical_accuracy: 0.6763 - val_loss: 0.6861 - val_categorical_accuracy: 0.6852\n",
            "Epoch 11/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6861 - categorical_accuracy: 0.6784 - val_loss: 0.6849 - val_categorical_accuracy: 0.6852\n",
            "Epoch 12/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6847 - categorical_accuracy: 0.6784 - val_loss: 0.6837 - val_categorical_accuracy: 0.6852\n",
            "Epoch 13/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6836 - categorical_accuracy: 0.6784 - val_loss: 0.6825 - val_categorical_accuracy: 0.6852\n",
            "Epoch 14/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.6825 - categorical_accuracy: 0.6784 - val_loss: 0.6813 - val_categorical_accuracy: 0.6852\n",
            "Epoch 15/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6814 - categorical_accuracy: 0.6784 - val_loss: 0.6801 - val_categorical_accuracy: 0.6852\n",
            "Epoch 16/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.6804 - categorical_accuracy: 0.6784 - val_loss: 0.6791 - val_categorical_accuracy: 0.6852\n",
            "Epoch 17/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6795 - categorical_accuracy: 0.6784 - val_loss: 0.6783 - val_categorical_accuracy: 0.6852\n",
            "Epoch 18/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6786 - categorical_accuracy: 0.6784 - val_loss: 0.6774 - val_categorical_accuracy: 0.6852\n",
            "Epoch 19/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6777 - categorical_accuracy: 0.6784 - val_loss: 0.6764 - val_categorical_accuracy: 0.6852\n",
            "Epoch 20/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6768 - categorical_accuracy: 0.6784 - val_loss: 0.6754 - val_categorical_accuracy: 0.6852\n",
            "Epoch 21/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6758 - categorical_accuracy: 0.6784 - val_loss: 0.6744 - val_categorical_accuracy: 0.6852\n",
            "Epoch 22/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.6748 - categorical_accuracy: 0.6784 - val_loss: 0.6735 - val_categorical_accuracy: 0.6852\n",
            "Epoch 23/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6738 - categorical_accuracy: 0.6784 - val_loss: 0.6724 - val_categorical_accuracy: 0.6852\n",
            "Epoch 24/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6727 - categorical_accuracy: 0.6784 - val_loss: 0.6714 - val_categorical_accuracy: 0.6852\n",
            "Epoch 25/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6717 - categorical_accuracy: 0.6784 - val_loss: 0.6704 - val_categorical_accuracy: 0.6852\n",
            "Epoch 26/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6707 - categorical_accuracy: 0.6784 - val_loss: 0.6692 - val_categorical_accuracy: 0.6852\n",
            "Epoch 27/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.6695 - categorical_accuracy: 0.6784 - val_loss: 0.6680 - val_categorical_accuracy: 0.6852\n",
            "Epoch 28/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6683 - categorical_accuracy: 0.6784 - val_loss: 0.6668 - val_categorical_accuracy: 0.6852\n",
            "Epoch 29/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.6671 - categorical_accuracy: 0.6784 - val_loss: 0.6656 - val_categorical_accuracy: 0.6852\n",
            "Epoch 30/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6660 - categorical_accuracy: 0.6784 - val_loss: 0.6644 - val_categorical_accuracy: 0.6852\n",
            "Epoch 31/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6648 - categorical_accuracy: 0.6784 - val_loss: 0.6632 - val_categorical_accuracy: 0.6852\n",
            "Epoch 32/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6636 - categorical_accuracy: 0.6784 - val_loss: 0.6620 - val_categorical_accuracy: 0.6852\n",
            "Epoch 33/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6624 - categorical_accuracy: 0.6784 - val_loss: 0.6607 - val_categorical_accuracy: 0.6852\n",
            "Epoch 34/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6611 - categorical_accuracy: 0.6784 - val_loss: 0.6594 - val_categorical_accuracy: 0.6852\n",
            "Epoch 35/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.6597 - categorical_accuracy: 0.6784 - val_loss: 0.6581 - val_categorical_accuracy: 0.6852\n",
            "Epoch 36/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.6584 - categorical_accuracy: 0.6784 - val_loss: 0.6567 - val_categorical_accuracy: 0.6852\n",
            "Epoch 37/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6571 - categorical_accuracy: 0.6784 - val_loss: 0.6553 - val_categorical_accuracy: 0.6852\n",
            "Epoch 38/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6559 - categorical_accuracy: 0.6784 - val_loss: 0.6539 - val_categorical_accuracy: 0.6852\n",
            "Epoch 39/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.6544 - categorical_accuracy: 0.6784 - val_loss: 0.6526 - val_categorical_accuracy: 0.6852\n",
            "Epoch 40/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6531 - categorical_accuracy: 0.6784 - val_loss: 0.6512 - val_categorical_accuracy: 0.6852\n",
            "Epoch 41/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6514 - categorical_accuracy: 0.6784 - val_loss: 0.6498 - val_categorical_accuracy: 0.6852\n",
            "Epoch 42/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6500 - categorical_accuracy: 0.6784 - val_loss: 0.6485 - val_categorical_accuracy: 0.6852\n",
            "Epoch 43/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6486 - categorical_accuracy: 0.6784 - val_loss: 0.6471 - val_categorical_accuracy: 0.6852\n",
            "Epoch 44/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6470 - categorical_accuracy: 0.6784 - val_loss: 0.6458 - val_categorical_accuracy: 0.6852\n",
            "Epoch 45/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.6453 - categorical_accuracy: 0.6784 - val_loss: 0.6446 - val_categorical_accuracy: 0.6852\n",
            "Epoch 46/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6438 - categorical_accuracy: 0.6784 - val_loss: 0.6432 - val_categorical_accuracy: 0.6852\n",
            "Epoch 47/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6421 - categorical_accuracy: 0.6784 - val_loss: 0.6419 - val_categorical_accuracy: 0.6852\n",
            "Epoch 48/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6401 - categorical_accuracy: 0.6784 - val_loss: 0.6408 - val_categorical_accuracy: 0.6852\n",
            "Epoch 49/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6386 - categorical_accuracy: 0.6784 - val_loss: 0.6397 - val_categorical_accuracy: 0.6852\n",
            "Epoch 50/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6369 - categorical_accuracy: 0.6784 - val_loss: 0.6388 - val_categorical_accuracy: 0.6852\n",
            "Epoch 51/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6353 - categorical_accuracy: 0.6784 - val_loss: 0.6379 - val_categorical_accuracy: 0.6852\n",
            "Epoch 52/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6335 - categorical_accuracy: 0.6784 - val_loss: 0.6372 - val_categorical_accuracy: 0.6852\n",
            "Epoch 53/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.6320 - categorical_accuracy: 0.6784 - val_loss: 0.6363 - val_categorical_accuracy: 0.6852\n",
            "Epoch 54/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6304 - categorical_accuracy: 0.6784 - val_loss: 0.6356 - val_categorical_accuracy: 0.6852\n",
            "Epoch 55/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6287 - categorical_accuracy: 0.6784 - val_loss: 0.6351 - val_categorical_accuracy: 0.6852\n",
            "Epoch 56/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6276 - categorical_accuracy: 0.6784 - val_loss: 0.6348 - val_categorical_accuracy: 0.6852\n",
            "Epoch 57/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6259 - categorical_accuracy: 0.6784 - val_loss: 0.6347 - val_categorical_accuracy: 0.6852\n",
            "Epoch 58/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.6247 - categorical_accuracy: 0.6784 - val_loss: 0.6348 - val_categorical_accuracy: 0.6852\n",
            "Epoch 59/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.6238 - categorical_accuracy: 0.6784 - val_loss: 0.6351 - val_categorical_accuracy: 0.6852\n",
            "Epoch 60/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6223 - categorical_accuracy: 0.6784 - val_loss: 0.6356 - val_categorical_accuracy: 0.6852\n",
            "Epoch 61/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6214 - categorical_accuracy: 0.6784 - val_loss: 0.6366 - val_categorical_accuracy: 0.6852\n",
            "Epoch 62/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6207 - categorical_accuracy: 0.6784 - val_loss: 0.6379 - val_categorical_accuracy: 0.6852\n",
            "Epoch 63/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.6197 - categorical_accuracy: 0.6784 - val_loss: 0.6392 - val_categorical_accuracy: 0.6852\n",
            "Epoch 64/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.6191 - categorical_accuracy: 0.6784 - val_loss: 0.6402 - val_categorical_accuracy: 0.6852\n",
            "Epoch 65/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6184 - categorical_accuracy: 0.6784 - val_loss: 0.6413 - val_categorical_accuracy: 0.6852\n",
            "Epoch 66/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.6178 - categorical_accuracy: 0.6784 - val_loss: 0.6424 - val_categorical_accuracy: 0.6852\n",
            "Epoch 67/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.6172 - categorical_accuracy: 0.6784 - val_loss: 0.6436 - val_categorical_accuracy: 0.6852\n",
            "Epoch 68/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6164 - categorical_accuracy: 0.6784 - val_loss: 0.6445 - val_categorical_accuracy: 0.6852\n",
            "Epoch 69/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6158 - categorical_accuracy: 0.6784 - val_loss: 0.6457 - val_categorical_accuracy: 0.6852\n",
            "Epoch 70/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.6152 - categorical_accuracy: 0.6784 - val_loss: 0.6470 - val_categorical_accuracy: 0.6852\n",
            "Epoch 71/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6145 - categorical_accuracy: 0.6784 - val_loss: 0.6484 - val_categorical_accuracy: 0.6852\n",
            "Epoch 72/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6138 - categorical_accuracy: 0.6784 - val_loss: 0.6497 - val_categorical_accuracy: 0.6852\n",
            "Epoch 73/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6132 - categorical_accuracy: 0.6784 - val_loss: 0.6510 - val_categorical_accuracy: 0.6852\n",
            "Epoch 74/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6127 - categorical_accuracy: 0.6784 - val_loss: 0.6527 - val_categorical_accuracy: 0.6852\n",
            "Epoch 75/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6120 - categorical_accuracy: 0.6784 - val_loss: 0.6543 - val_categorical_accuracy: 0.6852\n",
            "Epoch 76/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6114 - categorical_accuracy: 0.6784 - val_loss: 0.6559 - val_categorical_accuracy: 0.6852\n",
            "Epoch 77/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.6108 - categorical_accuracy: 0.6784 - val_loss: 0.6573 - val_categorical_accuracy: 0.6852\n",
            "Epoch 78/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.6102 - categorical_accuracy: 0.6784 - val_loss: 0.6579 - val_categorical_accuracy: 0.6852\n",
            "Epoch 79/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.6097 - categorical_accuracy: 0.6784 - val_loss: 0.6584 - val_categorical_accuracy: 0.6852\n",
            "Epoch 80/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6093 - categorical_accuracy: 0.6784 - val_loss: 0.6590 - val_categorical_accuracy: 0.6852\n",
            "Epoch 81/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6088 - categorical_accuracy: 0.6784 - val_loss: 0.6599 - val_categorical_accuracy: 0.6852\n",
            "Epoch 82/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6083 - categorical_accuracy: 0.6784 - val_loss: 0.6613 - val_categorical_accuracy: 0.6852\n",
            "Epoch 83/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.6078 - categorical_accuracy: 0.6784 - val_loss: 0.6632 - val_categorical_accuracy: 0.6852\n",
            "Epoch 84/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6072 - categorical_accuracy: 0.6784 - val_loss: 0.6651 - val_categorical_accuracy: 0.6852\n",
            "Epoch 85/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6070 - categorical_accuracy: 0.6784 - val_loss: 0.6674 - val_categorical_accuracy: 0.6852\n",
            "Epoch 86/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6063 - categorical_accuracy: 0.6784 - val_loss: 0.6687 - val_categorical_accuracy: 0.6852\n",
            "Epoch 87/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6058 - categorical_accuracy: 0.6784 - val_loss: 0.6703 - val_categorical_accuracy: 0.6852\n",
            "Epoch 88/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6053 - categorical_accuracy: 0.6784 - val_loss: 0.6716 - val_categorical_accuracy: 0.6852\n",
            "Epoch 89/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6048 - categorical_accuracy: 0.6784 - val_loss: 0.6734 - val_categorical_accuracy: 0.6852\n",
            "Epoch 90/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.6043 - categorical_accuracy: 0.6784 - val_loss: 0.6756 - val_categorical_accuracy: 0.6852\n",
            "Epoch 91/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6039 - categorical_accuracy: 0.6784 - val_loss: 0.6774 - val_categorical_accuracy: 0.6852\n",
            "Epoch 92/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6038 - categorical_accuracy: 0.6784 - val_loss: 0.6795 - val_categorical_accuracy: 0.6852\n",
            "Epoch 93/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.6031 - categorical_accuracy: 0.6784 - val_loss: 0.6800 - val_categorical_accuracy: 0.6852\n",
            "Epoch 94/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.6028 - categorical_accuracy: 0.6784 - val_loss: 0.6805 - val_categorical_accuracy: 0.6852\n",
            "Epoch 95/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.6024 - categorical_accuracy: 0.6784 - val_loss: 0.6807 - val_categorical_accuracy: 0.6852\n",
            "Epoch 96/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6020 - categorical_accuracy: 0.6784 - val_loss: 0.6823 - val_categorical_accuracy: 0.6852\n",
            "Epoch 97/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.6016 - categorical_accuracy: 0.6784 - val_loss: 0.6841 - val_categorical_accuracy: 0.6852\n",
            "Epoch 98/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.6012 - categorical_accuracy: 0.6784 - val_loss: 0.6862 - val_categorical_accuracy: 0.6852\n",
            "Epoch 99/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6007 - categorical_accuracy: 0.6784 - val_loss: 0.6879 - val_categorical_accuracy: 0.6852\n",
            "Epoch 100/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.6004 - categorical_accuracy: 0.6784 - val_loss: 0.6895 - val_categorical_accuracy: 0.6852\n",
            "Epoch 101/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.6000 - categorical_accuracy: 0.6784 - val_loss: 0.6907 - val_categorical_accuracy: 0.6852\n",
            "Epoch 102/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5996 - categorical_accuracy: 0.6784 - val_loss: 0.6922 - val_categorical_accuracy: 0.6852\n",
            "Epoch 103/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5992 - categorical_accuracy: 0.6784 - val_loss: 0.6944 - val_categorical_accuracy: 0.6852\n",
            "Epoch 104/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5988 - categorical_accuracy: 0.6784 - val_loss: 0.6961 - val_categorical_accuracy: 0.6852\n",
            "Epoch 105/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5984 - categorical_accuracy: 0.6784 - val_loss: 0.6978 - val_categorical_accuracy: 0.6852\n",
            "Epoch 106/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5980 - categorical_accuracy: 0.6784 - val_loss: 0.6994 - val_categorical_accuracy: 0.6852\n",
            "Epoch 107/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5977 - categorical_accuracy: 0.6784 - val_loss: 0.7019 - val_categorical_accuracy: 0.6852\n",
            "Epoch 108/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5973 - categorical_accuracy: 0.6784 - val_loss: 0.7035 - val_categorical_accuracy: 0.6852\n",
            "Epoch 109/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5971 - categorical_accuracy: 0.6784 - val_loss: 0.7048 - val_categorical_accuracy: 0.6852\n",
            "Epoch 110/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.5968 - categorical_accuracy: 0.6784 - val_loss: 0.7074 - val_categorical_accuracy: 0.6852\n",
            "Epoch 111/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5964 - categorical_accuracy: 0.6784 - val_loss: 0.7089 - val_categorical_accuracy: 0.6852\n",
            "Epoch 112/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5962 - categorical_accuracy: 0.6784 - val_loss: 0.7105 - val_categorical_accuracy: 0.6852\n",
            "Epoch 113/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5959 - categorical_accuracy: 0.6784 - val_loss: 0.7124 - val_categorical_accuracy: 0.6852\n",
            "Epoch 114/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5956 - categorical_accuracy: 0.6784 - val_loss: 0.7139 - val_categorical_accuracy: 0.6852\n",
            "Epoch 115/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5953 - categorical_accuracy: 0.6784 - val_loss: 0.7155 - val_categorical_accuracy: 0.6852\n",
            "Epoch 116/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5951 - categorical_accuracy: 0.6784 - val_loss: 0.7171 - val_categorical_accuracy: 0.6852\n",
            "Epoch 117/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5948 - categorical_accuracy: 0.6784 - val_loss: 0.7186 - val_categorical_accuracy: 0.6852\n",
            "Epoch 118/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5946 - categorical_accuracy: 0.6784 - val_loss: 0.7194 - val_categorical_accuracy: 0.6852\n",
            "Epoch 119/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5944 - categorical_accuracy: 0.6784 - val_loss: 0.7214 - val_categorical_accuracy: 0.6852\n",
            "Epoch 120/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5942 - categorical_accuracy: 0.6784 - val_loss: 0.7225 - val_categorical_accuracy: 0.6852\n",
            "Epoch 121/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5939 - categorical_accuracy: 0.6784 - val_loss: 0.7230 - val_categorical_accuracy: 0.6852\n",
            "Epoch 122/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5937 - categorical_accuracy: 0.6784 - val_loss: 0.7235 - val_categorical_accuracy: 0.6852\n",
            "Epoch 123/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5935 - categorical_accuracy: 0.6784 - val_loss: 0.7240 - val_categorical_accuracy: 0.6852\n",
            "Epoch 124/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5933 - categorical_accuracy: 0.6784 - val_loss: 0.7251 - val_categorical_accuracy: 0.6852\n",
            "Epoch 125/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5931 - categorical_accuracy: 0.6784 - val_loss: 0.7273 - val_categorical_accuracy: 0.6852\n",
            "Epoch 126/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5929 - categorical_accuracy: 0.6784 - val_loss: 0.7286 - val_categorical_accuracy: 0.6852\n",
            "Epoch 127/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5926 - categorical_accuracy: 0.6784 - val_loss: 0.7305 - val_categorical_accuracy: 0.6852\n",
            "Epoch 128/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5924 - categorical_accuracy: 0.6784 - val_loss: 0.7330 - val_categorical_accuracy: 0.6852\n",
            "Epoch 129/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5922 - categorical_accuracy: 0.6784 - val_loss: 0.7349 - val_categorical_accuracy: 0.6852\n",
            "Epoch 130/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5919 - categorical_accuracy: 0.6784 - val_loss: 0.7367 - val_categorical_accuracy: 0.6852\n",
            "Epoch 131/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5918 - categorical_accuracy: 0.6784 - val_loss: 0.7385 - val_categorical_accuracy: 0.6852\n",
            "Epoch 132/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5915 - categorical_accuracy: 0.6784 - val_loss: 0.7401 - val_categorical_accuracy: 0.6852\n",
            "Epoch 133/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5912 - categorical_accuracy: 0.6784 - val_loss: 0.7426 - val_categorical_accuracy: 0.6852\n",
            "Epoch 134/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5910 - categorical_accuracy: 0.6784 - val_loss: 0.7448 - val_categorical_accuracy: 0.6852\n",
            "Epoch 135/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5908 - categorical_accuracy: 0.6784 - val_loss: 0.7483 - val_categorical_accuracy: 0.6852\n",
            "Epoch 136/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5906 - categorical_accuracy: 0.6784 - val_loss: 0.7517 - val_categorical_accuracy: 0.6852\n",
            "Epoch 137/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5904 - categorical_accuracy: 0.6784 - val_loss: 0.7550 - val_categorical_accuracy: 0.6852\n",
            "Epoch 138/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5902 - categorical_accuracy: 0.6784 - val_loss: 0.7578 - val_categorical_accuracy: 0.6852\n",
            "Epoch 139/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5900 - categorical_accuracy: 0.6784 - val_loss: 0.7612 - val_categorical_accuracy: 0.6852\n",
            "Epoch 140/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5899 - categorical_accuracy: 0.6784 - val_loss: 0.7625 - val_categorical_accuracy: 0.6852\n",
            "Epoch 141/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5897 - categorical_accuracy: 0.6784 - val_loss: 0.7613 - val_categorical_accuracy: 0.6852\n",
            "Epoch 142/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5894 - categorical_accuracy: 0.6784 - val_loss: 0.7598 - val_categorical_accuracy: 0.6852\n",
            "Epoch 143/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5893 - categorical_accuracy: 0.6784 - val_loss: 0.7588 - val_categorical_accuracy: 0.6852\n",
            "Epoch 144/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5891 - categorical_accuracy: 0.6784 - val_loss: 0.7581 - val_categorical_accuracy: 0.6852\n",
            "Epoch 145/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5890 - categorical_accuracy: 0.6784 - val_loss: 0.7570 - val_categorical_accuracy: 0.6852\n",
            "Epoch 146/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5889 - categorical_accuracy: 0.6784 - val_loss: 0.7543 - val_categorical_accuracy: 0.6852\n",
            "Epoch 147/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5888 - categorical_accuracy: 0.6784 - val_loss: 0.7531 - val_categorical_accuracy: 0.6852\n",
            "Epoch 148/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5888 - categorical_accuracy: 0.6784 - val_loss: 0.7527 - val_categorical_accuracy: 0.6852\n",
            "Epoch 149/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5887 - categorical_accuracy: 0.6784 - val_loss: 0.7537 - val_categorical_accuracy: 0.6852\n",
            "Epoch 150/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5885 - categorical_accuracy: 0.6784 - val_loss: 0.7561 - val_categorical_accuracy: 0.6852\n",
            "Epoch 151/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5882 - categorical_accuracy: 0.6784 - val_loss: 0.7596 - val_categorical_accuracy: 0.6852\n",
            "Epoch 152/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5880 - categorical_accuracy: 0.6784 - val_loss: 0.7632 - val_categorical_accuracy: 0.6852\n",
            "Epoch 153/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5876 - categorical_accuracy: 0.6784 - val_loss: 0.7666 - val_categorical_accuracy: 0.6852\n",
            "Epoch 154/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5874 - categorical_accuracy: 0.6784 - val_loss: 0.7714 - val_categorical_accuracy: 0.6852\n",
            "Epoch 155/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5873 - categorical_accuracy: 0.6784 - val_loss: 0.7754 - val_categorical_accuracy: 0.6852\n",
            "Epoch 156/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5872 - categorical_accuracy: 0.6784 - val_loss: 0.7775 - val_categorical_accuracy: 0.6852\n",
            "Epoch 157/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5870 - categorical_accuracy: 0.6784 - val_loss: 0.7771 - val_categorical_accuracy: 0.6852\n",
            "Epoch 158/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5868 - categorical_accuracy: 0.6784 - val_loss: 0.7760 - val_categorical_accuracy: 0.6852\n",
            "Epoch 159/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5867 - categorical_accuracy: 0.6784 - val_loss: 0.7732 - val_categorical_accuracy: 0.6852\n",
            "Epoch 160/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5865 - categorical_accuracy: 0.6784 - val_loss: 0.7718 - val_categorical_accuracy: 0.6852\n",
            "Epoch 161/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5865 - categorical_accuracy: 0.6784 - val_loss: 0.7708 - val_categorical_accuracy: 0.6852\n",
            "Epoch 162/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5863 - categorical_accuracy: 0.6784 - val_loss: 0.7731 - val_categorical_accuracy: 0.6852\n",
            "Epoch 163/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5861 - categorical_accuracy: 0.6784 - val_loss: 0.7754 - val_categorical_accuracy: 0.6852\n",
            "Epoch 164/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5859 - categorical_accuracy: 0.6784 - val_loss: 0.7759 - val_categorical_accuracy: 0.6852\n",
            "Epoch 165/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5858 - categorical_accuracy: 0.6784 - val_loss: 0.7760 - val_categorical_accuracy: 0.6852\n",
            "Epoch 166/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5857 - categorical_accuracy: 0.6784 - val_loss: 0.7779 - val_categorical_accuracy: 0.6852\n",
            "Epoch 167/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5855 - categorical_accuracy: 0.6784 - val_loss: 0.7799 - val_categorical_accuracy: 0.6852\n",
            "Epoch 168/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5854 - categorical_accuracy: 0.6784 - val_loss: 0.7817 - val_categorical_accuracy: 0.6852\n",
            "Epoch 169/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5854 - categorical_accuracy: 0.6784 - val_loss: 0.7835 - val_categorical_accuracy: 0.6852\n",
            "Epoch 170/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5852 - categorical_accuracy: 0.6784 - val_loss: 0.7837 - val_categorical_accuracy: 0.6852\n",
            "Epoch 171/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5851 - categorical_accuracy: 0.6784 - val_loss: 0.7852 - val_categorical_accuracy: 0.6852\n",
            "Epoch 172/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5850 - categorical_accuracy: 0.6784 - val_loss: 0.7882 - val_categorical_accuracy: 0.6852\n",
            "Epoch 173/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5849 - categorical_accuracy: 0.6784 - val_loss: 0.7904 - val_categorical_accuracy: 0.6852\n",
            "Epoch 174/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5847 - categorical_accuracy: 0.6784 - val_loss: 0.7923 - val_categorical_accuracy: 0.6852\n",
            "Epoch 175/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5846 - categorical_accuracy: 0.6784 - val_loss: 0.7952 - val_categorical_accuracy: 0.6852\n",
            "Epoch 176/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5845 - categorical_accuracy: 0.6784 - val_loss: 0.7965 - val_categorical_accuracy: 0.6852\n",
            "Epoch 177/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5844 - categorical_accuracy: 0.6784 - val_loss: 0.7985 - val_categorical_accuracy: 0.6852\n",
            "Epoch 178/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5843 - categorical_accuracy: 0.6784 - val_loss: 0.8025 - val_categorical_accuracy: 0.6852\n",
            "Epoch 179/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5843 - categorical_accuracy: 0.6784 - val_loss: 0.8069 - val_categorical_accuracy: 0.6852\n",
            "Epoch 180/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5842 - categorical_accuracy: 0.6784 - val_loss: 0.8084 - val_categorical_accuracy: 0.6852\n",
            "Epoch 181/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5841 - categorical_accuracy: 0.6784 - val_loss: 0.8084 - val_categorical_accuracy: 0.6852\n",
            "Epoch 182/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5840 - categorical_accuracy: 0.6784 - val_loss: 0.8076 - val_categorical_accuracy: 0.6852\n",
            "Epoch 183/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5839 - categorical_accuracy: 0.6784 - val_loss: 0.8069 - val_categorical_accuracy: 0.6852\n",
            "Epoch 184/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5838 - categorical_accuracy: 0.6784 - val_loss: 0.8052 - val_categorical_accuracy: 0.6852\n",
            "Epoch 185/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5836 - categorical_accuracy: 0.6784 - val_loss: 0.8055 - val_categorical_accuracy: 0.6852\n",
            "Epoch 186/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5835 - categorical_accuracy: 0.6784 - val_loss: 0.8053 - val_categorical_accuracy: 0.6852\n",
            "Epoch 187/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5834 - categorical_accuracy: 0.6784 - val_loss: 0.8028 - val_categorical_accuracy: 0.6852\n",
            "Epoch 188/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5834 - categorical_accuracy: 0.6784 - val_loss: 0.8013 - val_categorical_accuracy: 0.6852\n",
            "Epoch 189/500\n",
            "482/482 [==============================] - 0s 23us/step - loss: 0.5833 - categorical_accuracy: 0.6784 - val_loss: 0.8004 - val_categorical_accuracy: 0.6852\n",
            "Epoch 190/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5833 - categorical_accuracy: 0.6784 - val_loss: 0.7984 - val_categorical_accuracy: 0.6852\n",
            "Epoch 191/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5833 - categorical_accuracy: 0.6784 - val_loss: 0.7981 - val_categorical_accuracy: 0.6852\n",
            "Epoch 192/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5831 - categorical_accuracy: 0.6784 - val_loss: 0.7999 - val_categorical_accuracy: 0.6852\n",
            "Epoch 193/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5831 - categorical_accuracy: 0.6784 - val_loss: 0.8022 - val_categorical_accuracy: 0.6852\n",
            "Epoch 194/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5829 - categorical_accuracy: 0.6784 - val_loss: 0.8047 - val_categorical_accuracy: 0.6852\n",
            "Epoch 195/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5829 - categorical_accuracy: 0.6784 - val_loss: 0.8081 - val_categorical_accuracy: 0.6852\n",
            "Epoch 196/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5827 - categorical_accuracy: 0.6784 - val_loss: 0.8098 - val_categorical_accuracy: 0.6852\n",
            "Epoch 197/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5826 - categorical_accuracy: 0.6784 - val_loss: 0.8114 - val_categorical_accuracy: 0.6852\n",
            "Epoch 198/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5825 - categorical_accuracy: 0.6784 - val_loss: 0.8128 - val_categorical_accuracy: 0.6852\n",
            "Epoch 199/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5824 - categorical_accuracy: 0.6784 - val_loss: 0.8152 - val_categorical_accuracy: 0.6852\n",
            "Epoch 200/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5824 - categorical_accuracy: 0.6784 - val_loss: 0.8173 - val_categorical_accuracy: 0.6852\n",
            "Epoch 201/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5823 - categorical_accuracy: 0.6784 - val_loss: 0.8179 - val_categorical_accuracy: 0.6852\n",
            "Epoch 202/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5820 - categorical_accuracy: 0.6784 - val_loss: 0.8166 - val_categorical_accuracy: 0.6852\n",
            "Epoch 203/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5821 - categorical_accuracy: 0.6784 - val_loss: 0.8133 - val_categorical_accuracy: 0.6852\n",
            "Epoch 204/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5819 - categorical_accuracy: 0.6784 - val_loss: 0.8113 - val_categorical_accuracy: 0.6852\n",
            "Epoch 205/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5818 - categorical_accuracy: 0.6784 - val_loss: 0.8078 - val_categorical_accuracy: 0.6852\n",
            "Epoch 206/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5819 - categorical_accuracy: 0.6784 - val_loss: 0.8038 - val_categorical_accuracy: 0.6852\n",
            "Epoch 207/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5819 - categorical_accuracy: 0.6784 - val_loss: 0.8037 - val_categorical_accuracy: 0.6852\n",
            "Epoch 208/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5819 - categorical_accuracy: 0.6784 - val_loss: 0.8043 - val_categorical_accuracy: 0.6852\n",
            "Epoch 209/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5817 - categorical_accuracy: 0.6784 - val_loss: 0.8068 - val_categorical_accuracy: 0.6852\n",
            "Epoch 210/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5815 - categorical_accuracy: 0.6784 - val_loss: 0.8104 - val_categorical_accuracy: 0.6852\n",
            "Epoch 211/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5813 - categorical_accuracy: 0.6784 - val_loss: 0.8155 - val_categorical_accuracy: 0.6852\n",
            "Epoch 212/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5812 - categorical_accuracy: 0.6784 - val_loss: 0.8191 - val_categorical_accuracy: 0.6852\n",
            "Epoch 213/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5811 - categorical_accuracy: 0.6784 - val_loss: 0.8213 - val_categorical_accuracy: 0.6852\n",
            "Epoch 214/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5811 - categorical_accuracy: 0.6784 - val_loss: 0.8239 - val_categorical_accuracy: 0.6852\n",
            "Epoch 215/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5810 - categorical_accuracy: 0.6784 - val_loss: 0.8244 - val_categorical_accuracy: 0.6852\n",
            "Epoch 216/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5810 - categorical_accuracy: 0.6784 - val_loss: 0.8234 - val_categorical_accuracy: 0.6852\n",
            "Epoch 217/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5808 - categorical_accuracy: 0.6784 - val_loss: 0.8234 - val_categorical_accuracy: 0.6852\n",
            "Epoch 218/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5806 - categorical_accuracy: 0.6784 - val_loss: 0.8242 - val_categorical_accuracy: 0.6852\n",
            "Epoch 219/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5805 - categorical_accuracy: 0.6784 - val_loss: 0.8232 - val_categorical_accuracy: 0.6852\n",
            "Epoch 220/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5803 - categorical_accuracy: 0.6784 - val_loss: 0.8229 - val_categorical_accuracy: 0.6852\n",
            "Epoch 221/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5803 - categorical_accuracy: 0.6784 - val_loss: 0.8218 - val_categorical_accuracy: 0.6852\n",
            "Epoch 222/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5802 - categorical_accuracy: 0.6784 - val_loss: 0.8221 - val_categorical_accuracy: 0.6852\n",
            "Epoch 223/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5801 - categorical_accuracy: 0.6784 - val_loss: 0.8229 - val_categorical_accuracy: 0.6852\n",
            "Epoch 224/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5799 - categorical_accuracy: 0.6784 - val_loss: 0.8237 - val_categorical_accuracy: 0.6852\n",
            "Epoch 225/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5798 - categorical_accuracy: 0.6784 - val_loss: 0.8270 - val_categorical_accuracy: 0.6852\n",
            "Epoch 226/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5798 - categorical_accuracy: 0.6784 - val_loss: 0.8290 - val_categorical_accuracy: 0.6852\n",
            "Epoch 227/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5797 - categorical_accuracy: 0.6784 - val_loss: 0.8301 - val_categorical_accuracy: 0.6852\n",
            "Epoch 228/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5796 - categorical_accuracy: 0.6784 - val_loss: 0.8309 - val_categorical_accuracy: 0.6852\n",
            "Epoch 229/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5795 - categorical_accuracy: 0.6784 - val_loss: 0.8328 - val_categorical_accuracy: 0.6852\n",
            "Epoch 230/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5796 - categorical_accuracy: 0.6784 - val_loss: 0.8337 - val_categorical_accuracy: 0.6852\n",
            "Epoch 231/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5794 - categorical_accuracy: 0.6784 - val_loss: 0.8314 - val_categorical_accuracy: 0.6852\n",
            "Epoch 232/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5792 - categorical_accuracy: 0.6784 - val_loss: 0.8293 - val_categorical_accuracy: 0.6852\n",
            "Epoch 233/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5789 - categorical_accuracy: 0.6784 - val_loss: 0.8274 - val_categorical_accuracy: 0.6852\n",
            "Epoch 234/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5788 - categorical_accuracy: 0.6784 - val_loss: 0.8255 - val_categorical_accuracy: 0.6852\n",
            "Epoch 235/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5786 - categorical_accuracy: 0.6784 - val_loss: 0.8229 - val_categorical_accuracy: 0.6852\n",
            "Epoch 236/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5788 - categorical_accuracy: 0.6784 - val_loss: 0.8191 - val_categorical_accuracy: 0.6852\n",
            "Epoch 237/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5787 - categorical_accuracy: 0.6784 - val_loss: 0.8187 - val_categorical_accuracy: 0.6852\n",
            "Epoch 238/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5786 - categorical_accuracy: 0.6784 - val_loss: 0.8194 - val_categorical_accuracy: 0.6852\n",
            "Epoch 239/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5785 - categorical_accuracy: 0.6784 - val_loss: 0.8209 - val_categorical_accuracy: 0.6852\n",
            "Epoch 240/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5784 - categorical_accuracy: 0.6784 - val_loss: 0.8237 - val_categorical_accuracy: 0.6852\n",
            "Epoch 241/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5782 - categorical_accuracy: 0.6784 - val_loss: 0.8256 - val_categorical_accuracy: 0.6852\n",
            "Epoch 242/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5781 - categorical_accuracy: 0.6784 - val_loss: 0.8270 - val_categorical_accuracy: 0.6852\n",
            "Epoch 243/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5780 - categorical_accuracy: 0.6784 - val_loss: 0.8281 - val_categorical_accuracy: 0.6852\n",
            "Epoch 244/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5778 - categorical_accuracy: 0.6784 - val_loss: 0.8300 - val_categorical_accuracy: 0.6852\n",
            "Epoch 245/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5779 - categorical_accuracy: 0.6784 - val_loss: 0.8315 - val_categorical_accuracy: 0.6852\n",
            "Epoch 246/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5778 - categorical_accuracy: 0.6784 - val_loss: 0.8297 - val_categorical_accuracy: 0.6852\n",
            "Epoch 247/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5778 - categorical_accuracy: 0.6784 - val_loss: 0.8307 - val_categorical_accuracy: 0.6852\n",
            "Epoch 248/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5775 - categorical_accuracy: 0.6784 - val_loss: 0.8302 - val_categorical_accuracy: 0.6852\n",
            "Epoch 249/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5775 - categorical_accuracy: 0.6784 - val_loss: 0.8293 - val_categorical_accuracy: 0.6852\n",
            "Epoch 250/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5773 - categorical_accuracy: 0.6784 - val_loss: 0.8301 - val_categorical_accuracy: 0.6852\n",
            "Epoch 251/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5772 - categorical_accuracy: 0.6784 - val_loss: 0.8311 - val_categorical_accuracy: 0.6852\n",
            "Epoch 252/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5772 - categorical_accuracy: 0.6784 - val_loss: 0.8327 - val_categorical_accuracy: 0.6852\n",
            "Epoch 253/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5771 - categorical_accuracy: 0.6784 - val_loss: 0.8321 - val_categorical_accuracy: 0.6852\n",
            "Epoch 254/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5770 - categorical_accuracy: 0.6784 - val_loss: 0.8326 - val_categorical_accuracy: 0.6852\n",
            "Epoch 255/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5769 - categorical_accuracy: 0.6784 - val_loss: 0.8314 - val_categorical_accuracy: 0.6852\n",
            "Epoch 256/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5768 - categorical_accuracy: 0.6784 - val_loss: 0.8267 - val_categorical_accuracy: 0.6852\n",
            "Epoch 257/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5766 - categorical_accuracy: 0.6784 - val_loss: 0.8244 - val_categorical_accuracy: 0.6852\n",
            "Epoch 258/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5768 - categorical_accuracy: 0.6784 - val_loss: 0.8238 - val_categorical_accuracy: 0.6852\n",
            "Epoch 259/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5767 - categorical_accuracy: 0.6784 - val_loss: 0.8250 - val_categorical_accuracy: 0.6852\n",
            "Epoch 260/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5766 - categorical_accuracy: 0.6784 - val_loss: 0.8270 - val_categorical_accuracy: 0.6852\n",
            "Epoch 261/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5765 - categorical_accuracy: 0.6784 - val_loss: 0.8296 - val_categorical_accuracy: 0.6852\n",
            "Epoch 262/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5764 - categorical_accuracy: 0.6784 - val_loss: 0.8304 - val_categorical_accuracy: 0.6852\n",
            "Epoch 263/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5763 - categorical_accuracy: 0.6784 - val_loss: 0.8327 - val_categorical_accuracy: 0.6852\n",
            "Epoch 264/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5761 - categorical_accuracy: 0.6784 - val_loss: 0.8346 - val_categorical_accuracy: 0.6852\n",
            "Epoch 265/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5761 - categorical_accuracy: 0.6784 - val_loss: 0.8374 - val_categorical_accuracy: 0.6852\n",
            "Epoch 266/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5759 - categorical_accuracy: 0.6784 - val_loss: 0.8390 - val_categorical_accuracy: 0.6852\n",
            "Epoch 267/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5759 - categorical_accuracy: 0.6784 - val_loss: 0.8409 - val_categorical_accuracy: 0.6852\n",
            "Epoch 268/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5757 - categorical_accuracy: 0.6784 - val_loss: 0.8414 - val_categorical_accuracy: 0.6852\n",
            "Epoch 269/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5757 - categorical_accuracy: 0.6784 - val_loss: 0.8406 - val_categorical_accuracy: 0.6852\n",
            "Epoch 270/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5755 - categorical_accuracy: 0.6784 - val_loss: 0.8411 - val_categorical_accuracy: 0.6852\n",
            "Epoch 271/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5754 - categorical_accuracy: 0.6784 - val_loss: 0.8382 - val_categorical_accuracy: 0.6852\n",
            "Epoch 272/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5753 - categorical_accuracy: 0.6784 - val_loss: 0.8360 - val_categorical_accuracy: 0.6296\n",
            "Epoch 273/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5753 - categorical_accuracy: 0.6680 - val_loss: 0.8352 - val_categorical_accuracy: 0.6296\n",
            "Epoch 274/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5750 - categorical_accuracy: 0.6680 - val_loss: 0.8377 - val_categorical_accuracy: 0.6296\n",
            "Epoch 275/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5749 - categorical_accuracy: 0.6680 - val_loss: 0.8394 - val_categorical_accuracy: 0.6852\n",
            "Epoch 276/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5749 - categorical_accuracy: 0.6784 - val_loss: 0.8427 - val_categorical_accuracy: 0.6852\n",
            "Epoch 277/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5748 - categorical_accuracy: 0.6784 - val_loss: 0.8434 - val_categorical_accuracy: 0.6852\n",
            "Epoch 278/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5748 - categorical_accuracy: 0.6784 - val_loss: 0.8424 - val_categorical_accuracy: 0.6852\n",
            "Epoch 279/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5746 - categorical_accuracy: 0.6826 - val_loss: 0.8412 - val_categorical_accuracy: 0.6296\n",
            "Epoch 280/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5745 - categorical_accuracy: 0.6680 - val_loss: 0.8408 - val_categorical_accuracy: 0.6296\n",
            "Epoch 281/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5744 - categorical_accuracy: 0.6680 - val_loss: 0.8374 - val_categorical_accuracy: 0.6296\n",
            "Epoch 282/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5744 - categorical_accuracy: 0.6680 - val_loss: 0.8356 - val_categorical_accuracy: 0.6296\n",
            "Epoch 283/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5744 - categorical_accuracy: 0.6680 - val_loss: 0.8332 - val_categorical_accuracy: 0.6296\n",
            "Epoch 284/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5745 - categorical_accuracy: 0.6680 - val_loss: 0.8326 - val_categorical_accuracy: 0.6296\n",
            "Epoch 285/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5743 - categorical_accuracy: 0.6680 - val_loss: 0.8340 - val_categorical_accuracy: 0.6296\n",
            "Epoch 286/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5742 - categorical_accuracy: 0.6680 - val_loss: 0.8350 - val_categorical_accuracy: 0.6296\n",
            "Epoch 287/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5739 - categorical_accuracy: 0.6680 - val_loss: 0.8394 - val_categorical_accuracy: 0.6296\n",
            "Epoch 288/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5739 - categorical_accuracy: 0.6680 - val_loss: 0.8436 - val_categorical_accuracy: 0.6296\n",
            "Epoch 289/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5737 - categorical_accuracy: 0.6701 - val_loss: 0.8442 - val_categorical_accuracy: 0.6296\n",
            "Epoch 290/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5736 - categorical_accuracy: 0.6680 - val_loss: 0.8460 - val_categorical_accuracy: 0.6481\n",
            "Epoch 291/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5734 - categorical_accuracy: 0.6701 - val_loss: 0.8477 - val_categorical_accuracy: 0.6481\n",
            "Epoch 292/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5736 - categorical_accuracy: 0.6701 - val_loss: 0.8494 - val_categorical_accuracy: 0.6481\n",
            "Epoch 293/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5733 - categorical_accuracy: 0.6701 - val_loss: 0.8473 - val_categorical_accuracy: 0.6481\n",
            "Epoch 294/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5732 - categorical_accuracy: 0.6701 - val_loss: 0.8449 - val_categorical_accuracy: 0.6481\n",
            "Epoch 295/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5731 - categorical_accuracy: 0.6701 - val_loss: 0.8430 - val_categorical_accuracy: 0.6481\n",
            "Epoch 296/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5730 - categorical_accuracy: 0.6701 - val_loss: 0.8414 - val_categorical_accuracy: 0.6481\n",
            "Epoch 297/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5729 - categorical_accuracy: 0.6701 - val_loss: 0.8426 - val_categorical_accuracy: 0.6481\n",
            "Epoch 298/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5728 - categorical_accuracy: 0.6701 - val_loss: 0.8414 - val_categorical_accuracy: 0.6481\n",
            "Epoch 299/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5725 - categorical_accuracy: 0.6722 - val_loss: 0.8374 - val_categorical_accuracy: 0.6481\n",
            "Epoch 300/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5728 - categorical_accuracy: 0.6701 - val_loss: 0.8329 - val_categorical_accuracy: 0.6481\n",
            "Epoch 301/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5727 - categorical_accuracy: 0.6701 - val_loss: 0.8293 - val_categorical_accuracy: 0.6481\n",
            "Epoch 302/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5730 - categorical_accuracy: 0.6701 - val_loss: 0.8276 - val_categorical_accuracy: 0.6481\n",
            "Epoch 303/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5730 - categorical_accuracy: 0.6701 - val_loss: 0.8278 - val_categorical_accuracy: 0.6481\n",
            "Epoch 304/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5730 - categorical_accuracy: 0.6701 - val_loss: 0.8285 - val_categorical_accuracy: 0.6481\n",
            "Epoch 305/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5730 - categorical_accuracy: 0.6701 - val_loss: 0.8320 - val_categorical_accuracy: 0.6481\n",
            "Epoch 306/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5726 - categorical_accuracy: 0.6701 - val_loss: 0.8365 - val_categorical_accuracy: 0.6481\n",
            "Epoch 307/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5722 - categorical_accuracy: 0.6701 - val_loss: 0.8395 - val_categorical_accuracy: 0.6481\n",
            "Epoch 308/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5720 - categorical_accuracy: 0.6701 - val_loss: 0.8417 - val_categorical_accuracy: 0.6481\n",
            "Epoch 309/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5719 - categorical_accuracy: 0.6701 - val_loss: 0.8430 - val_categorical_accuracy: 0.6481\n",
            "Epoch 310/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5718 - categorical_accuracy: 0.6701 - val_loss: 0.8433 - val_categorical_accuracy: 0.6481\n",
            "Epoch 311/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5716 - categorical_accuracy: 0.6701 - val_loss: 0.8439 - val_categorical_accuracy: 0.6481\n",
            "Epoch 312/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5715 - categorical_accuracy: 0.6701 - val_loss: 0.8442 - val_categorical_accuracy: 0.6481\n",
            "Epoch 313/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5714 - categorical_accuracy: 0.6701 - val_loss: 0.8446 - val_categorical_accuracy: 0.6481\n",
            "Epoch 314/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5713 - categorical_accuracy: 0.6701 - val_loss: 0.8445 - val_categorical_accuracy: 0.6481\n",
            "Epoch 315/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5713 - categorical_accuracy: 0.6701 - val_loss: 0.8437 - val_categorical_accuracy: 0.6481\n",
            "Epoch 316/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5711 - categorical_accuracy: 0.6701 - val_loss: 0.8449 - val_categorical_accuracy: 0.6481\n",
            "Epoch 317/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5711 - categorical_accuracy: 0.6701 - val_loss: 0.8450 - val_categorical_accuracy: 0.6481\n",
            "Epoch 318/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5709 - categorical_accuracy: 0.6701 - val_loss: 0.8448 - val_categorical_accuracy: 0.6481\n",
            "Epoch 319/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5708 - categorical_accuracy: 0.6743 - val_loss: 0.8458 - val_categorical_accuracy: 0.6481\n",
            "Epoch 320/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5707 - categorical_accuracy: 0.6743 - val_loss: 0.8478 - val_categorical_accuracy: 0.6481\n",
            "Epoch 321/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5706 - categorical_accuracy: 0.6743 - val_loss: 0.8498 - val_categorical_accuracy: 0.6481\n",
            "Epoch 322/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5705 - categorical_accuracy: 0.6743 - val_loss: 0.8507 - val_categorical_accuracy: 0.6481\n",
            "Epoch 323/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5704 - categorical_accuracy: 0.6743 - val_loss: 0.8518 - val_categorical_accuracy: 0.6481\n",
            "Epoch 324/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5703 - categorical_accuracy: 0.6743 - val_loss: 0.8524 - val_categorical_accuracy: 0.6481\n",
            "Epoch 325/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5703 - categorical_accuracy: 0.6743 - val_loss: 0.8521 - val_categorical_accuracy: 0.6481\n",
            "Epoch 326/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5703 - categorical_accuracy: 0.6743 - val_loss: 0.8527 - val_categorical_accuracy: 0.6481\n",
            "Epoch 327/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5703 - categorical_accuracy: 0.6743 - val_loss: 0.8545 - val_categorical_accuracy: 0.6481\n",
            "Epoch 328/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5701 - categorical_accuracy: 0.6743 - val_loss: 0.8566 - val_categorical_accuracy: 0.6481\n",
            "Epoch 329/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5701 - categorical_accuracy: 0.6743 - val_loss: 0.8577 - val_categorical_accuracy: 0.6481\n",
            "Epoch 330/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5700 - categorical_accuracy: 0.6743 - val_loss: 0.8564 - val_categorical_accuracy: 0.6481\n",
            "Epoch 331/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5698 - categorical_accuracy: 0.6743 - val_loss: 0.8558 - val_categorical_accuracy: 0.6481\n",
            "Epoch 332/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5697 - categorical_accuracy: 0.6743 - val_loss: 0.8535 - val_categorical_accuracy: 0.6481\n",
            "Epoch 333/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5696 - categorical_accuracy: 0.6743 - val_loss: 0.8500 - val_categorical_accuracy: 0.6481\n",
            "Epoch 334/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5697 - categorical_accuracy: 0.6743 - val_loss: 0.8477 - val_categorical_accuracy: 0.6481\n",
            "Epoch 335/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5696 - categorical_accuracy: 0.6743 - val_loss: 0.8481 - val_categorical_accuracy: 0.6481\n",
            "Epoch 336/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5695 - categorical_accuracy: 0.6743 - val_loss: 0.8483 - val_categorical_accuracy: 0.6481\n",
            "Epoch 337/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5694 - categorical_accuracy: 0.6743 - val_loss: 0.8478 - val_categorical_accuracy: 0.6481\n",
            "Epoch 338/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5693 - categorical_accuracy: 0.6743 - val_loss: 0.8489 - val_categorical_accuracy: 0.6481\n",
            "Epoch 339/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5693 - categorical_accuracy: 0.6743 - val_loss: 0.8505 - val_categorical_accuracy: 0.6481\n",
            "Epoch 340/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5692 - categorical_accuracy: 0.6743 - val_loss: 0.8525 - val_categorical_accuracy: 0.6481\n",
            "Epoch 341/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5690 - categorical_accuracy: 0.6743 - val_loss: 0.8531 - val_categorical_accuracy: 0.6481\n",
            "Epoch 342/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5689 - categorical_accuracy: 0.6743 - val_loss: 0.8528 - val_categorical_accuracy: 0.6481\n",
            "Epoch 343/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5688 - categorical_accuracy: 0.6743 - val_loss: 0.8507 - val_categorical_accuracy: 0.6481\n",
            "Epoch 344/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5689 - categorical_accuracy: 0.6743 - val_loss: 0.8483 - val_categorical_accuracy: 0.6481\n",
            "Epoch 345/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5689 - categorical_accuracy: 0.6743 - val_loss: 0.8472 - val_categorical_accuracy: 0.6481\n",
            "Epoch 346/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5686 - categorical_accuracy: 0.6722 - val_loss: 0.8480 - val_categorical_accuracy: 0.6481\n",
            "Epoch 347/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5686 - categorical_accuracy: 0.6743 - val_loss: 0.8507 - val_categorical_accuracy: 0.6481\n",
            "Epoch 348/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5685 - categorical_accuracy: 0.6743 - val_loss: 0.8519 - val_categorical_accuracy: 0.6481\n",
            "Epoch 349/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5683 - categorical_accuracy: 0.6743 - val_loss: 0.8510 - val_categorical_accuracy: 0.6481\n",
            "Epoch 350/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5683 - categorical_accuracy: 0.6743 - val_loss: 0.8499 - val_categorical_accuracy: 0.6481\n",
            "Epoch 351/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5682 - categorical_accuracy: 0.6722 - val_loss: 0.8506 - val_categorical_accuracy: 0.6481\n",
            "Epoch 352/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5682 - categorical_accuracy: 0.6743 - val_loss: 0.8503 - val_categorical_accuracy: 0.6481\n",
            "Epoch 353/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5681 - categorical_accuracy: 0.6743 - val_loss: 0.8503 - val_categorical_accuracy: 0.6481\n",
            "Epoch 354/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5680 - categorical_accuracy: 0.6743 - val_loss: 0.8517 - val_categorical_accuracy: 0.6481\n",
            "Epoch 355/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5679 - categorical_accuracy: 0.6743 - val_loss: 0.8536 - val_categorical_accuracy: 0.6481\n",
            "Epoch 356/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5678 - categorical_accuracy: 0.6743 - val_loss: 0.8561 - val_categorical_accuracy: 0.6481\n",
            "Epoch 357/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5678 - categorical_accuracy: 0.6743 - val_loss: 0.8577 - val_categorical_accuracy: 0.6481\n",
            "Epoch 358/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5676 - categorical_accuracy: 0.6743 - val_loss: 0.8564 - val_categorical_accuracy: 0.6481\n",
            "Epoch 359/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5676 - categorical_accuracy: 0.6743 - val_loss: 0.8526 - val_categorical_accuracy: 0.6481\n",
            "Epoch 360/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5678 - categorical_accuracy: 0.6743 - val_loss: 0.8489 - val_categorical_accuracy: 0.6481\n",
            "Epoch 361/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5676 - categorical_accuracy: 0.6743 - val_loss: 0.8482 - val_categorical_accuracy: 0.6481\n",
            "Epoch 362/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5677 - categorical_accuracy: 0.6743 - val_loss: 0.8491 - val_categorical_accuracy: 0.6481\n",
            "Epoch 363/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5675 - categorical_accuracy: 0.6743 - val_loss: 0.8511 - val_categorical_accuracy: 0.6481\n",
            "Epoch 364/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5672 - categorical_accuracy: 0.6743 - val_loss: 0.8533 - val_categorical_accuracy: 0.6481\n",
            "Epoch 365/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5672 - categorical_accuracy: 0.6743 - val_loss: 0.8540 - val_categorical_accuracy: 0.6481\n",
            "Epoch 366/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5671 - categorical_accuracy: 0.6743 - val_loss: 0.8549 - val_categorical_accuracy: 0.6481\n",
            "Epoch 367/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5670 - categorical_accuracy: 0.6743 - val_loss: 0.8533 - val_categorical_accuracy: 0.6481\n",
            "Epoch 368/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5672 - categorical_accuracy: 0.6743 - val_loss: 0.8507 - val_categorical_accuracy: 0.6481\n",
            "Epoch 369/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5671 - categorical_accuracy: 0.6743 - val_loss: 0.8504 - val_categorical_accuracy: 0.6481\n",
            "Epoch 370/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5669 - categorical_accuracy: 0.6743 - val_loss: 0.8522 - val_categorical_accuracy: 0.6481\n",
            "Epoch 371/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5667 - categorical_accuracy: 0.6743 - val_loss: 0.8562 - val_categorical_accuracy: 0.6481\n",
            "Epoch 372/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5665 - categorical_accuracy: 0.6743 - val_loss: 0.8595 - val_categorical_accuracy: 0.6481\n",
            "Epoch 373/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5668 - categorical_accuracy: 0.6743 - val_loss: 0.8629 - val_categorical_accuracy: 0.6481\n",
            "Epoch 374/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5669 - categorical_accuracy: 0.6743 - val_loss: 0.8649 - val_categorical_accuracy: 0.6481\n",
            "Epoch 375/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5670 - categorical_accuracy: 0.6743 - val_loss: 0.8638 - val_categorical_accuracy: 0.6481\n",
            "Epoch 376/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5667 - categorical_accuracy: 0.6743 - val_loss: 0.8626 - val_categorical_accuracy: 0.6481\n",
            "Epoch 377/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5667 - categorical_accuracy: 0.6743 - val_loss: 0.8607 - val_categorical_accuracy: 0.6481\n",
            "Epoch 378/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5665 - categorical_accuracy: 0.6743 - val_loss: 0.8570 - val_categorical_accuracy: 0.6481\n",
            "Epoch 379/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5665 - categorical_accuracy: 0.6743 - val_loss: 0.8559 - val_categorical_accuracy: 0.6481\n",
            "Epoch 380/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5662 - categorical_accuracy: 0.6743 - val_loss: 0.8515 - val_categorical_accuracy: 0.6481\n",
            "Epoch 381/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5660 - categorical_accuracy: 0.6743 - val_loss: 0.8490 - val_categorical_accuracy: 0.6481\n",
            "Epoch 382/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5659 - categorical_accuracy: 0.6743 - val_loss: 0.8467 - val_categorical_accuracy: 0.6481\n",
            "Epoch 383/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5659 - categorical_accuracy: 0.6743 - val_loss: 0.8450 - val_categorical_accuracy: 0.6481\n",
            "Epoch 384/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5659 - categorical_accuracy: 0.6743 - val_loss: 0.8443 - val_categorical_accuracy: 0.6481\n",
            "Epoch 385/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5657 - categorical_accuracy: 0.6826 - val_loss: 0.8452 - val_categorical_accuracy: 0.6481\n",
            "Epoch 386/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5656 - categorical_accuracy: 0.6826 - val_loss: 0.8477 - val_categorical_accuracy: 0.6481\n",
            "Epoch 387/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5654 - categorical_accuracy: 0.6826 - val_loss: 0.8509 - val_categorical_accuracy: 0.6481\n",
            "Epoch 388/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5652 - categorical_accuracy: 0.6826 - val_loss: 0.8519 - val_categorical_accuracy: 0.6481\n",
            "Epoch 389/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5651 - categorical_accuracy: 0.6826 - val_loss: 0.8519 - val_categorical_accuracy: 0.6481\n",
            "Epoch 390/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5649 - categorical_accuracy: 0.6826 - val_loss: 0.8493 - val_categorical_accuracy: 0.6481\n",
            "Epoch 391/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5650 - categorical_accuracy: 0.6826 - val_loss: 0.8489 - val_categorical_accuracy: 0.6481\n",
            "Epoch 392/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5649 - categorical_accuracy: 0.6826 - val_loss: 0.8508 - val_categorical_accuracy: 0.6481\n",
            "Epoch 393/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5646 - categorical_accuracy: 0.6826 - val_loss: 0.8538 - val_categorical_accuracy: 0.6481\n",
            "Epoch 394/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5647 - categorical_accuracy: 0.6826 - val_loss: 0.8586 - val_categorical_accuracy: 0.6481\n",
            "Epoch 395/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5642 - categorical_accuracy: 0.6826 - val_loss: 0.8626 - val_categorical_accuracy: 0.6481\n",
            "Epoch 396/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5640 - categorical_accuracy: 0.6826 - val_loss: 0.8647 - val_categorical_accuracy: 0.6481\n",
            "Epoch 397/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5639 - categorical_accuracy: 0.6826 - val_loss: 0.8675 - val_categorical_accuracy: 0.6481\n",
            "Epoch 398/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5639 - categorical_accuracy: 0.6826 - val_loss: 0.8675 - val_categorical_accuracy: 0.6481\n",
            "Epoch 399/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5634 - categorical_accuracy: 0.6867 - val_loss: 0.8656 - val_categorical_accuracy: 0.6481\n",
            "Epoch 400/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5632 - categorical_accuracy: 0.6867 - val_loss: 0.8636 - val_categorical_accuracy: 0.6481\n",
            "Epoch 401/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5630 - categorical_accuracy: 0.6867 - val_loss: 0.8635 - val_categorical_accuracy: 0.6481\n",
            "Epoch 402/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5628 - categorical_accuracy: 0.6867 - val_loss: 0.8655 - val_categorical_accuracy: 0.6481\n",
            "Epoch 403/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5626 - categorical_accuracy: 0.6867 - val_loss: 0.8657 - val_categorical_accuracy: 0.6481\n",
            "Epoch 404/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5624 - categorical_accuracy: 0.6867 - val_loss: 0.8677 - val_categorical_accuracy: 0.6481\n",
            "Epoch 405/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5623 - categorical_accuracy: 0.6867 - val_loss: 0.8715 - val_categorical_accuracy: 0.6481\n",
            "Epoch 406/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5620 - categorical_accuracy: 0.6867 - val_loss: 0.8722 - val_categorical_accuracy: 0.6481\n",
            "Epoch 407/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5618 - categorical_accuracy: 0.6867 - val_loss: 0.8720 - val_categorical_accuracy: 0.6481\n",
            "Epoch 408/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5617 - categorical_accuracy: 0.6867 - val_loss: 0.8720 - val_categorical_accuracy: 0.6481\n",
            "Epoch 409/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5615 - categorical_accuracy: 0.6867 - val_loss: 0.8736 - val_categorical_accuracy: 0.6481\n",
            "Epoch 410/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5613 - categorical_accuracy: 0.6867 - val_loss: 0.8726 - val_categorical_accuracy: 0.6481\n",
            "Epoch 411/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5604 - categorical_accuracy: 0.6867 - val_loss: 0.8721 - val_categorical_accuracy: 0.6481\n",
            "Epoch 412/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5598 - categorical_accuracy: 0.6867 - val_loss: 0.8712 - val_categorical_accuracy: 0.6481\n",
            "Epoch 413/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5590 - categorical_accuracy: 0.6867 - val_loss: 0.8707 - val_categorical_accuracy: 0.6481\n",
            "Epoch 414/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5580 - categorical_accuracy: 0.6909 - val_loss: 0.8679 - val_categorical_accuracy: 0.6667\n",
            "Epoch 415/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5574 - categorical_accuracy: 0.6929 - val_loss: 0.8656 - val_categorical_accuracy: 0.6852\n",
            "Epoch 416/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5566 - categorical_accuracy: 0.6971 - val_loss: 0.8629 - val_categorical_accuracy: 0.6852\n",
            "Epoch 417/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5558 - categorical_accuracy: 0.6971 - val_loss: 0.8634 - val_categorical_accuracy: 0.6667\n",
            "Epoch 418/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5551 - categorical_accuracy: 0.6929 - val_loss: 0.8662 - val_categorical_accuracy: 0.6667\n",
            "Epoch 419/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5544 - categorical_accuracy: 0.6929 - val_loss: 0.8695 - val_categorical_accuracy: 0.6667\n",
            "Epoch 420/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5538 - categorical_accuracy: 0.6929 - val_loss: 0.8728 - val_categorical_accuracy: 0.6667\n",
            "Epoch 421/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5533 - categorical_accuracy: 0.6929 - val_loss: 0.8730 - val_categorical_accuracy: 0.6667\n",
            "Epoch 422/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5523 - categorical_accuracy: 0.6929 - val_loss: 0.8724 - val_categorical_accuracy: 0.6667\n",
            "Epoch 423/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5517 - categorical_accuracy: 0.6929 - val_loss: 0.8734 - val_categorical_accuracy: 0.6667\n",
            "Epoch 424/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5514 - categorical_accuracy: 0.6929 - val_loss: 0.8754 - val_categorical_accuracy: 0.6667\n",
            "Epoch 425/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5510 - categorical_accuracy: 0.6929 - val_loss: 0.8731 - val_categorical_accuracy: 0.6667\n",
            "Epoch 426/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5504 - categorical_accuracy: 0.6929 - val_loss: 0.8677 - val_categorical_accuracy: 0.6667\n",
            "Epoch 427/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5498 - categorical_accuracy: 0.6929 - val_loss: 0.8628 - val_categorical_accuracy: 0.6667\n",
            "Epoch 428/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5498 - categorical_accuracy: 0.6929 - val_loss: 0.8592 - val_categorical_accuracy: 0.6852\n",
            "Epoch 429/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5494 - categorical_accuracy: 0.6950 - val_loss: 0.8584 - val_categorical_accuracy: 0.6667\n",
            "Epoch 430/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5492 - categorical_accuracy: 0.6992 - val_loss: 0.8598 - val_categorical_accuracy: 0.6667\n",
            "Epoch 431/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5490 - categorical_accuracy: 0.6992 - val_loss: 0.8639 - val_categorical_accuracy: 0.6667\n",
            "Epoch 432/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5485 - categorical_accuracy: 0.6929 - val_loss: 0.8677 - val_categorical_accuracy: 0.6667\n",
            "Epoch 433/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5486 - categorical_accuracy: 0.6929 - val_loss: 0.8723 - val_categorical_accuracy: 0.6667\n",
            "Epoch 434/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5484 - categorical_accuracy: 0.6929 - val_loss: 0.8736 - val_categorical_accuracy: 0.6667\n",
            "Epoch 435/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5480 - categorical_accuracy: 0.6929 - val_loss: 0.8739 - val_categorical_accuracy: 0.6667\n",
            "Epoch 436/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5478 - categorical_accuracy: 0.6929 - val_loss: 0.8723 - val_categorical_accuracy: 0.6667\n",
            "Epoch 437/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5477 - categorical_accuracy: 0.6929 - val_loss: 0.8701 - val_categorical_accuracy: 0.6667\n",
            "Epoch 438/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5472 - categorical_accuracy: 0.6971 - val_loss: 0.8619 - val_categorical_accuracy: 0.6667\n",
            "Epoch 439/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5467 - categorical_accuracy: 0.6992 - val_loss: 0.8568 - val_categorical_accuracy: 0.6667\n",
            "Epoch 440/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5472 - categorical_accuracy: 0.6992 - val_loss: 0.8557 - val_categorical_accuracy: 0.6667\n",
            "Epoch 441/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5466 - categorical_accuracy: 0.6992 - val_loss: 0.8580 - val_categorical_accuracy: 0.6667\n",
            "Epoch 442/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5462 - categorical_accuracy: 0.6992 - val_loss: 0.8613 - val_categorical_accuracy: 0.6667\n",
            "Epoch 443/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5460 - categorical_accuracy: 0.6992 - val_loss: 0.8651 - val_categorical_accuracy: 0.6667\n",
            "Epoch 444/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5458 - categorical_accuracy: 0.6992 - val_loss: 0.8695 - val_categorical_accuracy: 0.6667\n",
            "Epoch 445/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5457 - categorical_accuracy: 0.6992 - val_loss: 0.8740 - val_categorical_accuracy: 0.6667\n",
            "Epoch 446/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5452 - categorical_accuracy: 0.6992 - val_loss: 0.8759 - val_categorical_accuracy: 0.6667\n",
            "Epoch 447/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5450 - categorical_accuracy: 0.6992 - val_loss: 0.8760 - val_categorical_accuracy: 0.6667\n",
            "Epoch 448/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5447 - categorical_accuracy: 0.6992 - val_loss: 0.8721 - val_categorical_accuracy: 0.6667\n",
            "Epoch 449/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5444 - categorical_accuracy: 0.6992 - val_loss: 0.8700 - val_categorical_accuracy: 0.6667\n",
            "Epoch 450/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5441 - categorical_accuracy: 0.6992 - val_loss: 0.8713 - val_categorical_accuracy: 0.6667\n",
            "Epoch 451/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5439 - categorical_accuracy: 0.6992 - val_loss: 0.8720 - val_categorical_accuracy: 0.6667\n",
            "Epoch 452/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5437 - categorical_accuracy: 0.7054 - val_loss: 0.8705 - val_categorical_accuracy: 0.6852\n",
            "Epoch 453/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5436 - categorical_accuracy: 0.7033 - val_loss: 0.8677 - val_categorical_accuracy: 0.6852\n",
            "Epoch 454/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5432 - categorical_accuracy: 0.7033 - val_loss: 0.8657 - val_categorical_accuracy: 0.6852\n",
            "Epoch 455/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5435 - categorical_accuracy: 0.6992 - val_loss: 0.8653 - val_categorical_accuracy: 0.6667\n",
            "Epoch 456/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5430 - categorical_accuracy: 0.7054 - val_loss: 0.8661 - val_categorical_accuracy: 0.6852\n",
            "Epoch 457/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5429 - categorical_accuracy: 0.7033 - val_loss: 0.8630 - val_categorical_accuracy: 0.6852\n",
            "Epoch 458/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5425 - categorical_accuracy: 0.7033 - val_loss: 0.8650 - val_categorical_accuracy: 0.6852\n",
            "Epoch 459/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5424 - categorical_accuracy: 0.7033 - val_loss: 0.8701 - val_categorical_accuracy: 0.6852\n",
            "Epoch 460/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5418 - categorical_accuracy: 0.7033 - val_loss: 0.8686 - val_categorical_accuracy: 0.6852\n",
            "Epoch 461/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5417 - categorical_accuracy: 0.7178 - val_loss: 0.8649 - val_categorical_accuracy: 0.7037\n",
            "Epoch 462/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5416 - categorical_accuracy: 0.7220 - val_loss: 0.8643 - val_categorical_accuracy: 0.7037\n",
            "Epoch 463/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5414 - categorical_accuracy: 0.7220 - val_loss: 0.8645 - val_categorical_accuracy: 0.7037\n",
            "Epoch 464/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5411 - categorical_accuracy: 0.7220 - val_loss: 0.8663 - val_categorical_accuracy: 0.7037\n",
            "Epoch 465/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5409 - categorical_accuracy: 0.7220 - val_loss: 0.8696 - val_categorical_accuracy: 0.6852\n",
            "Epoch 466/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5413 - categorical_accuracy: 0.7178 - val_loss: 0.8746 - val_categorical_accuracy: 0.7222\n",
            "Epoch 467/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5410 - categorical_accuracy: 0.7158 - val_loss: 0.8743 - val_categorical_accuracy: 0.7037\n",
            "Epoch 468/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5405 - categorical_accuracy: 0.7199 - val_loss: 0.8764 - val_categorical_accuracy: 0.7037\n",
            "Epoch 469/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5399 - categorical_accuracy: 0.7199 - val_loss: 0.8767 - val_categorical_accuracy: 0.7037\n",
            "Epoch 470/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5397 - categorical_accuracy: 0.7241 - val_loss: 0.8724 - val_categorical_accuracy: 0.7037\n",
            "Epoch 471/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5392 - categorical_accuracy: 0.7241 - val_loss: 0.8725 - val_categorical_accuracy: 0.7037\n",
            "Epoch 472/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5386 - categorical_accuracy: 0.7241 - val_loss: 0.8738 - val_categorical_accuracy: 0.7037\n",
            "Epoch 473/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5384 - categorical_accuracy: 0.7241 - val_loss: 0.8744 - val_categorical_accuracy: 0.7037\n",
            "Epoch 474/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5381 - categorical_accuracy: 0.7241 - val_loss: 0.8794 - val_categorical_accuracy: 0.7037\n",
            "Epoch 475/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5381 - categorical_accuracy: 0.7199 - val_loss: 0.8807 - val_categorical_accuracy: 0.7037\n",
            "Epoch 476/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5371 - categorical_accuracy: 0.7220 - val_loss: 0.8804 - val_categorical_accuracy: 0.7037\n",
            "Epoch 477/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5371 - categorical_accuracy: 0.7220 - val_loss: 0.8795 - val_categorical_accuracy: 0.7037\n",
            "Epoch 478/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5366 - categorical_accuracy: 0.7241 - val_loss: 0.8738 - val_categorical_accuracy: 0.7037\n",
            "Epoch 479/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5369 - categorical_accuracy: 0.7241 - val_loss: 0.8698 - val_categorical_accuracy: 0.7037\n",
            "Epoch 480/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5370 - categorical_accuracy: 0.7220 - val_loss: 0.8726 - val_categorical_accuracy: 0.7037\n",
            "Epoch 481/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5373 - categorical_accuracy: 0.7199 - val_loss: 0.8751 - val_categorical_accuracy: 0.7037\n",
            "Epoch 482/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5366 - categorical_accuracy: 0.7220 - val_loss: 0.8765 - val_categorical_accuracy: 0.7037\n",
            "Epoch 483/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5360 - categorical_accuracy: 0.7241 - val_loss: 0.8761 - val_categorical_accuracy: 0.7037\n",
            "Epoch 484/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5360 - categorical_accuracy: 0.7241 - val_loss: 0.8741 - val_categorical_accuracy: 0.7037\n",
            "Epoch 485/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5358 - categorical_accuracy: 0.7199 - val_loss: 0.8715 - val_categorical_accuracy: 0.7037\n",
            "Epoch 486/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5357 - categorical_accuracy: 0.7178 - val_loss: 0.8674 - val_categorical_accuracy: 0.7037\n",
            "Epoch 487/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5357 - categorical_accuracy: 0.7178 - val_loss: 0.8664 - val_categorical_accuracy: 0.7037\n",
            "Epoch 488/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5355 - categorical_accuracy: 0.7241 - val_loss: 0.8667 - val_categorical_accuracy: 0.7037\n",
            "Epoch 489/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5355 - categorical_accuracy: 0.7241 - val_loss: 0.8712 - val_categorical_accuracy: 0.7037\n",
            "Epoch 490/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5352 - categorical_accuracy: 0.7220 - val_loss: 0.8787 - val_categorical_accuracy: 0.7037\n",
            "Epoch 491/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5350 - categorical_accuracy: 0.7220 - val_loss: 0.8865 - val_categorical_accuracy: 0.7037\n",
            "Epoch 492/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5351 - categorical_accuracy: 0.7220 - val_loss: 0.8916 - val_categorical_accuracy: 0.7037\n",
            "Epoch 493/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5350 - categorical_accuracy: 0.7261 - val_loss: 0.8905 - val_categorical_accuracy: 0.7037\n",
            "Epoch 494/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5347 - categorical_accuracy: 0.7241 - val_loss: 0.8908 - val_categorical_accuracy: 0.7037\n",
            "Epoch 495/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5346 - categorical_accuracy: 0.7241 - val_loss: 0.8941 - val_categorical_accuracy: 0.7037\n",
            "Epoch 496/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5345 - categorical_accuracy: 0.7241 - val_loss: 0.8963 - val_categorical_accuracy: 0.7037\n",
            "Epoch 497/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5343 - categorical_accuracy: 0.7241 - val_loss: 0.8980 - val_categorical_accuracy: 0.7037\n",
            "Epoch 498/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5338 - categorical_accuracy: 0.7241 - val_loss: 0.8993 - val_categorical_accuracy: 0.7037\n",
            "Epoch 499/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5345 - categorical_accuracy: 0.7220 - val_loss: 0.8987 - val_categorical_accuracy: 0.7037\n",
            "Epoch 500/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5339 - categorical_accuracy: 0.7220 - val_loss: 0.8984 - val_categorical_accuracy: 0.7037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLnxpNkCxFZ1",
        "colab_type": "code",
        "outputId": "669ce442-a7e2-4b26-b5bb-bb4698f5cdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test our model against the hidden one\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) # just by running a deep learning model with the goddamn ages and gender, wtf"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 150us/step\n",
            "Score: 0.49, Accuracy: 0.783333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p6xrWCyR0cO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7a6cbe4f-bcad-490c-9693-07c406a6697b"
      },
      "source": [
        "## BUILD CONFUSION MATRIX\n",
        "y_train_preds = as_model.predict([sex_train, ages_train])\n",
        "y_train_real = y_train\n",
        "\n",
        "y_test_preds = as_model.predict([sex_test, ages_test])\n",
        "y_test_real = y_test\n",
        "\n",
        "# Turn to binary outputs\n",
        "y_train_preds_binary = to_binary(y_train_preds)\n",
        "y_train_real_binary = to_binary(y_train_real)\n",
        "\n",
        "y_test_preds_binary = to_binary(y_test_preds)\n",
        "y_test_real_binary = to_binary(y_test_real)\n",
        "\n",
        "# MAKE CONFUSION MATRIX, can also do the same for train data\n",
        "print (confusion_matrix(y_train_real_binary, y_train_preds_binary))\n",
        "print (confusion_matrix(y_test_real_binary, y_test_preds_binary))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 34 138]\n",
            " [ 12 352]]\n",
            "[[ 4 10]\n",
            " [ 3 43]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysTchue5QU2i",
        "colab_type": "code",
        "outputId": "c0c35b92-57f3-427d-c8eb-60c77c49f73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "# save\n",
        "# plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_acc.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "# save\n",
        "# plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_val.png')\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcVNWZ//HP0xvdrM2qsjYiLuAC\nSlCjY8QVl0AmMQrGnzExcTKJiYlxEpyfY4zjJOpMFk2cvMaoUdQEzaIhGQw6kYxrRDC4ACqLIM1O\n083aTW/P/HFvVVdX9VLQXVXdfb/v16teXffeU/c+t2nuU+ece88xd0dERAQgL9cBiIhI16GkICIi\ncUoKIiISp6QgIiJxSgoiIhKnpCAiInFKChIJZlZmZm5mBWmUvcbMXspGXCJdjZKCdDlmts7Mas1s\nSNL6v4UX9rLcRCbS8ykpSFf1ATA7tmBmJwC9cxdO15BOTUekI5QUpKt6FLg6YfmzwNzEAmY2wMzm\nmtl2M1tvZreYWV64Ld/M/sPMdpjZWuCSFj77oJltNrONZnaHmeWnE5iZ/drMtpjZLjN7wcwmJmwr\nMbMfhPHsMrOXzKwk3Hammb1iZlVmtsHMrgnX/8XMvpCwj2bNV2Ht6CtmtgpYFa67J9zHbjNbamZ/\nl1A+38z+2czWmNmecPsoM7vPzH6QdC7zzewb6Zy3RIOSgnRVfwX6m9lx4cV6FvBYUpmfAAOAI4GP\nESSRz4XbvghcCkwGpgCXJX32YaAeOCoscwHwBdLzDDAeGAa8ATyesO0/gFOAjwKDgG8BjWY2Jvzc\nT4ChwCRgWZrHA/gEcCowIVx+PdzHIOCXwK/NrDjcdiNBLetioD/weWA/8AgwOyFxDgHOCz8vEnB3\nvfTqUi9gHcHF6hbg+8B04DmgAHCgDMgHaoEJCZ/7B+Av4fvngS8lbLsg/GwBcBhwAChJ2D4bWBS+\nvwZ4Kc1YS8P9DiD4klUNnNRCuZuBp1rZx1+ALyQsNzt+uP9z2omjMnZc4D1gZivlVgLnh++vBxbk\n+t9br671UvukdGWPAi8AY0lqOgKGAIXA+oR164ER4fvhwIakbTFjws9uNrPYuryk8i0Kay3/Bnya\n4Bt/Y0I8vYBiYE0LHx3Vyvp0NYvNzG4CriU4TyeoEcQ65ts61iPAVQRJ9irgng7EJD2Qmo+ky3L3\n9QQdzhcDv0vavAOoI7jAx4wGNobvNxNcHBO3xWwgqCkMcffS8NXf3SfSviuBmQQ1mQEEtRYAC2Oq\nAca18LkNrawH2EfzTvTDWygTH8447D/4FnA5MNDdS4FdYQztHesxYKaZnQQcBzzdSjmJKCUF6equ\nJWg62Ze40t0bgCeBfzOzfmGb/Y009Ts8CXzNzEaa2UBgTsJnNwPPAj8ws/5mlmdm48zsY2nE048g\noVQQXMi/l7DfRuAh4IdmNjzs8D3dzHoR9DucZ2aXm1mBmQ02s0nhR5cBnzSz3mZ2VHjO7cVQD2wH\nCszsVoKaQswDwL+a2XgLnGhmg8MYywn6Ix4Ffuvu1Wmcs0SIkoJ0ae6+xt2XtLL5qwTfstcCLxF0\nmD4Ubvs5sBB4k6AzOLmmcTVQBKwgaI//DXBEGiHNJWiK2hh+9q9J228C3ia48O4E7gLy3P1DghrP\nN8P1y4CTws/8iKB/ZCtB887jtG0h8Cfg/TCWGpo3L/2QICk+C+wGHgRKErY/ApxAkBhEmjF3TbIj\nEiVmdhZBjWqM6wIgSVRTEIkQMysEbgAeUEKQligpiESEmR0HVBE0k/04x+FIF6XmIxERiVNNQURE\n4rrdw2tDhgzxsrKyXIchItKtLF26dIe7D22vXLdLCmVlZSxZ0todiiIi0hIzW99+KTUfiYhIAiUF\nERGJU1IQEZG4bten0JK6ujrKy8upqanJdShZU1xczMiRIyksLMx1KCLSg/SIpFBeXk6/fv0oKysj\nYSjkHsvdqaiooLy8nLFjx+Y6HBHpQXpE81FNTQ2DBw+OREIAMDMGDx4cqZqRiGRHj0gKQGQSQkzU\nzldEsqPHJAURkZ5iV3Udv1r8IQ2N2R+GqEf0KeRaRUUF5557LgBbtmwhPz+foUODBwcXL15MUVFR\nu/v43Oc+x5w5czjmmGMyGquIZE/F3gM8/tqH1Dc0ghmfmDScXoX5/HZpOZeeeAR9exXwxOsbmHbs\nMA4fUMwvX/uQc44dxh/e2sR//e9aXlq9g3FD+jC4by9mTx1NUUHmv8d3uwHxpkyZ4slPNK9cuZLj\njjsuRxE1d9ttt9G3b19uuummZutjk2Ln5XXeP2pXOm+RqKuubeChlz+gpq6Bj1S/yN+ddBw/en8I\n9z6/GjNwhxGlJdQ2NLJ9zwHKBvdmWL9iFq/byUfKBnLW+KH84Ln3GVBSSGnvQtZX7CfWSuwOpx05\niDkXHcekUaWHFJ+ZLXX3Ke2VU/NRBq1evZoJEybwmc98hokTJ7J582auu+46pkyZwsSJE7n99tvj\nZc8880yWLVtGfX09paWlzJkzh5NOOonTTz+dbdu25fAsRCQd89/cyL8vfI+fLlrNWX/7Jvbwxdz7\n/Gqmlg3ig+9fwsdPGs7Gqmq27znA1aePYV3Ffhav28mwfr1Yur6Sv7y/HYCigjw2VlZzxyeO54Pv\nX8IH37+EK08dzeIPdrJ8066Mn0ePaz767h+Ws2LT7k7d54Th/fnOx9OZ0z3Vu+++y9y5c5kyJUjQ\nd955J4MGDaK+vp5p06Zx2WWXMWHChGaf2bVrFx/72Me48847ufHGG3nooYeYM2dOS7sXkS7iuRVb\nGVFawu++/NFgQtTQp6eMBODq08fwxvpK5l47lXFD+5JnRnnlfr55wTFcdM+LLF1fyScmDefHsyan\n7Pt7f38C3/v7E7JyHj0uKXQ148aNiycEgF/96lc8+OCD1NfXs2nTJlasWJGSFEpKSrjooosAOOWU\nU3jxxRezGrOIHLxlG6qYdswwDutfHF/3uTPK+PSUUQB8pGwQL885J77tthlNXzS/fPY4HnjpA845\n7rDsBdyKHpcUDvUbfab06dMn/n7VqlXcc889LF68mNLSUq666qoWnzVI7JjOz8+nvr4+K7GKyKFp\naHR27qvl8AHFzdZfHiaE9nxr+rF8a/qxmQjtoKlPIYt2795Nv3796N+/P5s3b2bhwoW5DklEOkHl\n/loaHYb07dVs/XGH98tRRIeux9UUurKTTz6ZCRMmcOyxxzJmzBjOOOOMXIckIp2gYm8tAIP7FkFD\nXdOGumoo6p2jqA6NkkInu+222+LvjzrqKJYtWxZfNjMeffTRFj/30ksvxd9XVVXF38+aNYtZs2Z1\nfqAi0ml27D0AwOA+vYJEEFNd2e2SgpqPREQOQuW+Wua+uo76hkb++NYm3t+6J54UhvYrgvqEfsKa\nqpZ30oVltKZgZtOBe4B84AF3vzNp+4+AaeFib2CYux/akxkHyxthVzn0OwLyNfy0iKTnFy9/wL3P\nr+b3yzaxdH0lAGceNYSv5D/NmAX3w76tTYXnfxWKB3Tewaf+AxwzvfP214KMJQUzywfuA84HyoHX\nzWy+u6+IlXH3bySU/yqQeoNuplRXwf6K4FHBgWOydlgR6d4+qNgPEE8IAEvW7+S+oj9RsL0Y9iU8\nbGp5cGBP5x28sa79Mh2UyZrCVGC1u68FMLN5wExgRSvlZwPfyWA8zcWfH2/M2iFFpGtZ9O423viw\nki/83ZEMKAlaDHbX1PHwy+vYe6DlW8FfXbOD6RMP50/LtwDw8pxzGDGgGG7fByd/EV78QVDwisfh\nuEuzch6dKZNJYQSwIWG5HDi1pYJmNgYYCzzfyvbrgOsARo8e3Unhhd0pSgoiPVLV/lp+/uJapowZ\nxLRjh6Vsb2h0vvTYUg7UNzK8tITZU4Nry7/9cSVPLNlASWF+i/vNM7hg4mGcOX4If3hzE8MHFAe1\nAW+AkoFNBQuLW/x8V9dV7j6aBfzG3Rta2uju9wP3QzAgXqccMXGkKRHpcZ54fQP3LVpDae8PeeOW\n88nLaz4HybINlRyoD74UvlW+i9lTgyGrf/tGOZ89fQzfnXl8u8e46rSw6bk6bEoqTugSLVBSSLYR\nSHycb2S4riWzgK9kMJZUFrvxquM1hWnTpjFnzhwuvPDC+Lof//jHvPfee/zsZz9r8TN9+/Zl7969\nHT62iLTsuRVBh2/V/jrm/O4t+hU3v6HktQ8q6NurgCOH9mHRu9v41z+uYFNVNfWNzoxJww/uYLG7\njBJrCgUlHQk/ZzKZFF4HxpvZWIJkMAu4MrmQmR0LDARezWAsqTqxpjB79mzmzZvXLCnMmzePu+++\nu8P7FpGD98qaHSxZX8mXzx7Hf7+9mQVvb0kpk59n3DZjIjV1Ddz1zLs88XrQ2n3iyAFMGjUwpXyb\nYjWFkoSagpqPmnP3ejO7HlhIcEvqQ+6+3MxuB5a4+/yw6CxgnudqYodO6FO47LLLuOWWW6itraWo\nqIh169axadMmJk+ezLnnnktlZSV1dXXccccdzJw5sxOCFpG2PPLKOob168XXzh2f1phC8WagQ1Wd\nUFPIL4KGWjUftcTdFwALktbdmrR8W6ce9Jk5sOXt9st5A9TtD5qRCvu0XfbwE+CiO1vdPGjQIKZO\nncozzzzDzJkzmTdvHpdffjklJSU89dRT9O/fnx07dnDaaacxY8YMza8s0gGvrqng2RWp3/wTvfD+\nDj51ygiKW+ks7nSJfQoFxUoK3Zo3Qt2+tstU74RtK9ssMvvis5j38P3MPP1o5j32MA/+6A586wr+\n+da7eOHVJeTlGRs3lrN1+YscPmxocNx29tmuPVvgPz/fsX1Ibk2+CsZfCL/7AtQfgCFHw+WP5Dqq\nLu3W37/Duop9bV7w+/TK5+v77oGHNsKVT0Bx/+YF3OF3X4STr4axZx1cAFuXw9NfDpqg84uCO4/2\nVwTbSkqh92A4sBvyspSQOlnPSwptfKNvpnYf7Hg/eF9QDAW92i7fjpmXXsQ3vnMXbyxfxf6aA5xy\nysk8/MvfsH1nFUsXzaewsJCySWdRU09wLLMOH5O8Ahg8rmP7kNxZ9zK89wz0PQw2/Q36j4QVT7Oi\nvJIJIw+yTbuHqKlr4N4/r2J/bYs3IlLb0MiqbXu59dIJfP7Msa3vyB2+e1nwfscqGHlK8+0H9sDb\nv4blT8GtFQcX5PpXYHPTmGYMHg+jpgY/i/rAVb8N9t0393MjHIqelxTSldiF0XsI9B3aod31HQTT\nzjmPz3/jVmZ/5moYdCS7GnoxbOSRFB52DIsWLWL9ho1QOhoGlQEGg47s0DHZegCueKxj+5DceeTj\nUF8bNDUAtWOnUfTmo1z985dY8t2P5zi43HhlzQ7+8y9r6NerIOUW0pgRpSVcdMLhbe+oNuHOvprK\n1O0dGZMo+bOTr4Izv960PHgcnN19Z0qMblJIUHWgkX311e0XbMcFMz7Jk1fN4p77H2ZjVTXnXPpJ\n5s6+jGMnHM9Jkydz1NHHsGV3DYVV1Tiwsapjx6zaX8t3fv9Oh+OW3LhqRy19GncxYH81fYAX1u3n\nPIDG2hxHljtrtgVNuS9+exqlvYvaKd2G6oREUN1CAqhuIVGkve+qoB8y1uxckp3h2rJFSQGoqmlk\nn3X8P+Jp06bzzsbgD7Bqfy15Jf2Z+/SzqcfbX8vi9zdStb9jx6yubeD3b25tv6B0SWc31jOscT9b\nNlZwMrByp3NePowt7cDFsJtbs30vg/oUdSwhQPNE0FICaClRHMy+S0oTkkLPaupTUgD6FveibHAn\njmSYJXm7Slh26wW5DkMOUeMTj7F6+Wa27Q8GWBs7fBhshX3VHa+1tmb1tr08+uo6GjtwA/joQb3Z\nuruGA/WNjBhYws59tVS30gdwsF54fzvjhrZzN2A6MlpTqAwSwe7wWdxi1RR6nLwC/Rok+/IKiijJ\nb2TXniAplPQJ7pDZu7+axkZvtU090WtrK9hVXccFE9tpYw/956LV/P7NTfHB3w5WXX0je8KB4voV\nF7CnJng/qE/n1W7On9AJHbQ17dUUOpgUEhOBagpdk7sf8v3/efnd79eQq2f9pBPlF1FsDWyt3A1A\n3/5BbTXf69hVXcfAhAvtcyu2smHnftZV7KMx4d/+sb9+CMCVp44mjRzCcyu3MvOk4fzwikmHFPKe\nmjq+OHcJV502hvOOO4wvzl3CjJOG8+k0J6jPmvhF31ruVO5oR3PiTSJKCl1PcXExFRUVDB48+CAS\nQ9N/rLy87vVrcHcqKiooLu6eD8dIKK+AIqun5kANFEBp/6CmUEg935m/nH7Fwd9lo8OvFn8Y/9jg\nhGQxpG8RtfWNLHyn7Ye5YnoX5XPZlJGHHHK/4kLmXXd6fPnRa1sc+Dj3Yk1Gg8a2XVNorIeGejiY\nL4bVlc07l9XR3PWMHDmS8vJytm/fnv6H6mtgbzAZxoGKInoVdK+ZSYuLixk58tD/c0sXkF9ESV4D\nAwqdWi9gRNivNWpAPq+s2dGsaGnvQkpLCvnhFZM4eXTP+maaEdWVwYNl/UfAqmfhrrLm22v3N73/\n93FNY6Glu++SgcGAd/XVUNS3U0LuKnpEUigsLGTs2DYeZGnJBy/Cby9nbv35nHr9Lzjm8H6ZCU6k\nNflFFFkj15w6At4opqh3MMH7g1dNhpFTchxcNxdr9z/rJnj3v1suUzo6GBWg4SDvArQ8mHw1nPxZ\n2LTs4BJKN9AjksKhCZqPFjSeyoW9NUez5EB+YXBBaqgN3sfmCj/Yi5SkqqkKvs0feXbwypQh4zO3\n7xzpXm0mnSnsrHO3Q74TQ6RDYqNpNtQG7/PDvgIlhY5LbveXtEU3KYQ1hfz8vOyNpCiSKL8I8KB/\nq1lSyPzk7D1edVWPuysoW6KbFMKaghKC5Eysuah2n5qPOlt1VY97qCxbopsUQpbOzd0imZCYFPIK\ngxeoptAZalRTOFQRTgqx5xQi/CuQ3Io1F9XuU/NRZ2qoD+YzUJ/CIYnu3Udh85EqCpIziTWFgl7t\nNx81NgQvaVt8whvVFA5FdJNCWFPQ1JiSM/Gawl7o1a/tu4/2bIGfnNJ8ngBpW+/BuY6gW4puUoi1\nHikpSK7EkkDd/rCjuY3mo51rg4Qw+f/BwLKshdhtFRTD0RfmOopuKbpJQTUFybVmdx8Vtd18FBur\n5yPXwvDJ2YlPIkm9rPoVSK7E7jaK1xTC5cYWagqxAd7UTi4ZFt0rosdqCjmOQ6IrP2EOgvaaj2I1\nBd17LxkW3aSg5iPJtfyE4VXyiyAvPxhsraXmo5qqYFuv/tmLTyIpukkhNlGJkoLkSrOaQlHTz9b6\nFIpLIS+6/2UlOyL8F6aaguRYcvNRbF1rzUd6GEuyILp3H7mSguRYcvNRbN3+nVC5vnnZPVvVySxZ\nEd2kEDKUFCRHivqkvi/qA2/NC17Jjp6enbgk0iKcFGJ9ChFuQZPcGnQkzJ4X3G46/vxg3eVzYeuK\nlsuP+Wj2YpPIim5S0C2pkmtmcMxFzdcNn6yH0ySnIvw1WX0KIiLJopsU1NEsIpIiuklBfQoiIiki\nf0VURUFEpEl0k0K8+Si6vwIRkWQRviKGM6/pOQURkbjoJoX42Ee5DUNEpCuJblJAzUciIsmie0XU\nKKkiIikymhTMbLqZvWdmq81sTitlLjezFWa23Mx+mcl4Wjl+tg8pItJlZWyYCzPLB+4DzgfKgdfN\nbL67r0goMx64GTjD3SvNbFim4mkj0KwfUkSkq8pkTWEqsNrd17p7LTAPmJlU5ovAfe5eCeDu2zIY\nT3Nh81GekoKISFwmk8IIYEPCcnm4LtHRwNFm9rKZ/dXMWhwb2MyuM7MlZrZk+/btnRSehrkQEUmW\n647mAmA8cDYwG/i5maVML+Xu97v7FHefMnTo0M45ssY+EhFJkcmksBEYlbA8MlyXqByY7+517v4B\n8D5BksgC3ZIqIpIsk1fE14HxZjbWzIqAWcD8pDJPE9QSMLMhBM1JazMYUwrVFEREmmQsKbh7PXA9\nsBBYCTzp7svN7HYzmxEWWwhUmNkKYBHwT+5ekamYkgIElBRERBJldOY1d18ALEhad2vCewduDF9Z\nFiYFtR6JiMRF95KoUVJFRFJE+Iqo5iMRkWTRTQrqUxARSRHdpBBS85GISJMIXxFVUxARSRbdpBBr\nPspxGCIiXUl0k4JqCiIiKaKbFHRLqohIighfEWMPr6mmICISE+GkEFDzkYhIk+gmBT2nICKSIrpJ\nAc28JiKSrN2kYGZfNbOB2Qgmq9TRLCKSIp0r4mHA62b2pJlNt57W3tKzzkZEpEPaTQrufgvBbGgP\nAtcAq8zse2Y2LsOxZZbHmo9UUxARiUnrihjOe7AlfNUDA4HfmNndGYwtK3paxUdEpCPanWTHzG4A\nrgZ2AA8QzI5WZ0Fj/CrgW5kNMVNUUxARSZbOzGuDgE+6+/rEle7eaGaXZiasLAibj1BNQUQkLp2v\nyc8AO2MLZtbfzE4FcPeVmQos88KagioKIiJx6VwSfwbsTVjeG67r3lRTEBFJkU5SsLCjGQiajUiv\n2amLU5+CiEiydK6Ia83sa2ZWGL5uANZmOrBMi+c5JQURkbh0rohfAj4KbATKgVOB6zIZVDZ4/DmF\nHAciItKFtNsM5O7bgFlZiCWrmpKCsoKISEw6zykUA9cCE4Hi2Hp3/3wG48oCjZIqIpIsneajR4HD\ngQuB/wVGAnsyGVQ2BP3lYLonVUQkLp0r4lHu/i/APnd/BLiEoF+hW2u6I1VJQUQkJp0rYl34s8rM\njgcGAMMyF1J2qKNZRCRVOs8b3B/Op3ALMB/oC/xLRqPKAtd8CiIiKdpMCuGgd7vdvRJ4ATgyK1Fl\nQVNSyHEgIiJdSJtfk8Onl7vpKKjt0HwKIiIp0rki/o+Z3WRmo8xsUOyV8cgyrOnuI1UVRERi0ulT\nuCL8+ZWEdU43b0qKdzTrllQRkbh0nmgem41Asi0+xJ86FURE4tJ5ovnqlta7+9zODyd74s1HKCmI\niMSk03z0kYT3xcC5wBtAN08KwU+NfSQi0iSd5qOvJi6bWSkwL2MRZUmsppCnjmYRkbhD6WXdB3T7\nfgY9vCYikiqdPoU/EBtSNEgiE4AnMxlUVrhGSRURSZZOn8J/JLyvB9a7e3k6Ozez6cA9QD7wgLvf\nmbT9GuDfCSbwAfipuz+Qzr47i25JFRFpkk5S+BDY7O41AGZWYmZl7r6urQ+ZWT5wH3A+wYxtr5vZ\nfHdfkVT0CXe//uBD75imPoVsH1lEpOtK55L4a6AxYbkhXNeeqcBqd1/r7rUEndMzDz7EzNDMayIi\nqdJJCgXhRR2A8H1RGp8bAWxIWC4P1yX7lJm9ZWa/MbNRLe3IzK4zsyVmtmT79u1pHLp97k6jm/oU\nREQSpJMUtpvZjNiCmc0EdnTS8f8AlLn7icBzwCMtFXL3+919irtPGTp0aOcc2R1H8ymIiCRKp0/h\nS8DjZvbTcLkcaPEp5yQbgcRv/iNp6lAGwN0rEhYfAO5OY7+dwt1xTM1HIiIJ0nl4bQ1wmpn1DZf3\nprnv14HxZjaWIBnMAq5MLGBmR7j75nBxBrAy3cA7yoklhWwdUUSk62u3+cjMvmdmpe6+1933mtlA\nM7ujvc+5ez1wPbCQ4GL/pLsvN7PbE5qjvmZmy83sTeBrwDWHfioHx8PmI/UpiIg0Saf56CJ3/+fY\ngrtXmtnFBNNztsndFwALktbdmvD+ZuDm9MPtRLr7SEQkRTodzflm1iu2YGYlQK82yncLTX0KuY5E\nRKTrSKem8DjwZzP7BWAETTwt3iXUnQTPKZimUxARSZBOR/NdYZv/eQRjIC0ExmQ6sMxTn4KISLJ0\nB3nYSpAQPg2cQxbvEsoU3ZIqIpKq1ZqCmR0NzA5fO4AnAHP3aVmKLaPUpyAikqqt5qN3gReBS919\nNYCZfSMrUWWD7j4SEUnRVvPRJ4HNwCIz+7mZnQs9Z0Jjj/cp5DoSEZGuo9Wk4O5Pu/ss4FhgEfB1\nYJiZ/czMLshWgBmjPgURkRTtdjS7+z53/6W7f5xg/KK/Ad/OeGSZpqQgIpLioKaYcffKcMTSczMV\nULbEmo/U0Swi0iSy844F/cyaT0FEJFGEk0I4HadygohIXGSTQtMkO8oKIiIxEU8K6mgWEUkU2aQQ\ne6JZOUFEpEl0k0L4UlIQEWkS2aSg5iMRkVSRTgqgjmYRkUSRTQpOo0ZJFRFJEt2k4JpkR0QkWWST\nAuF0nKopiIg0iW5SAHU0i4gkiW5S0BPNIiIpIpsUPLz7SDlBRKRJZJNCMHi2kadOBRGRuOgmhfjD\na7kORESk64hsUnD1KYiIpIhuUgANiCcikiSySQFv1C2pIiJJopsUQkoKIiJNopsUwuk4lRJERJpE\nNim4g7uaj0REEkU2KQTPKYBF+DcgIpIsupdETbIjIpIisknB0cNrIiLJIpsUNCCeiEiq6CYFNCCe\niEiy6CYF9SmIiKRQUlBSEBGJy2hSMLPpZvaema02szltlPuUmbmZTclkPIk8/KmOZhGRJhlLCmaW\nD9wHXARMAGab2YQWyvUDbgBey1QsLQprCqaagohIXCZrClOB1e6+1t1rgXnAzBbK/StwF1CTwVha\nECQFERFpksmkMALYkLBcHq6LM7OTgVHu/t9t7cjMrjOzJWa2ZPv27Z0TnbvuPBIRSZKzjmYzywN+\nCHyzvbLufr+7T3H3KUOHDu2kCFRTEBFJlsmksBEYlbA8MlwX0w84HviLma0DTgPmZ6uzOTbzmoiI\nNMlkUngdGG9mY82sCJgFzI9tdPdd7j7E3cvcvQz4KzDD3ZdkMKYkqimIiCTKWFJw93rgemAhsBJ4\n0t2Xm9ntZjYjU8dNm6v5SEQkWUEmd+7uC4AFSetubaXs2ZmMpYUjopqCiEhzkX6iWTlBRKS56CYF\n1NEsIpIsuknB1XwkIpIsskkhHDg7x1GIiHQtkU0KenhNRCRVZJOCuWuGHRGRJJFNCu7qZhYRSRbZ\npIDuPRIRSRHdpOCoT0FEJEl0kwLqUxARSRbppKCagohIc9FNCg56TkFEpLnoJgV1NIuIpIhuUtAt\nqSIiKSKbFEwdzSIiKSKbFBzdkioikiyySUGjpIqIpIpuUlDzkYhIimgnBRERaSayScHUfCQikiKy\nSQEcV/ORiEgzEU4KoJqCiEhXCubAAAAHUklEQVRz0U0Kaj4SEUkR3aSAKyeIiCSJdlJQVhARaSa6\nSUF3pIqIpIhsUjDVFEREUkQ2KQTTKSgpiIgkimxSMNdzCiIiySKbFNTRLCKSqiDXAWRLdW0D+2vr\nm1a4Bs4WEUkWmaQw99V1fP+Zd+PLTxfVU1CktCAikigySeGMo4Zw+8yJ8eWRr5ZQUtovhxGJiHQ9\nkUkKx48YwPEjBjSteLsIekXm9EVE0hKdq+Ibj8KrP21a3vkBHHl2rqIREemSopMUeg+Cocc0LQ89\nBk68InfxiIh0QdFJCsdeErxERKRVEX5OQUREkikpiIhIXEaTgplNN7P3zGy1mc1pYfuXzOxtM1tm\nZi+Z2YRMxiMiIm3LWFIws3zgPuAiYAIwu4WL/i/d/QR3nwTcDfwwU/GIiEj7MllTmAqsdve17l4L\nzANmJhZw990Ji33QLAciIjmVybuPRgAbEpbLgVOTC5nZV4AbgSLgnJZ2ZGbXAdcBjB49utMDFRGR\nQM47mt39PncfB3wbuKWVMve7+xR3nzJ06NDsBigiEiGZTAobgVEJyyPDda2ZB3wig/GIiEg7Mtl8\n9Dow3szGEiSDWcCViQXMbLy7rwoXLwFW0Y6lS5fuMLP1hxjTEGDHIX62u9I5R4POORo6cs5j0imU\nsaTg7vVmdj2wEMgHHnL35WZ2O7DE3ecD15vZeUAdUAl8No39HnL7kZktcfcph/r57kjnHA0652jI\nxjlndJgLd18ALEhad2vC+xsyeXwRETk4Oe9oFhGRriNqSeH+XAeQAzrnaNA5R0PGz9nc9byYiIgE\nolZTEBGRNigpiIhIXGSSQnsjtnZXZvaQmW0zs3cS1g0ys+fMbFX4c2C43szs3vB38JaZnZy7yA+d\nmY0ys0VmtsLMlpvZDeH6HnveZlZsZovN7M3wnL8brh9rZq+F5/aEmRWF63uFy6vD7WW5jP9QmVm+\nmf3NzP4YLvfo8wUws3UJo0cvCddl7W87EkkhzRFbu6uHgelJ6+YAf3b38cCfw2UIzn98+LoO+FmW\nYuxs9cA33X0CcBrwlfDfsyef9wHgHHc/CZgETDez04C7gB+5+1EEz/pcG5a/FqgM1/8oLNcd3QCs\nTFju6ecbM83dJyU8k5C9v2137/Ev4HRgYcLyzcDNuY6rE8+vDHgnYfk94Ijw/RHAe+H7/wJmt1Su\nO7+A3wPnR+W8gd7AGwQDTO4ACsL18b9zgodGTw/fF4TlLNexH+R5jgwvgOcAfwSsJ59vwnmvA4Yk\nrcva33Ykagq0PGLriBzFkg2Hufvm8P0W4LDwfY/7PYTNBJOB1+jh5x02pSwDtgHPAWuAKnevD4sk\nnlf8nMPtu4DB2Y24w34MfAtoDJcH07PPN8aBZ81saThCNGTxbzujTzRL7rm7m1mPvO/YzPoCvwW+\n7u67zSy+rSeet7s3AJPMrBR4Cjg2xyFljJldCmxz96Vmdnau48myM919o5kNA54zs3cTN2b6bzsq\nNYWDHbG1u9tqZkcAhD+3het7zO/BzAoJEsLj7v67cHWPP28Ad68CFhE0n5SaWezLXeJ5xc853D4A\nqMhyqB1xBjDDzNYRjKB8DnAPPfd849x9Y/hzG0Hyn0oW/7ajkhTiI7aGdyvMAubnOKZMmk/T4IKf\nJWhzj62/Orxj4TRgV0KVtNuwoErwILDS3ROncO2x521mQ8MaAmZWQtCHspIgOVwWFks+59jv4jLg\neQ8bnbsDd7/Z3Ue6exnB/9fn3f0z9NDzjTGzPmbWL/YeuAB4h2z+bee6UyWLnTcXA+8TtMP+/1zH\n04nn9StgM8FIs+UEd2EMJuigWwX8DzAoLGsEd2GtAd4GpuQ6/kM85zMJ2l3fApaFr4t78nkDJwJ/\nC8/5HeDWcP2RwGJgNfBroFe4vjhcXh1uPzLX59CBcz8b+GMUzjc8vzfD1/LYtSqbf9sa5kJEROKi\n0nwkIiJpUFIQEZE4JQUREYlTUhARkTglBRERiVNSEEliZg3hCJWxV6eNqmtmZZYwoq1IV6NhLkRS\nVbv7pFwHIZILqimIpCkc5/7ucKz7xWZ2VLi+zMyeD8ez/7OZjQ7XH2ZmT4VzILxpZh8Nd5VvZj8P\n50V4NnxCWaRLUFIQSVWS1Hx0RcK2Xe5+AvBTglE8AX4CPOLuJwKPA/eG6+8F/teDORBOJnhCFYKx\n7+9z94lAFfCpDJ+PSNr0RLNIEjPb6+59W1i/jmCim7XhgHxb3H2wme0gGMO+Lly/2d2HmNl2YKS7\nH0jYRxnwnAeTpWBm3wYK3f2OzJ+ZSPtUUxA5ON7K+4NxIOF9A+rbky5ESUHk4FyR8PPV8P0rBCN5\nAnwGeDF8/2fgHyE+Qc6AbAUpcqj0DUUkVUk4w1nMn9w9dlvqQDN7i+Db/uxw3VeBX5jZPwHbgc+F\n628A7jezawlqBP9IMKKtSJelPgWRNIV9ClPcfUeuYxHJFDUfiYhInGoKIiISp5qCiIjEKSmIiEic\nkoKIiMQpKYiISJySgoiIxP0fgaqOIlI6YnQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD8SKezYLYJtYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Esb6vfqURV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save our best model\n",
        "# pickle.dump( as_model, open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"wb\" ) )\n",
        "# load model\n",
        "# as_model = pickle.load( open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBO4xI1FbIR3",
        "colab_type": "text"
      },
      "source": [
        "**Testing our model with an independent batch of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARhPhXtQWUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load our pickle batch of data\n",
        "with open('all_mprage_grappa/processed_brains_aug/dbatch4.pkl', 'rb') as f: # also 'total_slices_all.pkl' ## RENAMED 5 TO 7, TESTING IT\n",
        "  total_slices_test, total_slices_info_test = pickle.load(f) # stored_batches/total_slices_batch5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-x9fN2IfVqy",
        "colab_type": "code",
        "outputId": "64d54b7f-ff52-4ca7-ccef-a902427cf3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(total_slices_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 160, 160, 160, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYjNw77Lt7nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the set an array\n",
        "total_slices_test = np.array(total_slices_test)\n",
        "\n",
        "# independent test\n",
        "y_test, sex_test, ages_test = get_classification(total_slices_info_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_mFSZqfvBCK",
        "colab_type": "code",
        "outputId": "26ec20f0-3a41-45f1-fe8e-38b669f5ee47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test our model against the independent set\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 103us/step\n",
            "Score: 0.73, Accuracy: 0.517241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNCAXUfgbYIs",
        "colab_type": "text"
      },
      "source": [
        "**Make predictions with our as model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kQ5Mrw5wsVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "as_train_preds = as_model.predict([sex_train, ages_train]) # this is what we will feed into next model\n",
        "as_test_preds = as_model.predict([sex_test, ages_test]) # this is what we will use when testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA2N4vQ7yVEv",
        "colab_type": "text"
      },
      "source": [
        "**Next part, get predictions from model on ages and sex and predictions from images, feed into another deep NN to try to predict the output, then test accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3v84hoBy5AE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6009a0d5-bc75-4e63-8d05-0e97fd4ecece"
      },
      "source": [
        "# ALTERNATIVELY: load our cnn_models\n",
        "cnn_model = load_model('all_mprage_grappa/stored_models/model4x/model40_aug_v0.h5') # take 040, best performing one so far"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0624 16:17:28.772655 139906754799488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHzkS1kxKWX",
        "colab_type": "code",
        "outputId": "b78c17d0-9493-4bdc-a25f-2736a612500c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#load batch slices, predict and aggregate scores\n",
        "cnn_train_preds = []\n",
        "\n",
        "# load the pickle (train with 0 to 3)\n",
        "print (training_batch_f)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) \n",
        "    # predict and aggregate\n",
        "    total_slices = np.array(total_slices)\n",
        "    \n",
        "    # split and predict, too many to handle otherwise and will crash\n",
        "    sub_arrays = np.array_split(total_slices, 50)\n",
        "    \n",
        "    for i in range(len(sub_arrays)):\n",
        "      \n",
        "      sub_array = sub_arrays[i]\n",
        "      cnn_prediction = cnn_model.predict(sub_array)\n",
        "      cnn_train_preds.extend(cnn_prediction)\n",
        "    \n",
        "print (np.shape(cnn_train_preds))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU5fMHzOjWmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# also get the test predictions\n",
        "cnn_test_preds = cnn_model.predict(total_slices_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0UzJCQti-pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save our preds (cause it takes ages to build)\n",
        "# pickle.dump( cnn_train_preds, open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"wb\" ) )\n",
        "# pickle.dump( cnn_test_preds, open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"wb\" ) )\n",
        "## load model\n",
        "cnn_train_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"rb\" ) )\n",
        "cnn_test_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaXsBh1NzDc",
        "colab_type": "text"
      },
      "source": [
        "**Now make deep learning framework where we load our cnn train preds, our as train preds and aggregate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYsnoPcwwue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_binary(cat_array):\n",
        "  '''Function to convert categorical back to binary values'''\n",
        "  binary_output_array = []\n",
        "  for i in range(len(cat_array)):\n",
        "    binary_output_array.append(np.argmax(cat_array[i]))\n",
        "    \n",
        "  binary_output_array = np.array(binary_output_array)\n",
        "  return binary_output_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3NgFPpkMq17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create MLP models\n",
        "mlp_cnn = create_mlp(2, regress=False) # value of 1 for binary, 2 for categorical\n",
        "mlp_as = create_mlp(2, regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input_two = concatenate([mlp_cnn.output, mlp_as.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "z = Dense(8, activation=\"relu\")(combined_input_two)\n",
        "z = Dropout(0.2)(z)\n",
        "z = Dense(2, activation=\"sigmoid\")(z)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "combined_model = Model(inputs=[mlp_cnn.input, mlp_as.input], outputs=z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKsn3mJSYNR",
        "colab_type": "code",
        "outputId": "58b20d60-4b23-43da-c18a-40da5afce28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "# compile our model\n",
        "combined_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "combined_model.fit([cnn_train_preds, as_train_preds], y_train, validation_split=0.1, epochs=200)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5763f0b74b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_train_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    226\u001b[0m         raise ValueError('All input arrays (x) should have '\n\u001b[1;32m    227\u001b[0m                          \u001b[0;34m'the same number of samples. Got array shapes: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                          str([x.shape for x in inputs]))\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         raise ValueError('All target arrays (y) should have '\n",
            "\u001b[0;31mValueError\u001b[0m: All input arrays (x) should have the same number of samples. Got array shapes: [(300, 2), (536, 2)]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx-CqpxPUSPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle.dump( combined_model, open( \"all_mprage_grappa/z_tests/combined_model_binary.pkl\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENTzWL9pTTYR",
        "colab_type": "text"
      },
      "source": [
        "**Load last one and test it out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us7wRSU2S4rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get predictions of cnn_model on test data\n",
        "# get predictions of as_model on test data\n",
        "# input these predictions into our combined_model\n",
        "# evaluate these against the actual y-value, pray it is not overfitted..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsGbpSiXWamH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm_score, cm_acc = combined_model.evaluate([cnn_test_preds, as_preds_test], y_test)\n",
        "\n",
        "print (\"Combined model score: %.2f, and combined model accuracy: %.2f\" % (cm_score, cm_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtf-HdCoJI0",
        "colab_type": "text"
      },
      "source": [
        "**Try: XGboosting the training predicted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZDlLxAzqkaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyBt5S9aw_Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert everything to binary\n",
        "cnn_train_preds_binary = to_binary(cnn_train_preds)\n",
        "as_train_preds_binary = to_binary(as_preds_train)\n",
        "y_train_binary = to_binary(y_train)\n",
        "\n",
        "cnn_test_preds_binary = to_binary(cnn_preds_test)\n",
        "as_test_preds_binary = to_binary(as_preds_test)\n",
        "y_test_binary = to_binary(y_values_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vxZoNjxyo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.shape(y_train_binary) # no worky"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umVR6Q_esX_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make into a matrix and input this into our space\n",
        "\n",
        "combined_zipped_train = list(zip(cnn_train_preds_binary, as_train_preds_binary))\n",
        "combined_zipped_train = np.array(combined_zipped_train)\n",
        "\n",
        "# fit model with training data\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(combined_zipped_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrOAKOxjrZYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_zipped_test = list(zip(cnn_preds_test, as_preds_test))\n",
        "combined_zipped_test = np.array(combined_zipped_test)\n",
        "\n",
        "y_values_predictions = []\n",
        "for i in range(len(combined_zipped_test)):\n",
        "  cz_test_csr = csr_matrix(combined_zipped_test[i])\n",
        "  xgb_preds = xgb_model.predict(cz_test_csr)\n",
        "  y_values_predictions.append(xgb_preds)\n",
        "  \n",
        "y_values_predictions = np.array(y_values_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDhTUzOvcJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(y_values_test, y_values_predictions, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIUzf6sUW73T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_train_preds # train with this as an input\n",
        "as_preds_train # another input\n",
        "y_train # ideal output\n",
        "\n",
        "#test\n",
        "cnn_preds_test\n",
        "as_preds_test\n",
        "y_values_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8O6PliZPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXxbK0vbB3B",
        "colab_type": "text"
      },
      "source": [
        "To try: reverse classification on each model and feed these in, see how our model holds up. Then do the same for our test and compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srl7AdZxbIaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
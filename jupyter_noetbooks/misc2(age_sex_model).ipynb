{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "misc2(age_sex_model).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/misc2(age_sex_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzBN1AhkB8-",
        "colab_type": "text"
      },
      "source": [
        "**Goal: Build a joint deeplearning model which trains on ages and genders, to classify that of the fifth batch.** \n",
        "**Then ensemble this with deep learning prediction outputs of our best model to get the final classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HffPUoiMkAb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f325a825-5f89-4c5a-ebf7-b6bbb0a629f3"
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# other imports to handle files\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU, Input, ReLU, concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJGlXqK7j8vD",
        "colab_type": "code",
        "outputId": "9885dc27-dce9-42b5-bb23-9987eb3f9b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where we will be working\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRqVyvgomjT",
        "colab_type": "text"
      },
      "source": [
        "**Run through our batches and train our model after building it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrA5vdtKpcqi",
        "colab_type": "code",
        "outputId": "af64be32-d1bc-4ad7-d427-66241afc6a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Loop through all our total slices and accumulate all the total slices info\n",
        "training_batch_f = os.listdir('all_mprage_grappa/processed_brains/processed_brains_aug')[:3]\n",
        "print (training_batch_f)\n",
        "\n",
        "# build ultima total slices info\n",
        "tsi_ultima = []\n",
        "\n",
        "# load the pickle (train with 0 to 6)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) # stored_batches/total_slices_batch5\n",
        "    \n",
        "    tsi_ultima.extend(total_slices_info)\n",
        "    \n",
        "print (np.shape(tsi_ultima))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8sh1rCrmME",
        "colab_type": "code",
        "outputId": "e8244022-653b-4dd8-ce6c-090c9c55fc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(tsi_ultima) # use this to train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE9O2hmGkBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classification(total_slices_info):\n",
        "    '''get information of patient from total slices info'''\n",
        "    classif = [s[2] for s in total_slices_info]\n",
        "    classif = np.array(to_categorical(classif, 2))\n",
        "    \n",
        "    # now do the same for the sex, 'F' is 0, 'M' is 1\n",
        "    sex = [s[1] for s in total_slices_info]\n",
        "    for i in range(len(sex)):\n",
        "        if sex[i] == 'F':\n",
        "            sex[i] = 0\n",
        "        if sex[i] == 'M':\n",
        "            sex[i] = 1\n",
        "    sex = np.array(to_categorical(sex, 2))\n",
        "    \n",
        "    # finally for age, one hot encode ages 0 to 100 (101 classes then)\n",
        "    ages = [s[3] for s in total_slices_info]\n",
        "    ages = np.array(to_categorical(ages, 101))\n",
        "    \n",
        "    return classif, sex, ages\n",
        "\n",
        "y_train, sex_train, ages_train = get_classification(tsi_ultima)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGfmbPdP52xt",
        "colab_type": "code",
        "outputId": "68b3e24d-5432-4665-c051-d6442edc1d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def larger_info_dataset(df_path):\n",
        "  '''Get the info from any given dataframe'''\n",
        "  ## Get the classification of the y-values, sex and ages from a dataframe instead\n",
        "  big_boi_df = pd.read_csv(df_path)\n",
        "\n",
        "  # remove duplicate subject ids\n",
        "  big_boi_df = big_boi_df.drop_duplicates(subset='Subject', keep='first')\n",
        "\n",
        "  # map groups, control to 0, pd to 1. Also do this for sex, M is 1, F is 0\n",
        "  big_boi_df = big_boi_df.replace({'Group': {'Control': 0, 'PD': 1}, 'Sex': {'M':1, 'F':0}})\n",
        "\n",
        "  # split into training and test sets\n",
        "  train_df, test_df = train_test_split(big_boi_df, test_size=0.10)\n",
        "\n",
        "  print (\"Train Size: %d, Test Size: %d\" % (len(train_df), len(test_df)))\n",
        "\n",
        "  # get the values for train\n",
        "  ages_train = np.array(to_categorical(train_df['Age'].values,101))\n",
        "  sex_train = np.array(to_categorical(train_df['Sex'].values, 2))\n",
        "  y_train = np.array(to_categorical(train_df['Group'].values, 2))\n",
        "\n",
        "  # get the values for test\n",
        "  ages_test = np.array(to_categorical(test_df['Age'].values,101))\n",
        "  sex_test = np.array(to_categorical(test_df['Sex'].values, 2))\n",
        "  y_test = np.array(to_categorical(test_df['Group'].values, 2))\n",
        "  \n",
        "  return [ages_train, sex_train, y_train, ages_test, sex_test, y_test, big_boi_df]\n",
        "\n",
        "ages_train, sex_train, y_train, ages_test, sex_test, y_test, big_boi_df = larger_info_dataset('all_mprage_grappa/Control_PD_6_21_2019.csv')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Size: 536, Test Size: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR2mKH_7iVWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "7f1a1725-74b5-421f-cf1b-a321e552798e"
      },
      "source": [
        "np.unique(to_binary(sex_train), return_counts=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bee93582da9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msex_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'to_binary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co7OHkQ3ZwCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_boi_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4S4W_NhZod5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "60008384-2f71-4645-cc3b-683fedf34aa2"
      },
      "source": [
        "# plot for the ages\n",
        "pd_df = big_boi_df[big_boi_df['Group'] == 1]\n",
        "control_df =  big_boi_df[big_boi_df['Group'] == 0]\n",
        "\n",
        "plt.hist(pd_df.Age, align='mid', alpha=0.25, color='r', label='pd', bins=10)\n",
        "plt.hist(control_df.Age, align='mid', alpha=0.25, color='b', label='control', bins=10)\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# set labels\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Age')\n",
        "plt.title('Frequency of Ages')\n",
        "\n",
        "# set ticks\n",
        "plt.xticks(np.arange(min(big_boi_df.Age), max(big_boi_df.Age)+1, 5.0))\n",
        "# plt.savefig('figures/age_sex_ageFreq.png')\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwMys4FwIzMwKzoXAzKzgXAjMzAru/wNx3jZcDlv4LwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trYdaA7idT9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68cb7576-a639-4db9-930a-1b6a52a8be47"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/msc_project'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WQbJqmsbzhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3dc3409a-11a8-4af1-d7b5-07c72d872fee"
      },
      "source": [
        "# some boxplots\n",
        "ax = sns.boxplot(x=\"Group\", y=\"Age\", data=big_boi_df)\n",
        "ax.set_xticklabels(['control', 'pd'])\n",
        "plt.title(\"Age Class Distribution\")\n",
        "# plt.savefig('figures/age_sex_boxplot.png')\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJIaZxBIUuMMAklqnEEgSY37/6P1s8e9RCx1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWqqxB8ypBgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "    '''Create our MLP network'''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "    model.add(Dense(2, activation=\"relu\"))\n",
        " \n",
        "    # check to see if the regression node should be added\n",
        "    if regress:\n",
        "        model.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "    # return our model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGyH7DU5NOwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(hist):\n",
        "  ''' Function to get our metrics from history and score as inputs'''\n",
        "\n",
        "  # actually obtain our metrics\n",
        "  val_loss = hist.history['val_loss'][0]\n",
        "  val_acc = hist.history['val_categorical_accuracy'][0]\n",
        "  train_loss = hist.history['loss'][0]\n",
        "  train_acc = hist.history['categorical_accuracy'][0]\n",
        "\n",
        "  # put everything into one array\n",
        "  return [val_loss, val_acc, train_loss, train_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqqiKz9VSlrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_binary(cat_array):\n",
        "  '''Function to convert categorical back to binary values'''\n",
        "  binary_output_array = []\n",
        "  for i in range(len(cat_array)):\n",
        "    binary_output_array.append(np.argmax(cat_array[i]))\n",
        "    \n",
        "  binary_output_array = np.array(binary_output_array)\n",
        "  return binary_output_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OS2QxNNpHaP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "2636f066-8160-4cf1-beb5-01b1d73a097e"
      },
      "source": [
        "# creds: https://www.puzzlr.org/the-keras-functional-api-five-simple-examples/\n",
        "# Create MLP models\n",
        "mlp_sex = create_mlp(sex_train.shape[1], regress=False)\n",
        "mlp_age = create_mlp(ages_train.shape[1], regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input = concatenate([mlp_sex.output, mlp_age.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(8, activation=\"relu\")(combined_input)\n",
        "x = Dense(2, activation=\"sigmoid\")(x)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "as_model = Model(inputs=[mlp_sex.input, mlp_age.input], outputs=x)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 07:33:50.713662 140320275679104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0729 07:33:50.769603 140320275679104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0729 07:33:50.779481 140320275679104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ9eUwAMpmj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a20914aa-2b63-43cd-eb78-612909000a02"
      },
      "source": [
        "as_model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "dense_1_input (InputLayer)      (None, 2)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3_input (InputLayer)      (None, 101)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            24          dense_1_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 8)            816         dense_3_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            18          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            18          dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4)            0           dense_2[0][0]                    \n",
            "                                                                 dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 8)            40          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 2)            18          dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 934\n",
            "Trainable params: 934\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wNgZ89trZc",
        "colab_type": "code",
        "outputId": "0180cf4d-c11b-402c-f4c2-20c8fb0ce257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compile our model\n",
        "as_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "history = as_model.fit([sex_train, ages_train], y_train, validation_split=0.1, epochs=500, batch_size=150)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n",
            "Train on 482 samples, validate on 54 samples\n",
            "Epoch 1/500\n",
            "482/482 [==============================] - 1s 3ms/step - loss: 0.5457 - categorical_accuracy: 0.7365 - val_loss: 0.6237 - val_categorical_accuracy: 0.6111\n",
            "Epoch 2/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5430 - categorical_accuracy: 0.7365 - val_loss: 0.6281 - val_categorical_accuracy: 0.6111\n",
            "Epoch 3/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5433 - categorical_accuracy: 0.7365 - val_loss: 0.6345 - val_categorical_accuracy: 0.6111\n",
            "Epoch 4/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5425 - categorical_accuracy: 0.7365 - val_loss: 0.6427 - val_categorical_accuracy: 0.6111\n",
            "Epoch 5/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5411 - categorical_accuracy: 0.7365 - val_loss: 0.6494 - val_categorical_accuracy: 0.6111\n",
            "Epoch 6/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5406 - categorical_accuracy: 0.7365 - val_loss: 0.6537 - val_categorical_accuracy: 0.6111\n",
            "Epoch 7/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5396 - categorical_accuracy: 0.7365 - val_loss: 0.6550 - val_categorical_accuracy: 0.6111\n",
            "Epoch 8/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5399 - categorical_accuracy: 0.7365 - val_loss: 0.6529 - val_categorical_accuracy: 0.6111\n",
            "Epoch 9/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5388 - categorical_accuracy: 0.7365 - val_loss: 0.6550 - val_categorical_accuracy: 0.6111\n",
            "Epoch 10/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5381 - categorical_accuracy: 0.7365 - val_loss: 0.6562 - val_categorical_accuracy: 0.6111\n",
            "Epoch 11/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5375 - categorical_accuracy: 0.7365 - val_loss: 0.6551 - val_categorical_accuracy: 0.6111\n",
            "Epoch 12/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5369 - categorical_accuracy: 0.7365 - val_loss: 0.6563 - val_categorical_accuracy: 0.6111\n",
            "Epoch 13/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5367 - categorical_accuracy: 0.7365 - val_loss: 0.6566 - val_categorical_accuracy: 0.6111\n",
            "Epoch 14/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5362 - categorical_accuracy: 0.7365 - val_loss: 0.6594 - val_categorical_accuracy: 0.6111\n",
            "Epoch 15/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5359 - categorical_accuracy: 0.7365 - val_loss: 0.6641 - val_categorical_accuracy: 0.6111\n",
            "Epoch 16/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5353 - categorical_accuracy: 0.7365 - val_loss: 0.6656 - val_categorical_accuracy: 0.6111\n",
            "Epoch 17/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5351 - categorical_accuracy: 0.7386 - val_loss: 0.6679 - val_categorical_accuracy: 0.6111\n",
            "Epoch 18/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5348 - categorical_accuracy: 0.7365 - val_loss: 0.6669 - val_categorical_accuracy: 0.6111\n",
            "Epoch 19/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5344 - categorical_accuracy: 0.7365 - val_loss: 0.6728 - val_categorical_accuracy: 0.6111\n",
            "Epoch 20/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5332 - categorical_accuracy: 0.7365 - val_loss: 0.6790 - val_categorical_accuracy: 0.6111\n",
            "Epoch 21/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5333 - categorical_accuracy: 0.7365 - val_loss: 0.6849 - val_categorical_accuracy: 0.6111\n",
            "Epoch 22/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5332 - categorical_accuracy: 0.7365 - val_loss: 0.6922 - val_categorical_accuracy: 0.6111\n",
            "Epoch 23/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5333 - categorical_accuracy: 0.7365 - val_loss: 0.6972 - val_categorical_accuracy: 0.6111\n",
            "Epoch 24/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5330 - categorical_accuracy: 0.7365 - val_loss: 0.6979 - val_categorical_accuracy: 0.6111\n",
            "Epoch 25/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5328 - categorical_accuracy: 0.7365 - val_loss: 0.6952 - val_categorical_accuracy: 0.6111\n",
            "Epoch 26/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5329 - categorical_accuracy: 0.7365 - val_loss: 0.6921 - val_categorical_accuracy: 0.6111\n",
            "Epoch 27/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5318 - categorical_accuracy: 0.7365 - val_loss: 0.6888 - val_categorical_accuracy: 0.6111\n",
            "Epoch 28/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5322 - categorical_accuracy: 0.7365 - val_loss: 0.6902 - val_categorical_accuracy: 0.6111\n",
            "Epoch 29/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5309 - categorical_accuracy: 0.7365 - val_loss: 0.6934 - val_categorical_accuracy: 0.6111\n",
            "Epoch 30/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5313 - categorical_accuracy: 0.7344 - val_loss: 0.6950 - val_categorical_accuracy: 0.6111\n",
            "Epoch 31/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5307 - categorical_accuracy: 0.7386 - val_loss: 0.6995 - val_categorical_accuracy: 0.6111\n",
            "Epoch 32/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5305 - categorical_accuracy: 0.7365 - val_loss: 0.7021 - val_categorical_accuracy: 0.6111\n",
            "Epoch 33/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5300 - categorical_accuracy: 0.7365 - val_loss: 0.7008 - val_categorical_accuracy: 0.6111\n",
            "Epoch 34/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5296 - categorical_accuracy: 0.7365 - val_loss: 0.7003 - val_categorical_accuracy: 0.6111\n",
            "Epoch 35/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5299 - categorical_accuracy: 0.7365 - val_loss: 0.6986 - val_categorical_accuracy: 0.6111\n",
            "Epoch 36/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5303 - categorical_accuracy: 0.7365 - val_loss: 0.6979 - val_categorical_accuracy: 0.6111\n",
            "Epoch 37/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5301 - categorical_accuracy: 0.7365 - val_loss: 0.6997 - val_categorical_accuracy: 0.6111\n",
            "Epoch 38/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5291 - categorical_accuracy: 0.7365 - val_loss: 0.6995 - val_categorical_accuracy: 0.6111\n",
            "Epoch 39/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5289 - categorical_accuracy: 0.7365 - val_loss: 0.6999 - val_categorical_accuracy: 0.6111\n",
            "Epoch 40/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5293 - categorical_accuracy: 0.7386 - val_loss: 0.7032 - val_categorical_accuracy: 0.6111\n",
            "Epoch 41/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5297 - categorical_accuracy: 0.7365 - val_loss: 0.7072 - val_categorical_accuracy: 0.6111\n",
            "Epoch 42/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5292 - categorical_accuracy: 0.7386 - val_loss: 0.7127 - val_categorical_accuracy: 0.6111\n",
            "Epoch 43/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5289 - categorical_accuracy: 0.7365 - val_loss: 0.7161 - val_categorical_accuracy: 0.6111\n",
            "Epoch 44/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5285 - categorical_accuracy: 0.7365 - val_loss: 0.7162 - val_categorical_accuracy: 0.6111\n",
            "Epoch 45/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5284 - categorical_accuracy: 0.7365 - val_loss: 0.7169 - val_categorical_accuracy: 0.6111\n",
            "Epoch 46/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5286 - categorical_accuracy: 0.7365 - val_loss: 0.7166 - val_categorical_accuracy: 0.6111\n",
            "Epoch 47/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5284 - categorical_accuracy: 0.7365 - val_loss: 0.7142 - val_categorical_accuracy: 0.6111\n",
            "Epoch 48/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5279 - categorical_accuracy: 0.7365 - val_loss: 0.7135 - val_categorical_accuracy: 0.6111\n",
            "Epoch 49/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5274 - categorical_accuracy: 0.7365 - val_loss: 0.7138 - val_categorical_accuracy: 0.6111\n",
            "Epoch 50/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5276 - categorical_accuracy: 0.7365 - val_loss: 0.7137 - val_categorical_accuracy: 0.6111\n",
            "Epoch 51/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5273 - categorical_accuracy: 0.7365 - val_loss: 0.7128 - val_categorical_accuracy: 0.6111\n",
            "Epoch 52/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5274 - categorical_accuracy: 0.7365 - val_loss: 0.7122 - val_categorical_accuracy: 0.6111\n",
            "Epoch 53/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5277 - categorical_accuracy: 0.7365 - val_loss: 0.7117 - val_categorical_accuracy: 0.6111\n",
            "Epoch 54/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5274 - categorical_accuracy: 0.7365 - val_loss: 0.7130 - val_categorical_accuracy: 0.6111\n",
            "Epoch 55/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5275 - categorical_accuracy: 0.7365 - val_loss: 0.7189 - val_categorical_accuracy: 0.6111\n",
            "Epoch 56/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5271 - categorical_accuracy: 0.7365 - val_loss: 0.7217 - val_categorical_accuracy: 0.6111\n",
            "Epoch 57/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5274 - categorical_accuracy: 0.7365 - val_loss: 0.7262 - val_categorical_accuracy: 0.6111\n",
            "Epoch 58/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5270 - categorical_accuracy: 0.7365 - val_loss: 0.7314 - val_categorical_accuracy: 0.6111\n",
            "Epoch 59/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5263 - categorical_accuracy: 0.7365 - val_loss: 0.7335 - val_categorical_accuracy: 0.6111\n",
            "Epoch 60/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5260 - categorical_accuracy: 0.7365 - val_loss: 0.7328 - val_categorical_accuracy: 0.6111\n",
            "Epoch 61/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5270 - categorical_accuracy: 0.7365 - val_loss: 0.7331 - val_categorical_accuracy: 0.5926\n",
            "Epoch 62/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5282 - categorical_accuracy: 0.7344 - val_loss: 0.7309 - val_categorical_accuracy: 0.5926\n",
            "Epoch 63/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5278 - categorical_accuracy: 0.7344 - val_loss: 0.7311 - val_categorical_accuracy: 0.5926\n",
            "Epoch 64/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5265 - categorical_accuracy: 0.7386 - val_loss: 0.7329 - val_categorical_accuracy: 0.6111\n",
            "Epoch 65/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5254 - categorical_accuracy: 0.7365 - val_loss: 0.7346 - val_categorical_accuracy: 0.6111\n",
            "Epoch 66/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5261 - categorical_accuracy: 0.7365 - val_loss: 0.7356 - val_categorical_accuracy: 0.6111\n",
            "Epoch 67/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5272 - categorical_accuracy: 0.7365 - val_loss: 0.7314 - val_categorical_accuracy: 0.6111\n",
            "Epoch 68/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5280 - categorical_accuracy: 0.7365 - val_loss: 0.7293 - val_categorical_accuracy: 0.6111\n",
            "Epoch 69/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5267 - categorical_accuracy: 0.7365 - val_loss: 0.7279 - val_categorical_accuracy: 0.6111\n",
            "Epoch 70/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5251 - categorical_accuracy: 0.7365 - val_loss: 0.7266 - val_categorical_accuracy: 0.6111\n",
            "Epoch 71/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5271 - categorical_accuracy: 0.7324 - val_loss: 0.7282 - val_categorical_accuracy: 0.5926\n",
            "Epoch 72/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5279 - categorical_accuracy: 0.7344 - val_loss: 0.7312 - val_categorical_accuracy: 0.5926\n",
            "Epoch 73/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5269 - categorical_accuracy: 0.7344 - val_loss: 0.7345 - val_categorical_accuracy: 0.6111\n",
            "Epoch 74/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5256 - categorical_accuracy: 0.7365 - val_loss: 0.7380 - val_categorical_accuracy: 0.6111\n",
            "Epoch 75/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5247 - categorical_accuracy: 0.7344 - val_loss: 0.7425 - val_categorical_accuracy: 0.6111\n",
            "Epoch 76/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5246 - categorical_accuracy: 0.7365 - val_loss: 0.7460 - val_categorical_accuracy: 0.6111\n",
            "Epoch 77/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5256 - categorical_accuracy: 0.7365 - val_loss: 0.7515 - val_categorical_accuracy: 0.6111\n",
            "Epoch 78/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5262 - categorical_accuracy: 0.7365 - val_loss: 0.7499 - val_categorical_accuracy: 0.6111\n",
            "Epoch 79/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5249 - categorical_accuracy: 0.7365 - val_loss: 0.7441 - val_categorical_accuracy: 0.6111\n",
            "Epoch 80/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5240 - categorical_accuracy: 0.7365 - val_loss: 0.7385 - val_categorical_accuracy: 0.6111\n",
            "Epoch 81/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5242 - categorical_accuracy: 0.7365 - val_loss: 0.7361 - val_categorical_accuracy: 0.6111\n",
            "Epoch 82/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5254 - categorical_accuracy: 0.7365 - val_loss: 0.7370 - val_categorical_accuracy: 0.6111\n",
            "Epoch 83/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5249 - categorical_accuracy: 0.7365 - val_loss: 0.7408 - val_categorical_accuracy: 0.6111\n",
            "Epoch 84/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5249 - categorical_accuracy: 0.7365 - val_loss: 0.7438 - val_categorical_accuracy: 0.6111\n",
            "Epoch 85/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5245 - categorical_accuracy: 0.7365 - val_loss: 0.7463 - val_categorical_accuracy: 0.6111\n",
            "Epoch 86/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5243 - categorical_accuracy: 0.7344 - val_loss: 0.7457 - val_categorical_accuracy: 0.6111\n",
            "Epoch 87/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5239 - categorical_accuracy: 0.7365 - val_loss: 0.7452 - val_categorical_accuracy: 0.6111\n",
            "Epoch 88/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5242 - categorical_accuracy: 0.7365 - val_loss: 0.7441 - val_categorical_accuracy: 0.6111\n",
            "Epoch 89/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5244 - categorical_accuracy: 0.7365 - val_loss: 0.7441 - val_categorical_accuracy: 0.6111\n",
            "Epoch 90/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5243 - categorical_accuracy: 0.7365 - val_loss: 0.7444 - val_categorical_accuracy: 0.6111\n",
            "Epoch 91/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5239 - categorical_accuracy: 0.7365 - val_loss: 0.7459 - val_categorical_accuracy: 0.6111\n",
            "Epoch 92/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5238 - categorical_accuracy: 0.7365 - val_loss: 0.7467 - val_categorical_accuracy: 0.6111\n",
            "Epoch 93/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5241 - categorical_accuracy: 0.7365 - val_loss: 0.7499 - val_categorical_accuracy: 0.6111\n",
            "Epoch 94/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5236 - categorical_accuracy: 0.7365 - val_loss: 0.7521 - val_categorical_accuracy: 0.6111\n",
            "Epoch 95/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5233 - categorical_accuracy: 0.7365 - val_loss: 0.7543 - val_categorical_accuracy: 0.6111\n",
            "Epoch 96/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5234 - categorical_accuracy: 0.7344 - val_loss: 0.7554 - val_categorical_accuracy: 0.6111\n",
            "Epoch 97/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5237 - categorical_accuracy: 0.7365 - val_loss: 0.7573 - val_categorical_accuracy: 0.6111\n",
            "Epoch 98/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5233 - categorical_accuracy: 0.7365 - val_loss: 0.7602 - val_categorical_accuracy: 0.6111\n",
            "Epoch 99/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5236 - categorical_accuracy: 0.7365 - val_loss: 0.7630 - val_categorical_accuracy: 0.6111\n",
            "Epoch 100/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5236 - categorical_accuracy: 0.7365 - val_loss: 0.7662 - val_categorical_accuracy: 0.6111\n",
            "Epoch 101/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5236 - categorical_accuracy: 0.7365 - val_loss: 0.7690 - val_categorical_accuracy: 0.6111\n",
            "Epoch 102/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5234 - categorical_accuracy: 0.7365 - val_loss: 0.7686 - val_categorical_accuracy: 0.6111\n",
            "Epoch 103/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5230 - categorical_accuracy: 0.7365 - val_loss: 0.7671 - val_categorical_accuracy: 0.6111\n",
            "Epoch 104/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5229 - categorical_accuracy: 0.7365 - val_loss: 0.7643 - val_categorical_accuracy: 0.6111\n",
            "Epoch 105/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5225 - categorical_accuracy: 0.7365 - val_loss: 0.7630 - val_categorical_accuracy: 0.6111\n",
            "Epoch 106/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5224 - categorical_accuracy: 0.7365 - val_loss: 0.7613 - val_categorical_accuracy: 0.6111\n",
            "Epoch 107/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5226 - categorical_accuracy: 0.7365 - val_loss: 0.7577 - val_categorical_accuracy: 0.6111\n",
            "Epoch 108/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5223 - categorical_accuracy: 0.7365 - val_loss: 0.7571 - val_categorical_accuracy: 0.6111\n",
            "Epoch 109/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5222 - categorical_accuracy: 0.7365 - val_loss: 0.7582 - val_categorical_accuracy: 0.5741\n",
            "Epoch 110/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5232 - categorical_accuracy: 0.7344 - val_loss: 0.7595 - val_categorical_accuracy: 0.5741\n",
            "Epoch 111/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5227 - categorical_accuracy: 0.7365 - val_loss: 0.7558 - val_categorical_accuracy: 0.6111\n",
            "Epoch 112/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5219 - categorical_accuracy: 0.7365 - val_loss: 0.7540 - val_categorical_accuracy: 0.6111\n",
            "Epoch 113/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5219 - categorical_accuracy: 0.7365 - val_loss: 0.7530 - val_categorical_accuracy: 0.6111\n",
            "Epoch 114/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5219 - categorical_accuracy: 0.7365 - val_loss: 0.7533 - val_categorical_accuracy: 0.6111\n",
            "Epoch 115/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5223 - categorical_accuracy: 0.7365 - val_loss: 0.7556 - val_categorical_accuracy: 0.6111\n",
            "Epoch 116/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5217 - categorical_accuracy: 0.7365 - val_loss: 0.7558 - val_categorical_accuracy: 0.6111\n",
            "Epoch 117/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5214 - categorical_accuracy: 0.7365 - val_loss: 0.7585 - val_categorical_accuracy: 0.6111\n",
            "Epoch 118/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5213 - categorical_accuracy: 0.7365 - val_loss: 0.7616 - val_categorical_accuracy: 0.6111\n",
            "Epoch 119/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5209 - categorical_accuracy: 0.7365 - val_loss: 0.7677 - val_categorical_accuracy: 0.6111\n",
            "Epoch 120/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5210 - categorical_accuracy: 0.7365 - val_loss: 0.7749 - val_categorical_accuracy: 0.6111\n",
            "Epoch 121/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5212 - categorical_accuracy: 0.7365 - val_loss: 0.7796 - val_categorical_accuracy: 0.6111\n",
            "Epoch 122/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5214 - categorical_accuracy: 0.7365 - val_loss: 0.7822 - val_categorical_accuracy: 0.6111\n",
            "Epoch 123/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5212 - categorical_accuracy: 0.7365 - val_loss: 0.7809 - val_categorical_accuracy: 0.6111\n",
            "Epoch 124/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5212 - categorical_accuracy: 0.7365 - val_loss: 0.7779 - val_categorical_accuracy: 0.6111\n",
            "Epoch 125/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5208 - categorical_accuracy: 0.7365 - val_loss: 0.7767 - val_categorical_accuracy: 0.6111\n",
            "Epoch 126/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5214 - categorical_accuracy: 0.7365 - val_loss: 0.7751 - val_categorical_accuracy: 0.6111\n",
            "Epoch 127/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5207 - categorical_accuracy: 0.7365 - val_loss: 0.7737 - val_categorical_accuracy: 0.6111\n",
            "Epoch 128/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5205 - categorical_accuracy: 0.7365 - val_loss: 0.7721 - val_categorical_accuracy: 0.6111\n",
            "Epoch 129/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5202 - categorical_accuracy: 0.7365 - val_loss: 0.7724 - val_categorical_accuracy: 0.6111\n",
            "Epoch 130/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5201 - categorical_accuracy: 0.7365 - val_loss: 0.7707 - val_categorical_accuracy: 0.6111\n",
            "Epoch 131/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5200 - categorical_accuracy: 0.7365 - val_loss: 0.7695 - val_categorical_accuracy: 0.6111\n",
            "Epoch 132/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5200 - categorical_accuracy: 0.7407 - val_loss: 0.7678 - val_categorical_accuracy: 0.6111\n",
            "Epoch 133/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5202 - categorical_accuracy: 0.7365 - val_loss: 0.7664 - val_categorical_accuracy: 0.6111\n",
            "Epoch 134/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5200 - categorical_accuracy: 0.7365 - val_loss: 0.7661 - val_categorical_accuracy: 0.6111\n",
            "Epoch 135/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5197 - categorical_accuracy: 0.7386 - val_loss: 0.7652 - val_categorical_accuracy: 0.6111\n",
            "Epoch 136/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5195 - categorical_accuracy: 0.7365 - val_loss: 0.7657 - val_categorical_accuracy: 0.6111\n",
            "Epoch 137/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5196 - categorical_accuracy: 0.7365 - val_loss: 0.7692 - val_categorical_accuracy: 0.6111\n",
            "Epoch 138/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5201 - categorical_accuracy: 0.7365 - val_loss: 0.7716 - val_categorical_accuracy: 0.6111\n",
            "Epoch 139/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5198 - categorical_accuracy: 0.7365 - val_loss: 0.7724 - val_categorical_accuracy: 0.6111\n",
            "Epoch 140/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5192 - categorical_accuracy: 0.7365 - val_loss: 0.7751 - val_categorical_accuracy: 0.6111\n",
            "Epoch 141/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5208 - categorical_accuracy: 0.7365 - val_loss: 0.7783 - val_categorical_accuracy: 0.5741\n",
            "Epoch 142/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5207 - categorical_accuracy: 0.7344 - val_loss: 0.7775 - val_categorical_accuracy: 0.5741\n",
            "Epoch 143/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5204 - categorical_accuracy: 0.7344 - val_loss: 0.7755 - val_categorical_accuracy: 0.6111\n",
            "Epoch 144/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5201 - categorical_accuracy: 0.7365 - val_loss: 0.7737 - val_categorical_accuracy: 0.6111\n",
            "Epoch 145/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5195 - categorical_accuracy: 0.7324 - val_loss: 0.7728 - val_categorical_accuracy: 0.6111\n",
            "Epoch 146/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5197 - categorical_accuracy: 0.7365 - val_loss: 0.7708 - val_categorical_accuracy: 0.6111\n",
            "Epoch 147/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5194 - categorical_accuracy: 0.7365 - val_loss: 0.7673 - val_categorical_accuracy: 0.6111\n",
            "Epoch 148/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5193 - categorical_accuracy: 0.7344 - val_loss: 0.7669 - val_categorical_accuracy: 0.6111\n",
            "Epoch 149/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5192 - categorical_accuracy: 0.7365 - val_loss: 0.7669 - val_categorical_accuracy: 0.6111\n",
            "Epoch 150/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5190 - categorical_accuracy: 0.7365 - val_loss: 0.7667 - val_categorical_accuracy: 0.6111\n",
            "Epoch 151/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5190 - categorical_accuracy: 0.7365 - val_loss: 0.7649 - val_categorical_accuracy: 0.6111\n",
            "Epoch 152/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5195 - categorical_accuracy: 0.7365 - val_loss: 0.7623 - val_categorical_accuracy: 0.6111\n",
            "Epoch 153/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5195 - categorical_accuracy: 0.7365 - val_loss: 0.7628 - val_categorical_accuracy: 0.6111\n",
            "Epoch 154/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5193 - categorical_accuracy: 0.7365 - val_loss: 0.7646 - val_categorical_accuracy: 0.6111\n",
            "Epoch 155/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5196 - categorical_accuracy: 0.7365 - val_loss: 0.7648 - val_categorical_accuracy: 0.6111\n",
            "Epoch 156/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5200 - categorical_accuracy: 0.7365 - val_loss: 0.7652 - val_categorical_accuracy: 0.6111\n",
            "Epoch 157/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5199 - categorical_accuracy: 0.7365 - val_loss: 0.7669 - val_categorical_accuracy: 0.6111\n",
            "Epoch 158/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5195 - categorical_accuracy: 0.7365 - val_loss: 0.7681 - val_categorical_accuracy: 0.6111\n",
            "Epoch 159/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5191 - categorical_accuracy: 0.7365 - val_loss: 0.7716 - val_categorical_accuracy: 0.6111\n",
            "Epoch 160/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5185 - categorical_accuracy: 0.7365 - val_loss: 0.7731 - val_categorical_accuracy: 0.6111\n",
            "Epoch 161/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5184 - categorical_accuracy: 0.7365 - val_loss: 0.7744 - val_categorical_accuracy: 0.6111\n",
            "Epoch 162/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5183 - categorical_accuracy: 0.7365 - val_loss: 0.7743 - val_categorical_accuracy: 0.6111\n",
            "Epoch 163/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7742 - val_categorical_accuracy: 0.6111\n",
            "Epoch 164/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7735 - val_categorical_accuracy: 0.6111\n",
            "Epoch 165/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5187 - categorical_accuracy: 0.7365 - val_loss: 0.7725 - val_categorical_accuracy: 0.6111\n",
            "Epoch 166/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5182 - categorical_accuracy: 0.7365 - val_loss: 0.7739 - val_categorical_accuracy: 0.6111\n",
            "Epoch 167/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5182 - categorical_accuracy: 0.7365 - val_loss: 0.7751 - val_categorical_accuracy: 0.6111\n",
            "Epoch 168/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7764 - val_categorical_accuracy: 0.6111\n",
            "Epoch 169/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5179 - categorical_accuracy: 0.7365 - val_loss: 0.7757 - val_categorical_accuracy: 0.6111\n",
            "Epoch 170/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5178 - categorical_accuracy: 0.7365 - val_loss: 0.7758 - val_categorical_accuracy: 0.6111\n",
            "Epoch 171/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5184 - categorical_accuracy: 0.7365 - val_loss: 0.7748 - val_categorical_accuracy: 0.6111\n",
            "Epoch 172/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5183 - categorical_accuracy: 0.7365 - val_loss: 0.7729 - val_categorical_accuracy: 0.6111\n",
            "Epoch 173/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7714 - val_categorical_accuracy: 0.6111\n",
            "Epoch 174/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5188 - categorical_accuracy: 0.7365 - val_loss: 0.7717 - val_categorical_accuracy: 0.6111\n",
            "Epoch 175/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5187 - categorical_accuracy: 0.7365 - val_loss: 0.7730 - val_categorical_accuracy: 0.6111\n",
            "Epoch 176/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5186 - categorical_accuracy: 0.7365 - val_loss: 0.7747 - val_categorical_accuracy: 0.6111\n",
            "Epoch 177/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7763 - val_categorical_accuracy: 0.6111\n",
            "Epoch 178/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5183 - categorical_accuracy: 0.7344 - val_loss: 0.7772 - val_categorical_accuracy: 0.6111\n",
            "Epoch 179/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7785 - val_categorical_accuracy: 0.6111\n",
            "Epoch 180/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5184 - categorical_accuracy: 0.7365 - val_loss: 0.7797 - val_categorical_accuracy: 0.6111\n",
            "Epoch 181/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5183 - categorical_accuracy: 0.7365 - val_loss: 0.7821 - val_categorical_accuracy: 0.6111\n",
            "Epoch 182/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5190 - categorical_accuracy: 0.7365 - val_loss: 0.7866 - val_categorical_accuracy: 0.6111\n",
            "Epoch 183/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5193 - categorical_accuracy: 0.7365 - val_loss: 0.7919 - val_categorical_accuracy: 0.6111\n",
            "Epoch 184/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5183 - categorical_accuracy: 0.7365 - val_loss: 0.7940 - val_categorical_accuracy: 0.6111\n",
            "Epoch 185/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.7923 - val_categorical_accuracy: 0.6111\n",
            "Epoch 186/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5180 - categorical_accuracy: 0.7324 - val_loss: 0.7893 - val_categorical_accuracy: 0.6111\n",
            "Epoch 187/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5196 - categorical_accuracy: 0.7324 - val_loss: 0.7856 - val_categorical_accuracy: 0.6111\n",
            "Epoch 188/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5193 - categorical_accuracy: 0.7365 - val_loss: 0.7847 - val_categorical_accuracy: 0.6111\n",
            "Epoch 189/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5184 - categorical_accuracy: 0.7365 - val_loss: 0.7861 - val_categorical_accuracy: 0.6111\n",
            "Epoch 190/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5180 - categorical_accuracy: 0.7365 - val_loss: 0.7871 - val_categorical_accuracy: 0.6111\n",
            "Epoch 191/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5179 - categorical_accuracy: 0.7365 - val_loss: 0.7893 - val_categorical_accuracy: 0.6111\n",
            "Epoch 192/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5183 - categorical_accuracy: 0.7365 - val_loss: 0.7916 - val_categorical_accuracy: 0.6111\n",
            "Epoch 193/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5188 - categorical_accuracy: 0.7344 - val_loss: 0.7939 - val_categorical_accuracy: 0.5741\n",
            "Epoch 194/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5188 - categorical_accuracy: 0.7365 - val_loss: 0.7919 - val_categorical_accuracy: 0.6111\n",
            "Epoch 195/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5181 - categorical_accuracy: 0.7365 - val_loss: 0.7877 - val_categorical_accuracy: 0.6111\n",
            "Epoch 196/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5176 - categorical_accuracy: 0.7365 - val_loss: 0.7840 - val_categorical_accuracy: 0.6111\n",
            "Epoch 197/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5173 - categorical_accuracy: 0.7344 - val_loss: 0.7820 - val_categorical_accuracy: 0.6111\n",
            "Epoch 198/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5176 - categorical_accuracy: 0.7365 - val_loss: 0.7834 - val_categorical_accuracy: 0.6111\n",
            "Epoch 199/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5170 - categorical_accuracy: 0.7344 - val_loss: 0.7858 - val_categorical_accuracy: 0.6111\n",
            "Epoch 200/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.7892 - val_categorical_accuracy: 0.6111\n",
            "Epoch 201/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5179 - categorical_accuracy: 0.7344 - val_loss: 0.7948 - val_categorical_accuracy: 0.6111\n",
            "Epoch 202/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5176 - categorical_accuracy: 0.7365 - val_loss: 0.7975 - val_categorical_accuracy: 0.6111\n",
            "Epoch 203/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5171 - categorical_accuracy: 0.7386 - val_loss: 0.7994 - val_categorical_accuracy: 0.6111\n",
            "Epoch 204/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.7994 - val_categorical_accuracy: 0.6111\n",
            "Epoch 205/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.7999 - val_categorical_accuracy: 0.6111\n",
            "Epoch 206/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5170 - categorical_accuracy: 0.7344 - val_loss: 0.8033 - val_categorical_accuracy: 0.6111\n",
            "Epoch 207/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5176 - categorical_accuracy: 0.7365 - val_loss: 0.8032 - val_categorical_accuracy: 0.6111\n",
            "Epoch 208/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5172 - categorical_accuracy: 0.7344 - val_loss: 0.8024 - val_categorical_accuracy: 0.6111\n",
            "Epoch 209/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8017 - val_categorical_accuracy: 0.6111\n",
            "Epoch 210/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8019 - val_categorical_accuracy: 0.6111\n",
            "Epoch 211/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.7993 - val_categorical_accuracy: 0.6111\n",
            "Epoch 212/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.7991 - val_categorical_accuracy: 0.6111\n",
            "Epoch 213/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5175 - categorical_accuracy: 0.7365 - val_loss: 0.7986 - val_categorical_accuracy: 0.6111\n",
            "Epoch 214/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.7953 - val_categorical_accuracy: 0.6111\n",
            "Epoch 215/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.7916 - val_categorical_accuracy: 0.6111\n",
            "Epoch 216/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.7898 - val_categorical_accuracy: 0.6111\n",
            "Epoch 217/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.7920 - val_categorical_accuracy: 0.6111\n",
            "Epoch 218/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5178 - categorical_accuracy: 0.7365 - val_loss: 0.7940 - val_categorical_accuracy: 0.5741\n",
            "Epoch 219/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5186 - categorical_accuracy: 0.7365 - val_loss: 0.7936 - val_categorical_accuracy: 0.5741\n",
            "Epoch 220/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5183 - categorical_accuracy: 0.7386 - val_loss: 0.7949 - val_categorical_accuracy: 0.6111\n",
            "Epoch 221/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5174 - categorical_accuracy: 0.7365 - val_loss: 0.7972 - val_categorical_accuracy: 0.6111\n",
            "Epoch 222/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5177 - categorical_accuracy: 0.7365 - val_loss: 0.8012 - val_categorical_accuracy: 0.6111\n",
            "Epoch 223/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8038 - val_categorical_accuracy: 0.6111\n",
            "Epoch 224/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5174 - categorical_accuracy: 0.7365 - val_loss: 0.8040 - val_categorical_accuracy: 0.6111\n",
            "Epoch 225/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5174 - categorical_accuracy: 0.7365 - val_loss: 0.8020 - val_categorical_accuracy: 0.6111\n",
            "Epoch 226/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8011 - val_categorical_accuracy: 0.6111\n",
            "Epoch 227/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8018 - val_categorical_accuracy: 0.6111\n",
            "Epoch 228/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8030 - val_categorical_accuracy: 0.6111\n",
            "Epoch 229/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8034 - val_categorical_accuracy: 0.6111\n",
            "Epoch 230/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8046 - val_categorical_accuracy: 0.6111\n",
            "Epoch 231/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8046 - val_categorical_accuracy: 0.6111\n",
            "Epoch 232/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.8047 - val_categorical_accuracy: 0.6111\n",
            "Epoch 233/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8052 - val_categorical_accuracy: 0.6111\n",
            "Epoch 234/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5173 - categorical_accuracy: 0.7386 - val_loss: 0.8055 - val_categorical_accuracy: 0.6111\n",
            "Epoch 235/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8053 - val_categorical_accuracy: 0.6111\n",
            "Epoch 236/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8054 - val_categorical_accuracy: 0.6111\n",
            "Epoch 237/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8047 - val_categorical_accuracy: 0.6111\n",
            "Epoch 238/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8044 - val_categorical_accuracy: 0.6111\n",
            "Epoch 239/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.8072 - val_categorical_accuracy: 0.6111\n",
            "Epoch 240/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5169 - categorical_accuracy: 0.7365 - val_loss: 0.8083 - val_categorical_accuracy: 0.6111\n",
            "Epoch 241/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8106 - val_categorical_accuracy: 0.6111\n",
            "Epoch 242/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5172 - categorical_accuracy: 0.7324 - val_loss: 0.8105 - val_categorical_accuracy: 0.6111\n",
            "Epoch 243/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5168 - categorical_accuracy: 0.7386 - val_loss: 0.8113 - val_categorical_accuracy: 0.6111\n",
            "Epoch 244/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8086 - val_categorical_accuracy: 0.6111\n",
            "Epoch 245/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8072 - val_categorical_accuracy: 0.6111\n",
            "Epoch 246/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.8057 - val_categorical_accuracy: 0.6111\n",
            "Epoch 247/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5190 - categorical_accuracy: 0.7365 - val_loss: 0.8045 - val_categorical_accuracy: 0.6111\n",
            "Epoch 248/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5192 - categorical_accuracy: 0.7365 - val_loss: 0.8067 - val_categorical_accuracy: 0.6111\n",
            "Epoch 249/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5182 - categorical_accuracy: 0.7365 - val_loss: 0.8092 - val_categorical_accuracy: 0.6111\n",
            "Epoch 250/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8115 - val_categorical_accuracy: 0.6111\n",
            "Epoch 251/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8144 - val_categorical_accuracy: 0.6111\n",
            "Epoch 252/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5181 - categorical_accuracy: 0.7344 - val_loss: 0.8204 - val_categorical_accuracy: 0.6111\n",
            "Epoch 253/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5180 - categorical_accuracy: 0.7344 - val_loss: 0.8224 - val_categorical_accuracy: 0.6111\n",
            "Epoch 254/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.8208 - val_categorical_accuracy: 0.6111\n",
            "Epoch 255/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5169 - categorical_accuracy: 0.7365 - val_loss: 0.8192 - val_categorical_accuracy: 0.6111\n",
            "Epoch 256/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8143 - val_categorical_accuracy: 0.6111\n",
            "Epoch 257/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8103 - val_categorical_accuracy: 0.6111\n",
            "Epoch 258/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5169 - categorical_accuracy: 0.7365 - val_loss: 0.8072 - val_categorical_accuracy: 0.6111\n",
            "Epoch 259/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8063 - val_categorical_accuracy: 0.6111\n",
            "Epoch 260/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.8019 - val_categorical_accuracy: 0.6111\n",
            "Epoch 261/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5175 - categorical_accuracy: 0.7365 - val_loss: 0.8021 - val_categorical_accuracy: 0.6111\n",
            "Epoch 262/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8018 - val_categorical_accuracy: 0.6111\n",
            "Epoch 263/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5169 - categorical_accuracy: 0.7365 - val_loss: 0.8012 - val_categorical_accuracy: 0.6111\n",
            "Epoch 264/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8023 - val_categorical_accuracy: 0.6111\n",
            "Epoch 265/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8024 - val_categorical_accuracy: 0.6111\n",
            "Epoch 266/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8050 - val_categorical_accuracy: 0.6111\n",
            "Epoch 267/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8102 - val_categorical_accuracy: 0.6111\n",
            "Epoch 268/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8144 - val_categorical_accuracy: 0.6111\n",
            "Epoch 269/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8178 - val_categorical_accuracy: 0.6111\n",
            "Epoch 270/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5162 - categorical_accuracy: 0.7386 - val_loss: 0.8220 - val_categorical_accuracy: 0.6111\n",
            "Epoch 271/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8256 - val_categorical_accuracy: 0.6111\n",
            "Epoch 272/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8251 - val_categorical_accuracy: 0.6111\n",
            "Epoch 273/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8229 - val_categorical_accuracy: 0.6111\n",
            "Epoch 274/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8180 - val_categorical_accuracy: 0.6111\n",
            "Epoch 275/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8152 - val_categorical_accuracy: 0.6111\n",
            "Epoch 276/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8144 - val_categorical_accuracy: 0.6111\n",
            "Epoch 277/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8151 - val_categorical_accuracy: 0.6111\n",
            "Epoch 278/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8137 - val_categorical_accuracy: 0.6111\n",
            "Epoch 279/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8133 - val_categorical_accuracy: 0.6111\n",
            "Epoch 280/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5171 - categorical_accuracy: 0.7324 - val_loss: 0.8150 - val_categorical_accuracy: 0.5741\n",
            "Epoch 281/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5181 - categorical_accuracy: 0.7344 - val_loss: 0.8151 - val_categorical_accuracy: 0.5741\n",
            "Epoch 282/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5184 - categorical_accuracy: 0.7344 - val_loss: 0.8142 - val_categorical_accuracy: 0.6111\n",
            "Epoch 283/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8141 - val_categorical_accuracy: 0.6111\n",
            "Epoch 284/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5169 - categorical_accuracy: 0.7365 - val_loss: 0.8133 - val_categorical_accuracy: 0.6111\n",
            "Epoch 285/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8140 - val_categorical_accuracy: 0.6111\n",
            "Epoch 286/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8171 - val_categorical_accuracy: 0.6111\n",
            "Epoch 287/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8192 - val_categorical_accuracy: 0.6111\n",
            "Epoch 288/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8210 - val_categorical_accuracy: 0.6111\n",
            "Epoch 289/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8196 - val_categorical_accuracy: 0.6111\n",
            "Epoch 290/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8177 - val_categorical_accuracy: 0.6111\n",
            "Epoch 291/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5161 - categorical_accuracy: 0.7344 - val_loss: 0.8166 - val_categorical_accuracy: 0.6111\n",
            "Epoch 292/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8171 - val_categorical_accuracy: 0.6111\n",
            "Epoch 293/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8171 - val_categorical_accuracy: 0.6111\n",
            "Epoch 294/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8190 - val_categorical_accuracy: 0.6111\n",
            "Epoch 295/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8208 - val_categorical_accuracy: 0.6111\n",
            "Epoch 296/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8212 - val_categorical_accuracy: 0.6111\n",
            "Epoch 297/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8203 - val_categorical_accuracy: 0.6111\n",
            "Epoch 298/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8179 - val_categorical_accuracy: 0.6111\n",
            "Epoch 299/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8151 - val_categorical_accuracy: 0.6111\n",
            "Epoch 300/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8151 - val_categorical_accuracy: 0.6111\n",
            "Epoch 301/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8134 - val_categorical_accuracy: 0.6111\n",
            "Epoch 302/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5172 - categorical_accuracy: 0.7365 - val_loss: 0.8156 - val_categorical_accuracy: 0.6111\n",
            "Epoch 303/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8180 - val_categorical_accuracy: 0.6111\n",
            "Epoch 304/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8193 - val_categorical_accuracy: 0.6111\n",
            "Epoch 305/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5168 - categorical_accuracy: 0.7365 - val_loss: 0.8214 - val_categorical_accuracy: 0.6111\n",
            "Epoch 306/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5171 - categorical_accuracy: 0.7365 - val_loss: 0.8219 - val_categorical_accuracy: 0.6111\n",
            "Epoch 307/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5170 - categorical_accuracy: 0.7365 - val_loss: 0.8226 - val_categorical_accuracy: 0.6111\n",
            "Epoch 308/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8217 - val_categorical_accuracy: 0.6111\n",
            "Epoch 309/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8221 - val_categorical_accuracy: 0.6111\n",
            "Epoch 310/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8226 - val_categorical_accuracy: 0.5741\n",
            "Epoch 311/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5166 - categorical_accuracy: 0.7344 - val_loss: 0.8250 - val_categorical_accuracy: 0.5741\n",
            "Epoch 312/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5168 - categorical_accuracy: 0.7344 - val_loss: 0.8274 - val_categorical_accuracy: 0.5741\n",
            "Epoch 313/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5165 - categorical_accuracy: 0.7344 - val_loss: 0.8280 - val_categorical_accuracy: 0.5741\n",
            "Epoch 314/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5164 - categorical_accuracy: 0.7324 - val_loss: 0.8292 - val_categorical_accuracy: 0.6111\n",
            "Epoch 315/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8311 - val_categorical_accuracy: 0.6111\n",
            "Epoch 316/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8310 - val_categorical_accuracy: 0.6111\n",
            "Epoch 317/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 318/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8302 - val_categorical_accuracy: 0.6111\n",
            "Epoch 319/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8270 - val_categorical_accuracy: 0.6111\n",
            "Epoch 320/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8257 - val_categorical_accuracy: 0.6111\n",
            "Epoch 321/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8226 - val_categorical_accuracy: 0.6111\n",
            "Epoch 322/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8214 - val_categorical_accuracy: 0.6111\n",
            "Epoch 323/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8222 - val_categorical_accuracy: 0.6111\n",
            "Epoch 324/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8246 - val_categorical_accuracy: 0.6111\n",
            "Epoch 325/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8245 - val_categorical_accuracy: 0.6111\n",
            "Epoch 326/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8244 - val_categorical_accuracy: 0.6111\n",
            "Epoch 327/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8229 - val_categorical_accuracy: 0.6111\n",
            "Epoch 328/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8219 - val_categorical_accuracy: 0.6111\n",
            "Epoch 329/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5166 - categorical_accuracy: 0.7386 - val_loss: 0.8213 - val_categorical_accuracy: 0.6111\n",
            "Epoch 330/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8242 - val_categorical_accuracy: 0.6111\n",
            "Epoch 331/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8261 - val_categorical_accuracy: 0.6111\n",
            "Epoch 332/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8284 - val_categorical_accuracy: 0.6111\n",
            "Epoch 333/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8305 - val_categorical_accuracy: 0.6111\n",
            "Epoch 334/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8289 - val_categorical_accuracy: 0.6111\n",
            "Epoch 335/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8250 - val_categorical_accuracy: 0.6111\n",
            "Epoch 336/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8228 - val_categorical_accuracy: 0.6111\n",
            "Epoch 337/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8198 - val_categorical_accuracy: 0.6111\n",
            "Epoch 338/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8213 - val_categorical_accuracy: 0.6111\n",
            "Epoch 339/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8254 - val_categorical_accuracy: 0.6111\n",
            "Epoch 340/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8278 - val_categorical_accuracy: 0.6111\n",
            "Epoch 341/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8269 - val_categorical_accuracy: 0.6111\n",
            "Epoch 342/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8282 - val_categorical_accuracy: 0.6111\n",
            "Epoch 343/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8302 - val_categorical_accuracy: 0.5741\n",
            "Epoch 344/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5159 - categorical_accuracy: 0.7344 - val_loss: 0.8326 - val_categorical_accuracy: 0.5741\n",
            "Epoch 345/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5168 - categorical_accuracy: 0.7344 - val_loss: 0.8336 - val_categorical_accuracy: 0.5741\n",
            "Epoch 346/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5164 - categorical_accuracy: 0.7344 - val_loss: 0.8315 - val_categorical_accuracy: 0.5741\n",
            "Epoch 347/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5160 - categorical_accuracy: 0.7386 - val_loss: 0.8304 - val_categorical_accuracy: 0.6111\n",
            "Epoch 348/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8312 - val_categorical_accuracy: 0.6111\n",
            "Epoch 349/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8348 - val_categorical_accuracy: 0.6111\n",
            "Epoch 350/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8376 - val_categorical_accuracy: 0.6111\n",
            "Epoch 351/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5160 - categorical_accuracy: 0.7344 - val_loss: 0.8378 - val_categorical_accuracy: 0.6111\n",
            "Epoch 352/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8368 - val_categorical_accuracy: 0.6111\n",
            "Epoch 353/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8337 - val_categorical_accuracy: 0.6111\n",
            "Epoch 354/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8321 - val_categorical_accuracy: 0.6111\n",
            "Epoch 355/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8328 - val_categorical_accuracy: 0.6111\n",
            "Epoch 356/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5164 - categorical_accuracy: 0.7365 - val_loss: 0.8341 - val_categorical_accuracy: 0.5741\n",
            "Epoch 357/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5173 - categorical_accuracy: 0.7344 - val_loss: 0.8348 - val_categorical_accuracy: 0.5741\n",
            "Epoch 358/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5174 - categorical_accuracy: 0.7344 - val_loss: 0.8347 - val_categorical_accuracy: 0.5741\n",
            "Epoch 359/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5170 - categorical_accuracy: 0.7324 - val_loss: 0.8335 - val_categorical_accuracy: 0.6111\n",
            "Epoch 360/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5156 - categorical_accuracy: 0.7386 - val_loss: 0.8356 - val_categorical_accuracy: 0.6111\n",
            "Epoch 361/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8358 - val_categorical_accuracy: 0.6111\n",
            "Epoch 362/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8363 - val_categorical_accuracy: 0.6111\n",
            "Epoch 363/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8358 - val_categorical_accuracy: 0.6111\n",
            "Epoch 364/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8339 - val_categorical_accuracy: 0.6111\n",
            "Epoch 365/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8329 - val_categorical_accuracy: 0.6111\n",
            "Epoch 366/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 367/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8273 - val_categorical_accuracy: 0.6111\n",
            "Epoch 368/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5168 - categorical_accuracy: 0.7344 - val_loss: 0.8239 - val_categorical_accuracy: 0.6111\n",
            "Epoch 369/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5173 - categorical_accuracy: 0.7365 - val_loss: 0.8216 - val_categorical_accuracy: 0.6111\n",
            "Epoch 370/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8231 - val_categorical_accuracy: 0.6111\n",
            "Epoch 371/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5164 - categorical_accuracy: 0.7324 - val_loss: 0.8264 - val_categorical_accuracy: 0.6111\n",
            "Epoch 372/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8301 - val_categorical_accuracy: 0.6111\n",
            "Epoch 373/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 374/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8310 - val_categorical_accuracy: 0.6111\n",
            "Epoch 375/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8280 - val_categorical_accuracy: 0.6111\n",
            "Epoch 376/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8262 - val_categorical_accuracy: 0.6111\n",
            "Epoch 377/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5166 - categorical_accuracy: 0.7365 - val_loss: 0.8282 - val_categorical_accuracy: 0.6111\n",
            "Epoch 378/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8279 - val_categorical_accuracy: 0.6111\n",
            "Epoch 379/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8298 - val_categorical_accuracy: 0.6111\n",
            "Epoch 380/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 381/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8317 - val_categorical_accuracy: 0.6111\n",
            "Epoch 382/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8349 - val_categorical_accuracy: 0.6111\n",
            "Epoch 383/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5154 - categorical_accuracy: 0.7386 - val_loss: 0.8358 - val_categorical_accuracy: 0.6111\n",
            "Epoch 384/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8384 - val_categorical_accuracy: 0.6111\n",
            "Epoch 385/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8401 - val_categorical_accuracy: 0.6111\n",
            "Epoch 386/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8437 - val_categorical_accuracy: 0.6111\n",
            "Epoch 387/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8447 - val_categorical_accuracy: 0.6111\n",
            "Epoch 388/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5155 - categorical_accuracy: 0.7324 - val_loss: 0.8457 - val_categorical_accuracy: 0.6111\n",
            "Epoch 389/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8455 - val_categorical_accuracy: 0.6111\n",
            "Epoch 390/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8450 - val_categorical_accuracy: 0.6111\n",
            "Epoch 391/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8454 - val_categorical_accuracy: 0.6111\n",
            "Epoch 392/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8458 - val_categorical_accuracy: 0.6111\n",
            "Epoch 393/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8456 - val_categorical_accuracy: 0.6111\n",
            "Epoch 394/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8464 - val_categorical_accuracy: 0.6111\n",
            "Epoch 395/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8479 - val_categorical_accuracy: 0.6111\n",
            "Epoch 396/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8462 - val_categorical_accuracy: 0.6111\n",
            "Epoch 397/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8461 - val_categorical_accuracy: 0.6111\n",
            "Epoch 398/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5167 - categorical_accuracy: 0.7365 - val_loss: 0.8442 - val_categorical_accuracy: 0.6111\n",
            "Epoch 399/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5165 - categorical_accuracy: 0.7365 - val_loss: 0.8421 - val_categorical_accuracy: 0.6111\n",
            "Epoch 400/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8368 - val_categorical_accuracy: 0.6111\n",
            "Epoch 401/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8317 - val_categorical_accuracy: 0.6111\n",
            "Epoch 402/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8280 - val_categorical_accuracy: 0.6111\n",
            "Epoch 403/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8266 - val_categorical_accuracy: 0.6111\n",
            "Epoch 404/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8253 - val_categorical_accuracy: 0.6111\n",
            "Epoch 405/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8240 - val_categorical_accuracy: 0.6111\n",
            "Epoch 406/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5163 - categorical_accuracy: 0.7365 - val_loss: 0.8249 - val_categorical_accuracy: 0.6111\n",
            "Epoch 407/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8260 - val_categorical_accuracy: 0.6111\n",
            "Epoch 408/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8286 - val_categorical_accuracy: 0.6111\n",
            "Epoch 409/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8294 - val_categorical_accuracy: 0.6111\n",
            "Epoch 410/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 411/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8313 - val_categorical_accuracy: 0.6111\n",
            "Epoch 412/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5159 - categorical_accuracy: 0.7324 - val_loss: 0.8348 - val_categorical_accuracy: 0.5741\n",
            "Epoch 413/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5163 - categorical_accuracy: 0.7344 - val_loss: 0.8377 - val_categorical_accuracy: 0.5741\n",
            "Epoch 414/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5168 - categorical_accuracy: 0.7324 - val_loss: 0.8376 - val_categorical_accuracy: 0.5741\n",
            "Epoch 415/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5169 - categorical_accuracy: 0.7324 - val_loss: 0.8360 - val_categorical_accuracy: 0.5741\n",
            "Epoch 416/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5160 - categorical_accuracy: 0.7324 - val_loss: 0.8333 - val_categorical_accuracy: 0.5741\n",
            "Epoch 417/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5156 - categorical_accuracy: 0.7344 - val_loss: 0.8309 - val_categorical_accuracy: 0.6111\n",
            "Epoch 418/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8305 - val_categorical_accuracy: 0.6111\n",
            "Epoch 419/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8317 - val_categorical_accuracy: 0.6111\n",
            "Epoch 420/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8314 - val_categorical_accuracy: 0.6111\n",
            "Epoch 421/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8334 - val_categorical_accuracy: 0.6111\n",
            "Epoch 422/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8340 - val_categorical_accuracy: 0.6111\n",
            "Epoch 423/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8370 - val_categorical_accuracy: 0.6111\n",
            "Epoch 424/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8379 - val_categorical_accuracy: 0.6111\n",
            "Epoch 425/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8354 - val_categorical_accuracy: 0.6111\n",
            "Epoch 426/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8330 - val_categorical_accuracy: 0.6111\n",
            "Epoch 427/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5154 - categorical_accuracy: 0.7344 - val_loss: 0.8326 - val_categorical_accuracy: 0.6111\n",
            "Epoch 428/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8342 - val_categorical_accuracy: 0.6111\n",
            "Epoch 429/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8379 - val_categorical_accuracy: 0.6111\n",
            "Epoch 430/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8435 - val_categorical_accuracy: 0.6111\n",
            "Epoch 431/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5160 - categorical_accuracy: 0.7365 - val_loss: 0.8479 - val_categorical_accuracy: 0.6111\n",
            "Epoch 432/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5158 - categorical_accuracy: 0.7365 - val_loss: 0.8494 - val_categorical_accuracy: 0.6111\n",
            "Epoch 433/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8521 - val_categorical_accuracy: 0.6111\n",
            "Epoch 434/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8554 - val_categorical_accuracy: 0.6111\n",
            "Epoch 435/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8574 - val_categorical_accuracy: 0.6111\n",
            "Epoch 436/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8571 - val_categorical_accuracy: 0.6111\n",
            "Epoch 437/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8533 - val_categorical_accuracy: 0.6111\n",
            "Epoch 438/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5162 - categorical_accuracy: 0.7365 - val_loss: 0.8503 - val_categorical_accuracy: 0.6111\n",
            "Epoch 439/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8464 - val_categorical_accuracy: 0.6111\n",
            "Epoch 440/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8411 - val_categorical_accuracy: 0.6111\n",
            "Epoch 441/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8389 - val_categorical_accuracy: 0.6111\n",
            "Epoch 442/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8372 - val_categorical_accuracy: 0.6111\n",
            "Epoch 443/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8299 - val_categorical_accuracy: 0.6111\n",
            "Epoch 444/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8271 - val_categorical_accuracy: 0.6111\n",
            "Epoch 445/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8294 - val_categorical_accuracy: 0.6111\n",
            "Epoch 446/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8352 - val_categorical_accuracy: 0.6111\n",
            "Epoch 447/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8381 - val_categorical_accuracy: 0.6111\n",
            "Epoch 448/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8389 - val_categorical_accuracy: 0.6111\n",
            "Epoch 449/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8377 - val_categorical_accuracy: 0.6111\n",
            "Epoch 450/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5159 - categorical_accuracy: 0.7344 - val_loss: 0.8335 - val_categorical_accuracy: 0.6111\n",
            "Epoch 451/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8328 - val_categorical_accuracy: 0.6111\n",
            "Epoch 452/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8353 - val_categorical_accuracy: 0.6111\n",
            "Epoch 453/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8377 - val_categorical_accuracy: 0.6111\n",
            "Epoch 454/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8392 - val_categorical_accuracy: 0.6111\n",
            "Epoch 455/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8411 - val_categorical_accuracy: 0.6111\n",
            "Epoch 456/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5156 - categorical_accuracy: 0.7324 - val_loss: 0.8424 - val_categorical_accuracy: 0.6111\n",
            "Epoch 457/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5149 - categorical_accuracy: 0.7365 - val_loss: 0.8446 - val_categorical_accuracy: 0.6111\n",
            "Epoch 458/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8478 - val_categorical_accuracy: 0.6111\n",
            "Epoch 459/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8485 - val_categorical_accuracy: 0.6111\n",
            "Epoch 460/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8487 - val_categorical_accuracy: 0.6111\n",
            "Epoch 461/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8461 - val_categorical_accuracy: 0.6111\n",
            "Epoch 462/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8432 - val_categorical_accuracy: 0.6111\n",
            "Epoch 463/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8404 - val_categorical_accuracy: 0.6111\n",
            "Epoch 464/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8400 - val_categorical_accuracy: 0.6111\n",
            "Epoch 465/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8371 - val_categorical_accuracy: 0.6111\n",
            "Epoch 466/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8338 - val_categorical_accuracy: 0.6111\n",
            "Epoch 467/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8315 - val_categorical_accuracy: 0.6111\n",
            "Epoch 468/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5152 - categorical_accuracy: 0.7344 - val_loss: 0.8303 - val_categorical_accuracy: 0.6111\n",
            "Epoch 469/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8332 - val_categorical_accuracy: 0.6111\n",
            "Epoch 470/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5152 - categorical_accuracy: 0.7344 - val_loss: 0.8378 - val_categorical_accuracy: 0.6111\n",
            "Epoch 471/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8413 - val_categorical_accuracy: 0.6111\n",
            "Epoch 472/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8446 - val_categorical_accuracy: 0.6111\n",
            "Epoch 473/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5147 - categorical_accuracy: 0.7365 - val_loss: 0.8502 - val_categorical_accuracy: 0.6111\n",
            "Epoch 474/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8579 - val_categorical_accuracy: 0.6111\n",
            "Epoch 475/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5153 - categorical_accuracy: 0.7365 - val_loss: 0.8619 - val_categorical_accuracy: 0.6111\n",
            "Epoch 476/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8650 - val_categorical_accuracy: 0.6111\n",
            "Epoch 477/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5156 - categorical_accuracy: 0.7365 - val_loss: 0.8653 - val_categorical_accuracy: 0.6111\n",
            "Epoch 478/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5153 - categorical_accuracy: 0.7386 - val_loss: 0.8609 - val_categorical_accuracy: 0.6111\n",
            "Epoch 479/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8531 - val_categorical_accuracy: 0.6111\n",
            "Epoch 480/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5144 - categorical_accuracy: 0.7365 - val_loss: 0.8453 - val_categorical_accuracy: 0.6111\n",
            "Epoch 481/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5149 - categorical_accuracy: 0.7365 - val_loss: 0.8361 - val_categorical_accuracy: 0.6111\n",
            "Epoch 482/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8317 - val_categorical_accuracy: 0.6111\n",
            "Epoch 483/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8304 - val_categorical_accuracy: 0.6111\n",
            "Epoch 484/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8285 - val_categorical_accuracy: 0.6111\n",
            "Epoch 485/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5156 - categorical_accuracy: 0.7344 - val_loss: 0.8263 - val_categorical_accuracy: 0.6111\n",
            "Epoch 486/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.5154 - categorical_accuracy: 0.7365 - val_loss: 0.8273 - val_categorical_accuracy: 0.6111\n",
            "Epoch 487/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5161 - categorical_accuracy: 0.7365 - val_loss: 0.8322 - val_categorical_accuracy: 0.6111\n",
            "Epoch 488/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8383 - val_categorical_accuracy: 0.6111\n",
            "Epoch 489/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5159 - categorical_accuracy: 0.7365 - val_loss: 0.8435 - val_categorical_accuracy: 0.6111\n",
            "Epoch 490/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8454 - val_categorical_accuracy: 0.6111\n",
            "Epoch 491/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5157 - categorical_accuracy: 0.7365 - val_loss: 0.8483 - val_categorical_accuracy: 0.6111\n",
            "Epoch 492/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8485 - val_categorical_accuracy: 0.6111\n",
            "Epoch 493/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5149 - categorical_accuracy: 0.7365 - val_loss: 0.8473 - val_categorical_accuracy: 0.6111\n",
            "Epoch 494/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5149 - categorical_accuracy: 0.7365 - val_loss: 0.8464 - val_categorical_accuracy: 0.6111\n",
            "Epoch 495/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5151 - categorical_accuracy: 0.7365 - val_loss: 0.8435 - val_categorical_accuracy: 0.6111\n",
            "Epoch 496/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8405 - val_categorical_accuracy: 0.6111\n",
            "Epoch 497/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5148 - categorical_accuracy: 0.7365 - val_loss: 0.8387 - val_categorical_accuracy: 0.6111\n",
            "Epoch 498/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5150 - categorical_accuracy: 0.7365 - val_loss: 0.8379 - val_categorical_accuracy: 0.6111\n",
            "Epoch 499/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5152 - categorical_accuracy: 0.7365 - val_loss: 0.8387 - val_categorical_accuracy: 0.6111\n",
            "Epoch 500/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5155 - categorical_accuracy: 0.7365 - val_loss: 0.8381 - val_categorical_accuracy: 0.6111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLnxpNkCxFZ1",
        "colab_type": "code",
        "outputId": "414a3a58-7c8b-4efe-ea24-5086f1dc85a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test our model against the hidden one\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) # just by running a deep learning model with the goddamn ages and gender, wtf"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 151us/step\n",
            "Score: 0.81, Accuracy: 0.816667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p6xrWCyR0cO",
        "colab_type": "code",
        "outputId": "a110362f-1800-4fb8-bd90-ed26930756a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## BUILD CONFUSION MATRIX\n",
        "y_train_preds = as_model.predict([sex_train, ages_train])\n",
        "y_train_real = y_train\n",
        "\n",
        "y_test_preds = as_model.predict([sex_test, ages_test])\n",
        "y_test_real = y_test\n",
        "\n",
        "# Turn to binary outputs\n",
        "y_train_preds_binary = to_binary(y_train_preds)\n",
        "y_train_real_binary = to_binary(y_train_real)\n",
        "\n",
        "y_test_preds_binary = to_binary(y_test_preds)\n",
        "y_test_real_binary = to_binary(y_test_real)\n",
        "\n",
        "# MAKE CONFUSION MATRIX, can also do the same for train data\n",
        "print (confusion_matrix(y_train_real_binary, y_train_preds_binary))\n",
        "print (confusion_matrix(y_test_real_binary, y_test_preds_binary))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 49 128]\n",
            " [ 20 339]]\n",
            "[[ 1  8]\n",
            " [ 3 48]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysTchue5QU2i",
        "colab_type": "code",
        "outputId": "7c72cb20-2f9e-4327-938e-c11cf4c1af66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['val_acc', 'train_acc'], loc='best')\n",
        "# save\n",
        "plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_acc.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['val_loss', 'train_loss'], loc='best')\n",
        "# save\n",
        "plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_val.png')\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x8LEp2VoLaz3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSU3FmZmVukfWMOyoBNI5GwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Esb6vfqURV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save our best model\n",
        "pickle.dump( as_model, open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"wb\" ) )\n",
        "# load model\n",
        "# as_model = pickle.load( open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBO4xI1FbIR3",
        "colab_type": "text"
      },
      "source": [
        "**Testing our model with an independent batch of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARhPhXtQWUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load our pickle batch of data\n",
        "with open('all_mprage_grappa/processed_brains_aug/dbatch4.pkl', 'rb') as f: # also 'total_slices_all.pkl' ## RENAMED 5 TO 7, TESTING IT\n",
        "  total_slices_test, total_slices_info_test = pickle.load(f) # stored_batches/total_slices_batch5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-x9fN2IfVqy",
        "colab_type": "code",
        "outputId": "64d54b7f-ff52-4ca7-ccef-a902427cf3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(total_slices_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 160, 160, 160, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYjNw77Lt7nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the set an array\n",
        "total_slices_test = np.array(total_slices_test)\n",
        "\n",
        "# independent test\n",
        "y_test, sex_test, ages_test = get_classification(total_slices_info_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_mFSZqfvBCK",
        "colab_type": "code",
        "outputId": "26ec20f0-3a41-45f1-fe8e-38b669f5ee47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test our model against the independent set\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 103us/step\n",
            "Score: 0.73, Accuracy: 0.517241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNCAXUfgbYIs",
        "colab_type": "text"
      },
      "source": [
        "**Make predictions with our as model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kQ5Mrw5wsVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "as_train_preds = as_model.predict([sex_train, ages_train]) # this is what we will feed into next model\n",
        "as_test_preds = as_model.predict([sex_test, ages_test]) # this is what we will use when testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA2N4vQ7yVEv",
        "colab_type": "text"
      },
      "source": [
        "**Next part, get predictions from model on ages and sex and predictions from images, feed into another deep NN to try to predict the output, then test accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3v84hoBy5AE",
        "colab_type": "code",
        "outputId": "6009a0d5-bc75-4e63-8d05-0e97fd4ecece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# ALTERNATIVELY: load our cnn_models\n",
        "cnn_model = load_model('all_mprage_grappa/stored_models/model4x/model40_aug_v0.h5') # take 040, best performing one so far"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0624 16:17:28.772655 139906754799488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHzkS1kxKWX",
        "colab_type": "code",
        "outputId": "b78c17d0-9493-4bdc-a25f-2736a612500c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#load batch slices, predict and aggregate scores\n",
        "cnn_train_preds = []\n",
        "\n",
        "# load the pickle (train with 0 to 3)\n",
        "print (training_batch_f)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) \n",
        "    # predict and aggregate\n",
        "    total_slices = np.array(total_slices)\n",
        "    \n",
        "    # split and predict, too many to handle otherwise and will crash\n",
        "    sub_arrays = np.array_split(total_slices, 50)\n",
        "    \n",
        "    for i in range(len(sub_arrays)):\n",
        "      \n",
        "      sub_array = sub_arrays[i]\n",
        "      cnn_prediction = cnn_model.predict(sub_array)\n",
        "      cnn_train_preds.extend(cnn_prediction)\n",
        "    \n",
        "print (np.shape(cnn_train_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU5fMHzOjWmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# also get the test predictions\n",
        "cnn_test_preds = cnn_model.predict(total_slices_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0UzJCQti-pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save our preds (cause it takes ages to build)\n",
        "# pickle.dump( cnn_train_preds, open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"wb\" ) )\n",
        "# pickle.dump( cnn_test_preds, open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"wb\" ) )\n",
        "## load model\n",
        "cnn_train_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"rb\" ) )\n",
        "cnn_test_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaXsBh1NzDc",
        "colab_type": "text"
      },
      "source": [
        "**Now make deep learning framework where we load our cnn train preds, our as train preds and aggregate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYsnoPcwwue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_binary(cat_array):\n",
        "  '''Function to convert categorical back to binary values'''\n",
        "  binary_output_array = []\n",
        "  for i in range(len(cat_array)):\n",
        "    binary_output_array.append(np.argmax(cat_array[i]))\n",
        "    \n",
        "  binary_output_array = np.array(binary_output_array)\n",
        "  return binary_output_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3NgFPpkMq17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create MLP models\n",
        "mlp_cnn = create_mlp(2, regress=False) # value of 1 for binary, 2 for categorical\n",
        "mlp_as = create_mlp(2, regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input_two = concatenate([mlp_cnn.output, mlp_as.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "z = Dense(8, activation=\"relu\")(combined_input_two)\n",
        "z = Dropout(0.2)(z)\n",
        "z = Dense(2, activation=\"sigmoid\")(z)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "combined_model = Model(inputs=[mlp_cnn.input, mlp_as.input], outputs=z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKsn3mJSYNR",
        "colab_type": "code",
        "outputId": "58b20d60-4b23-43da-c18a-40da5afce28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "# compile our model\n",
        "combined_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "combined_model.fit([cnn_train_preds, as_train_preds], y_train, validation_split=0.1, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5763f0b74b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_train_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    226\u001b[0m         raise ValueError('All input arrays (x) should have '\n\u001b[1;32m    227\u001b[0m                          \u001b[0;34m'the same number of samples. Got array shapes: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                          str([x.shape for x in inputs]))\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         raise ValueError('All target arrays (y) should have '\n",
            "\u001b[0;31mValueError\u001b[0m: All input arrays (x) should have the same number of samples. Got array shapes: [(300, 2), (536, 2)]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx-CqpxPUSPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle.dump( combined_model, open( \"all_mprage_grappa/z_tests/combined_model_binary.pkl\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENTzWL9pTTYR",
        "colab_type": "text"
      },
      "source": [
        "**Load last one and test it out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us7wRSU2S4rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get predictions of cnn_model on test data\n",
        "# get predictions of as_model on test data\n",
        "# input these predictions into our combined_model\n",
        "# evaluate these against the actual y-value, pray it is not overfitted..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsGbpSiXWamH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm_score, cm_acc = combined_model.evaluate([cnn_test_preds, as_preds_test], y_test)\n",
        "\n",
        "print (\"Combined model score: %.2f, and combined model accuracy: %.2f\" % (cm_score, cm_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtf-HdCoJI0",
        "colab_type": "text"
      },
      "source": [
        "**Try: XGboosting the training predicted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZDlLxAzqkaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyBt5S9aw_Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert everything to binary\n",
        "cnn_train_preds_binary = to_binary(cnn_train_preds)\n",
        "as_train_preds_binary = to_binary(as_preds_train)\n",
        "y_train_binary = to_binary(y_train)\n",
        "\n",
        "cnn_test_preds_binary = to_binary(cnn_preds_test)\n",
        "as_test_preds_binary = to_binary(as_preds_test)\n",
        "y_test_binary = to_binary(y_values_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vxZoNjxyo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.shape(y_train_binary) # no worky"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umVR6Q_esX_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make into a matrix and input this into our space\n",
        "\n",
        "combined_zipped_train = list(zip(cnn_train_preds_binary, as_train_preds_binary))\n",
        "combined_zipped_train = np.array(combined_zipped_train)\n",
        "\n",
        "# fit model with training data\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(combined_zipped_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrOAKOxjrZYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_zipped_test = list(zip(cnn_preds_test, as_preds_test))\n",
        "combined_zipped_test = np.array(combined_zipped_test)\n",
        "\n",
        "y_values_predictions = []\n",
        "for i in range(len(combined_zipped_test)):\n",
        "  cz_test_csr = csr_matrix(combined_zipped_test[i])\n",
        "  xgb_preds = xgb_model.predict(cz_test_csr)\n",
        "  y_values_predictions.append(xgb_preds)\n",
        "  \n",
        "y_values_predictions = np.array(y_values_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDhTUzOvcJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(y_values_test, y_values_predictions, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIUzf6sUW73T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_train_preds # train with this as an input\n",
        "as_preds_train # another input\n",
        "y_train # ideal output\n",
        "\n",
        "#test\n",
        "cnn_preds_test\n",
        "as_preds_test\n",
        "y_values_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8O6PliZPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXxbK0vbB3B",
        "colab_type": "text"
      },
      "source": [
        "To try: reverse classification on each model and feed these in, see how our model holds up. Then do the same for our test and compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srl7AdZxbIaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
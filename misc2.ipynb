{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "misc2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/misc2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzBN1AhkB8-",
        "colab_type": "text"
      },
      "source": [
        "**Goal: Build a joint deeplearning model which trains on ages and genders, to classify that of the fifth batch.** \n",
        "**Then ensemble this with deep learning prediction outputs of our best model to get the final classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HffPUoiMkAb4",
        "colab_type": "code",
        "outputId": "90c6557d-5dba-44bc-e396-9a2c23f7472f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# other imports to handle files\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU, Input, ReLU, concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJGlXqK7j8vD",
        "colab_type": "code",
        "outputId": "e3e79bc1-6797-4104-9ff7-1d890d34a4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# mount google drive into google colab\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where we will be working\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRqVyvgomjT",
        "colab_type": "text"
      },
      "source": [
        "**Run through our batches and train our model after building it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrA5vdtKpcqi",
        "colab_type": "code",
        "outputId": "0a1013c3-6f85-4920-8232-da04fe2256bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## Loop through all our total slices and accumulate all the total slices info\n",
        "training_batch_f = os.listdir('all_mprage_grappa/processed_brains_aug')[:3]\n",
        "print (training_batch_f)\n",
        "\n",
        "# build ultima total slices info\n",
        "tsi_ultima = []\n",
        "\n",
        "# load the pickle (train with 0 to 6)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) # stored_batches/total_slices_batch5\n",
        "    \n",
        "    tsi_ultima.extend(total_slices_info)\n",
        "    \n",
        "print (np.shape(tsi_ultima))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8sh1rCrmME",
        "colab_type": "code",
        "outputId": "9ccc90c0-a042-42c3-856c-24dbed7ae04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(tsi_ultima) # use this to train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE9O2hmGkBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classification(total_slices_info):\n",
        "    '''get information of patient from total slices info'''\n",
        "    classif = [s[2] for s in total_slices_info]\n",
        "    classif = np.array(to_categorical(classif, 2))\n",
        "    \n",
        "    # now do the same for the sex, 'F' is 0, 'M' is 1\n",
        "    sex = [s[1] for s in total_slices_info]\n",
        "    for i in range(len(sex)):\n",
        "        if sex[i] == 'F':\n",
        "            sex[i] = 0\n",
        "        if sex[i] == 'M':\n",
        "            sex[i] = 1\n",
        "    sex = np.array(to_categorical(sex, 2))\n",
        "    \n",
        "    # finally for age, one hot encode ages 0 to 100 (101 classes then)\n",
        "    ages = [s[3] for s in total_slices_info]\n",
        "    ages = np.array(to_categorical(ages, 101))\n",
        "    \n",
        "    return classif, sex, ages\n",
        "\n",
        "y_train, sex_train, ages_train = get_classification(tsi_ultima)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGfmbPdP52xt",
        "colab_type": "code",
        "outputId": "8e5669f0-8f72-44e0-8862-c16b7a07d600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def larger_info_dataset(df_path):\n",
        "  '''Get the info from any given dataframe'''\n",
        "  ## Get the classification of the y-values, sex and ages from a dataframe instead\n",
        "  big_boi_df = pd.read_csv(df_path)\n",
        "\n",
        "  # remove duplicate subject ids\n",
        "  big_boi_df = big_boi_df.drop_duplicates(subset='Subject', keep='first')\n",
        "\n",
        "  # map groups, control to 0, pd to 1. Also do this for sex, M is 1, F is 0\n",
        "  big_boi_df = big_boi_df.replace({'Group': {'Control': 0, 'PD': 1}, 'Sex': {'M':1, 'F':0}})\n",
        "\n",
        "  # split into training and test sets\n",
        "  train_df, test_df = train_test_split(big_boi_df, test_size=0.10)\n",
        "\n",
        "  print (\"Train Size: %d, Test Size: %d\" % (len(train_df), len(test_df)))\n",
        "\n",
        "  # get the values for train\n",
        "  ages_train = np.array(to_categorical(train_df['Age'].values,101))\n",
        "  sex_train = np.array(to_categorical(train_df['Sex'].values, 2))\n",
        "  y_train = np.array(to_categorical(train_df['Group'].values, 2))\n",
        "\n",
        "  # get the values for test\n",
        "  ages_test = np.array(to_categorical(test_df['Age'].values,101))\n",
        "  sex_test = np.array(to_categorical(test_df['Sex'].values, 2))\n",
        "  y_test = np.array(to_categorical(test_df['Group'].values, 2))\n",
        "  \n",
        "  return [ages_train, sex_train, y_train, ages_test, sex_test, y_test]\n",
        "\n",
        "ages_train, sex_train, y_train, ages_test, sex_test, y_test = larger_info_dataset('all_mprage_grappa/Control_PD_6_21_2019.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Size: 536, Test Size: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWqqxB8ypBgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "    '''Create our MLP network'''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "    model.add(Dense(2, activation=\"relu\"))\n",
        " \n",
        "    # check to see if the regression node should be added\n",
        "    if regress:\n",
        "        model.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "    # return our model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGyH7DU5NOwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(hist):\n",
        "  ''' Function to get our metrics from history and score as inputs'''\n",
        "\n",
        "  # actually obtain our metrics\n",
        "  val_loss = hist.history['val_loss'][0]\n",
        "  val_acc = hist.history['val_categorical_accuracy'][0]\n",
        "  train_loss = hist.history['loss'][0]\n",
        "  train_acc = hist.history['categorical_accuracy'][0]\n",
        "\n",
        "  # put everything into one array\n",
        "  return [val_loss, val_acc, train_loss, train_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OS2QxNNpHaP",
        "colab_type": "code",
        "outputId": "8536b729-1e46-486a-9848-d8ed6f683f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# creds: https://www.puzzlr.org/the-keras-functional-api-five-simple-examples/\n",
        "# Create MLP models\n",
        "mlp_sex = create_mlp(sex_train.shape[1], regress=False)\n",
        "mlp_age = create_mlp(ages_train.shape[1], regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input = concatenate([mlp_sex.output, mlp_age.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(8, activation=\"relu\")(combined_input)\n",
        "x = Dense(2, activation=\"sigmoid\")(x)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "as_model = Model(inputs=[mlp_sex.input, mlp_age.input], outputs=x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0622 08:00:17.568677 140000742377344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0622 08:00:17.598321 140000742377344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0622 08:00:17.608994 140000742377344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wNgZ89trZc",
        "colab_type": "code",
        "outputId": "16de9b10-ffba-4382-bd45-075aa79a1361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16850
        }
      },
      "source": [
        "# compile our model\n",
        "as_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "history = as_model.fit([sex_train, ages_train], y_train, validation_split=0.1, epochs=500, batch_size=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n",
            "Train on 482 samples, validate on 54 samples\n",
            "Epoch 1/500\n",
            "482/482 [==============================] - 3s 6ms/step - loss: 0.5024 - categorical_accuracy: 0.7261 - val_loss: 1.3813 - val_categorical_accuracy: 0.5741\n",
            "Epoch 2/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4996 - categorical_accuracy: 0.7282 - val_loss: 1.3705 - val_categorical_accuracy: 0.5556\n",
            "Epoch 3/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5000 - categorical_accuracy: 0.7199 - val_loss: 1.3741 - val_categorical_accuracy: 0.5370\n",
            "Epoch 4/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.4996 - categorical_accuracy: 0.7199 - val_loss: 1.3823 - val_categorical_accuracy: 0.5741\n",
            "Epoch 5/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3875 - val_categorical_accuracy: 0.5741\n",
            "Epoch 6/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3844 - val_categorical_accuracy: 0.5741\n",
            "Epoch 7/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3846 - val_categorical_accuracy: 0.5741\n",
            "Epoch 8/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3886 - val_categorical_accuracy: 0.5741\n",
            "Epoch 9/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3927 - val_categorical_accuracy: 0.5741\n",
            "Epoch 10/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 11/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4997 - categorical_accuracy: 0.7261 - val_loss: 1.3779 - val_categorical_accuracy: 0.5370\n",
            "Epoch 12/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3783 - val_categorical_accuracy: 0.5370\n",
            "Epoch 13/500\n",
            "482/482 [==============================] - 0s 70us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3795 - val_categorical_accuracy: 0.5370\n",
            "Epoch 14/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3863 - val_categorical_accuracy: 0.5741\n",
            "Epoch 15/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.3922 - val_categorical_accuracy: 0.5741\n",
            "Epoch 16/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3890 - val_categorical_accuracy: 0.5741\n",
            "Epoch 17/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3845 - val_categorical_accuracy: 0.5741\n",
            "Epoch 18/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3833 - val_categorical_accuracy: 0.5741\n",
            "Epoch 19/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3817 - val_categorical_accuracy: 0.5741\n",
            "Epoch 20/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3849 - val_categorical_accuracy: 0.5741\n",
            "Epoch 21/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3913 - val_categorical_accuracy: 0.5741\n",
            "Epoch 22/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3884 - val_categorical_accuracy: 0.5741\n",
            "Epoch 23/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4995 - categorical_accuracy: 0.7199 - val_loss: 1.3836 - val_categorical_accuracy: 0.5370\n",
            "Epoch 24/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3881 - val_categorical_accuracy: 0.5741\n",
            "Epoch 25/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4999 - categorical_accuracy: 0.7282 - val_loss: 1.3977 - val_categorical_accuracy: 0.5741\n",
            "Epoch 26/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3941 - val_categorical_accuracy: 0.5741\n",
            "Epoch 27/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3862 - val_categorical_accuracy: 0.5370\n",
            "Epoch 28/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3853 - val_categorical_accuracy: 0.5370\n",
            "Epoch 29/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3831 - val_categorical_accuracy: 0.4444\n",
            "Epoch 30/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5011 - categorical_accuracy: 0.7220 - val_loss: 1.3844 - val_categorical_accuracy: 0.4630\n",
            "Epoch 31/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5013 - categorical_accuracy: 0.7178 - val_loss: 1.3873 - val_categorical_accuracy: 0.5370\n",
            "Epoch 32/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3933 - val_categorical_accuracy: 0.5741\n",
            "Epoch 33/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3940 - val_categorical_accuracy: 0.5741\n",
            "Epoch 34/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3897 - val_categorical_accuracy: 0.5370\n",
            "Epoch 35/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.5009 - categorical_accuracy: 0.7220 - val_loss: 1.3845 - val_categorical_accuracy: 0.5370\n",
            "Epoch 36/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5008 - categorical_accuracy: 0.7199 - val_loss: 1.3879 - val_categorical_accuracy: 0.5370\n",
            "Epoch 37/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5004 - categorical_accuracy: 0.7282 - val_loss: 1.3950 - val_categorical_accuracy: 0.5741\n",
            "Epoch 38/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3958 - val_categorical_accuracy: 0.5741\n",
            "Epoch 39/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3891 - val_categorical_accuracy: 0.5741\n",
            "Epoch 40/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3864 - val_categorical_accuracy: 0.5741\n",
            "Epoch 41/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3868 - val_categorical_accuracy: 0.5370\n",
            "Epoch 42/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3926 - val_categorical_accuracy: 0.5741\n",
            "Epoch 43/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3922 - val_categorical_accuracy: 0.5741\n",
            "Epoch 44/500\n",
            "482/482 [==============================] - 0s 69us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3979 - val_categorical_accuracy: 0.5741\n",
            "Epoch 45/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.4028 - val_categorical_accuracy: 0.5741\n",
            "Epoch 46/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3932 - val_categorical_accuracy: 0.5741\n",
            "Epoch 47/500\n",
            "482/482 [==============================] - 0s 72us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3800 - val_categorical_accuracy: 0.5741\n",
            "Epoch 48/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5006 - categorical_accuracy: 0.7261 - val_loss: 1.3747 - val_categorical_accuracy: 0.5370\n",
            "Epoch 49/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5007 - categorical_accuracy: 0.7261 - val_loss: 1.3799 - val_categorical_accuracy: 0.5741\n",
            "Epoch 50/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3750 - val_categorical_accuracy: 0.5741\n",
            "Epoch 51/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5006 - categorical_accuracy: 0.7199 - val_loss: 1.3739 - val_categorical_accuracy: 0.5741\n",
            "Epoch 52/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5002 - categorical_accuracy: 0.7261 - val_loss: 1.3830 - val_categorical_accuracy: 0.5741\n",
            "Epoch 53/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3981 - val_categorical_accuracy: 0.5741\n",
            "Epoch 54/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5008 - categorical_accuracy: 0.7261 - val_loss: 1.4071 - val_categorical_accuracy: 0.5370\n",
            "Epoch 55/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5010 - categorical_accuracy: 0.7220 - val_loss: 1.3978 - val_categorical_accuracy: 0.5741\n",
            "Epoch 56/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4989 - categorical_accuracy: 0.7303 - val_loss: 1.3828 - val_categorical_accuracy: 0.5370\n",
            "Epoch 57/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5022 - categorical_accuracy: 0.7220 - val_loss: 1.3747 - val_categorical_accuracy: 0.5370\n",
            "Epoch 58/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5021 - categorical_accuracy: 0.7241 - val_loss: 1.3838 - val_categorical_accuracy: 0.5370\n",
            "Epoch 59/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3974 - val_categorical_accuracy: 0.5741\n",
            "Epoch 60/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.5010 - categorical_accuracy: 0.7241 - val_loss: 1.4051 - val_categorical_accuracy: 0.5741\n",
            "Epoch 61/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3996 - val_categorical_accuracy: 0.5741\n",
            "Epoch 62/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3963 - val_categorical_accuracy: 0.5370\n",
            "Epoch 63/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3960 - val_categorical_accuracy: 0.5370\n",
            "Epoch 64/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3982 - val_categorical_accuracy: 0.5741\n",
            "Epoch 65/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5001 - categorical_accuracy: 0.7199 - val_loss: 1.4004 - val_categorical_accuracy: 0.5741\n",
            "Epoch 66/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.4040 - val_categorical_accuracy: 0.5741\n",
            "Epoch 67/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5016 - categorical_accuracy: 0.7241 - val_loss: 1.4094 - val_categorical_accuracy: 0.5741\n",
            "Epoch 68/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.5017 - categorical_accuracy: 0.7241 - val_loss: 1.4044 - val_categorical_accuracy: 0.5741\n",
            "Epoch 69/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5010 - categorical_accuracy: 0.7241 - val_loss: 1.4052 - val_categorical_accuracy: 0.5741\n",
            "Epoch 70/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.3989 - val_categorical_accuracy: 0.5741\n",
            "Epoch 71/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3881 - val_categorical_accuracy: 0.5370\n",
            "Epoch 72/500\n",
            "482/482 [==============================] - 0s 73us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3875 - val_categorical_accuracy: 0.5370\n",
            "Epoch 73/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3861 - val_categorical_accuracy: 0.5370\n",
            "Epoch 74/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3858 - val_categorical_accuracy: 0.5370\n",
            "Epoch 75/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3879 - val_categorical_accuracy: 0.5370\n",
            "Epoch 76/500\n",
            "482/482 [==============================] - 0s 93us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3867 - val_categorical_accuracy: 0.5370\n",
            "Epoch 77/500\n",
            "482/482 [==============================] - 0s 72us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3861 - val_categorical_accuracy: 0.5370\n",
            "Epoch 78/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3850 - val_categorical_accuracy: 0.5370\n",
            "Epoch 79/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4996 - categorical_accuracy: 0.7261 - val_loss: 1.3933 - val_categorical_accuracy: 0.5370\n",
            "Epoch 80/500\n",
            "482/482 [==============================] - 0s 66us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.4003 - val_categorical_accuracy: 0.5741\n",
            "Epoch 81/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.5012 - categorical_accuracy: 0.7199 - val_loss: 1.4038 - val_categorical_accuracy: 0.5370\n",
            "Epoch 82/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5009 - categorical_accuracy: 0.7241 - val_loss: 1.3947 - val_categorical_accuracy: 0.5741\n",
            "Epoch 83/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3839 - val_categorical_accuracy: 0.5370\n",
            "Epoch 84/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3782 - val_categorical_accuracy: 0.5370\n",
            "Epoch 85/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3814 - val_categorical_accuracy: 0.5370\n",
            "Epoch 86/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3847 - val_categorical_accuracy: 0.5741\n",
            "Epoch 87/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3837 - val_categorical_accuracy: 0.5370\n",
            "Epoch 88/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3835 - val_categorical_accuracy: 0.5370\n",
            "Epoch 89/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4994 - categorical_accuracy: 0.7220 - val_loss: 1.3868 - val_categorical_accuracy: 0.5741\n",
            "Epoch 90/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3894 - val_categorical_accuracy: 0.5741\n",
            "Epoch 91/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3916 - val_categorical_accuracy: 0.5741\n",
            "Epoch 92/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3890 - val_categorical_accuracy: 0.5741\n",
            "Epoch 93/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3800 - val_categorical_accuracy: 0.5370\n",
            "Epoch 94/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3754 - val_categorical_accuracy: 0.5370\n",
            "Epoch 95/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3774 - val_categorical_accuracy: 0.5370\n",
            "Epoch 96/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3830 - val_categorical_accuracy: 0.5741\n",
            "Epoch 97/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.4999 - categorical_accuracy: 0.7220 - val_loss: 1.3859 - val_categorical_accuracy: 0.5741\n",
            "Epoch 98/500\n",
            "482/482 [==============================] - 0s 26us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3809 - val_categorical_accuracy: 0.5741\n",
            "Epoch 99/500\n",
            "482/482 [==============================] - 0s 96us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3837 - val_categorical_accuracy: 0.5741\n",
            "Epoch 100/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5003 - categorical_accuracy: 0.7261 - val_loss: 1.3910 - val_categorical_accuracy: 0.5741\n",
            "Epoch 101/500\n",
            "482/482 [==============================] - 0s 59us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.3885 - val_categorical_accuracy: 0.5741\n",
            "Epoch 102/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3781 - val_categorical_accuracy: 0.5741\n",
            "Epoch 103/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.4999 - categorical_accuracy: 0.7261 - val_loss: 1.3673 - val_categorical_accuracy: 0.5556\n",
            "Epoch 104/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5008 - categorical_accuracy: 0.7220 - val_loss: 1.3665 - val_categorical_accuracy: 0.5556\n",
            "Epoch 105/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.5008 - categorical_accuracy: 0.7199 - val_loss: 1.3734 - val_categorical_accuracy: 0.5741\n",
            "Epoch 106/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3808 - val_categorical_accuracy: 0.5741\n",
            "Epoch 107/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3794 - val_categorical_accuracy: 0.5741\n",
            "Epoch 108/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3789 - val_categorical_accuracy: 0.5741\n",
            "Epoch 109/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3785 - val_categorical_accuracy: 0.5741\n",
            "Epoch 110/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3795 - val_categorical_accuracy: 0.5741\n",
            "Epoch 111/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4997 - categorical_accuracy: 0.7261 - val_loss: 1.3719 - val_categorical_accuracy: 0.5556\n",
            "Epoch 112/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.3714 - val_categorical_accuracy: 0.5556\n",
            "Epoch 113/500\n",
            "482/482 [==============================] - 0s 71us/step - loss: 0.5005 - categorical_accuracy: 0.7241 - val_loss: 1.3768 - val_categorical_accuracy: 0.5370\n",
            "Epoch 114/500\n",
            "482/482 [==============================] - 0s 65us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3834 - val_categorical_accuracy: 0.5370\n",
            "Epoch 115/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3908 - val_categorical_accuracy: 0.5370\n",
            "Epoch 116/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3825 - val_categorical_accuracy: 0.5370\n",
            "Epoch 117/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3695 - val_categorical_accuracy: 0.5556\n",
            "Epoch 118/500\n",
            "482/482 [==============================] - 0s 74us/step - loss: 0.5017 - categorical_accuracy: 0.7220 - val_loss: 1.3690 - val_categorical_accuracy: 0.5556\n",
            "Epoch 119/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.5009 - categorical_accuracy: 0.7199 - val_loss: 1.3736 - val_categorical_accuracy: 0.5370\n",
            "Epoch 120/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3830 - val_categorical_accuracy: 0.5741\n",
            "Epoch 121/500\n",
            "482/482 [==============================] - 0s 88us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3883 - val_categorical_accuracy: 0.5741\n",
            "Epoch 122/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3899 - val_categorical_accuracy: 0.5741\n",
            "Epoch 123/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3850 - val_categorical_accuracy: 0.5741\n",
            "Epoch 124/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3792 - val_categorical_accuracy: 0.5741\n",
            "Epoch 125/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3719 - val_categorical_accuracy: 0.5370\n",
            "Epoch 126/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3787 - val_categorical_accuracy: 0.5741\n",
            "Epoch 127/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3881 - val_categorical_accuracy: 0.5741\n",
            "Epoch 128/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5012 - categorical_accuracy: 0.7158 - val_loss: 1.3892 - val_categorical_accuracy: 0.5741\n",
            "Epoch 129/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5004 - categorical_accuracy: 0.7261 - val_loss: 1.3741 - val_categorical_accuracy: 0.5741\n",
            "Epoch 130/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3675 - val_categorical_accuracy: 0.5556\n",
            "Epoch 131/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.3679 - val_categorical_accuracy: 0.5556\n",
            "Epoch 132/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5002 - categorical_accuracy: 0.7261 - val_loss: 1.3777 - val_categorical_accuracy: 0.5741\n",
            "Epoch 133/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.4995 - categorical_accuracy: 0.7220 - val_loss: 1.3899 - val_categorical_accuracy: 0.5741\n",
            "Epoch 134/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5008 - categorical_accuracy: 0.7220 - val_loss: 1.3956 - val_categorical_accuracy: 0.5370\n",
            "Epoch 135/500\n",
            "482/482 [==============================] - 0s 102us/step - loss: 0.5008 - categorical_accuracy: 0.7220 - val_loss: 1.3919 - val_categorical_accuracy: 0.5741\n",
            "Epoch 136/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3809 - val_categorical_accuracy: 0.5741\n",
            "Epoch 137/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3767 - val_categorical_accuracy: 0.5741\n",
            "Epoch 138/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5000 - categorical_accuracy: 0.7261 - val_loss: 1.3717 - val_categorical_accuracy: 0.5741\n",
            "Epoch 139/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3728 - val_categorical_accuracy: 0.5741\n",
            "Epoch 140/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3757 - val_categorical_accuracy: 0.5741\n",
            "Epoch 141/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4993 - categorical_accuracy: 0.7220 - val_loss: 1.3866 - val_categorical_accuracy: 0.5741\n",
            "Epoch 142/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3996 - val_categorical_accuracy: 0.5741\n",
            "Epoch 143/500\n",
            "482/482 [==============================] - 0s 70us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.4017 - val_categorical_accuracy: 0.5741\n",
            "Epoch 144/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.5007 - categorical_accuracy: 0.7199 - val_loss: 1.3948 - val_categorical_accuracy: 0.5741\n",
            "Epoch 145/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4994 - categorical_accuracy: 0.7220 - val_loss: 1.3798 - val_categorical_accuracy: 0.5741\n",
            "Epoch 146/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4999 - categorical_accuracy: 0.7282 - val_loss: 1.3723 - val_categorical_accuracy: 0.5370\n",
            "Epoch 147/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3774 - val_categorical_accuracy: 0.5741\n",
            "Epoch 148/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4994 - categorical_accuracy: 0.7261 - val_loss: 1.3897 - val_categorical_accuracy: 0.5741\n",
            "Epoch 149/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3961 - val_categorical_accuracy: 0.5370\n",
            "Epoch 150/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5017 - categorical_accuracy: 0.7220 - val_loss: 1.3984 - val_categorical_accuracy: 0.5370\n",
            "Epoch 151/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5012 - categorical_accuracy: 0.7220 - val_loss: 1.3833 - val_categorical_accuracy: 0.5741\n",
            "Epoch 152/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3766 - val_categorical_accuracy: 0.5741\n",
            "Epoch 153/500\n",
            "482/482 [==============================] - 0s 62us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3753 - val_categorical_accuracy: 0.5741\n",
            "Epoch 154/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3737 - val_categorical_accuracy: 0.5741\n",
            "Epoch 155/500\n",
            "482/482 [==============================] - 0s 74us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3737 - val_categorical_accuracy: 0.5741\n",
            "Epoch 156/500\n",
            "482/482 [==============================] - 0s 65us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3758 - val_categorical_accuracy: 0.5741\n",
            "Epoch 157/500\n",
            "482/482 [==============================] - 0s 74us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3706 - val_categorical_accuracy: 0.5741\n",
            "Epoch 158/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3695 - val_categorical_accuracy: 0.5370\n",
            "Epoch 159/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3746 - val_categorical_accuracy: 0.5741\n",
            "Epoch 160/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3849 - val_categorical_accuracy: 0.5741\n",
            "Epoch 161/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3845 - val_categorical_accuracy: 0.5741\n",
            "Epoch 162/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3890 - val_categorical_accuracy: 0.5741\n",
            "Epoch 163/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3906 - val_categorical_accuracy: 0.5741\n",
            "Epoch 164/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3824 - val_categorical_accuracy: 0.5370\n",
            "Epoch 165/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3748 - val_categorical_accuracy: 0.5556\n",
            "Epoch 166/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.5007 - categorical_accuracy: 0.7220 - val_loss: 1.3749 - val_categorical_accuracy: 0.5556\n",
            "Epoch 167/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.5007 - categorical_accuracy: 0.7261 - val_loss: 1.3817 - val_categorical_accuracy: 0.5370\n",
            "Epoch 168/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4998 - categorical_accuracy: 0.7178 - val_loss: 1.4018 - val_categorical_accuracy: 0.5370\n",
            "Epoch 169/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.5005 - categorical_accuracy: 0.7220 - val_loss: 1.4131 - val_categorical_accuracy: 0.5370\n",
            "Epoch 170/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5027 - categorical_accuracy: 0.7220 - val_loss: 1.4113 - val_categorical_accuracy: 0.5370\n",
            "Epoch 171/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.5015 - categorical_accuracy: 0.7220 - val_loss: 1.3994 - val_categorical_accuracy: 0.5741\n",
            "Epoch 172/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3928 - val_categorical_accuracy: 0.5741\n",
            "Epoch 173/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3933 - val_categorical_accuracy: 0.5741\n",
            "Epoch 174/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.4996 - categorical_accuracy: 0.7261 - val_loss: 1.3877 - val_categorical_accuracy: 0.5370\n",
            "Epoch 175/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3781 - val_categorical_accuracy: 0.5556\n",
            "Epoch 176/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3787 - val_categorical_accuracy: 0.5370\n",
            "Epoch 177/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3808 - val_categorical_accuracy: 0.5370\n",
            "Epoch 178/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3767 - val_categorical_accuracy: 0.5370\n",
            "Epoch 179/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3823 - val_categorical_accuracy: 0.5741\n",
            "Epoch 180/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3891 - val_categorical_accuracy: 0.5741\n",
            "Epoch 181/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3921 - val_categorical_accuracy: 0.5741\n",
            "Epoch 182/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3931 - val_categorical_accuracy: 0.5741\n",
            "Epoch 183/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3905 - val_categorical_accuracy: 0.5741\n",
            "Epoch 184/500\n",
            "482/482 [==============================] - 0s 64us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3820 - val_categorical_accuracy: 0.5741\n",
            "Epoch 185/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3760 - val_categorical_accuracy: 0.5741\n",
            "Epoch 186/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5000 - categorical_accuracy: 0.7158 - val_loss: 1.3748 - val_categorical_accuracy: 0.5926\n",
            "Epoch 187/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5007 - categorical_accuracy: 0.7220 - val_loss: 1.3744 - val_categorical_accuracy: 0.5926\n",
            "Epoch 188/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5000 - categorical_accuracy: 0.7261 - val_loss: 1.3817 - val_categorical_accuracy: 0.5741\n",
            "Epoch 189/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3847 - val_categorical_accuracy: 0.5741\n",
            "Epoch 190/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4996 - categorical_accuracy: 0.7261 - val_loss: 1.3808 - val_categorical_accuracy: 0.5741\n",
            "Epoch 191/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3738 - val_categorical_accuracy: 0.5556\n",
            "Epoch 192/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5010 - categorical_accuracy: 0.7178 - val_loss: 1.3767 - val_categorical_accuracy: 0.5741\n",
            "Epoch 193/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.3762 - val_categorical_accuracy: 0.5926\n",
            "Epoch 194/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4999 - categorical_accuracy: 0.7261 - val_loss: 1.3843 - val_categorical_accuracy: 0.5741\n",
            "Epoch 195/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5009 - categorical_accuracy: 0.7241 - val_loss: 1.3972 - val_categorical_accuracy: 0.5370\n",
            "Epoch 196/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.5007 - categorical_accuracy: 0.7220 - val_loss: 1.3916 - val_categorical_accuracy: 0.5741\n",
            "Epoch 197/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3830 - val_categorical_accuracy: 0.5741\n",
            "Epoch 198/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3787 - val_categorical_accuracy: 0.5741\n",
            "Epoch 199/500\n",
            "482/482 [==============================] - 0s 59us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3796 - val_categorical_accuracy: 0.5741\n",
            "Epoch 200/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3819 - val_categorical_accuracy: 0.5741\n",
            "Epoch 201/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3785 - val_categorical_accuracy: 0.5741\n",
            "Epoch 202/500\n",
            "482/482 [==============================] - 0s 66us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 203/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3832 - val_categorical_accuracy: 0.5741\n",
            "Epoch 204/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3811 - val_categorical_accuracy: 0.5741\n",
            "Epoch 205/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5000 - categorical_accuracy: 0.7261 - val_loss: 1.3752 - val_categorical_accuracy: 0.5741\n",
            "Epoch 206/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3805 - val_categorical_accuracy: 0.5741\n",
            "Epoch 207/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3883 - val_categorical_accuracy: 0.5741\n",
            "Epoch 208/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3888 - val_categorical_accuracy: 0.5741\n",
            "Epoch 209/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3897 - val_categorical_accuracy: 0.5741\n",
            "Epoch 210/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3906 - val_categorical_accuracy: 0.5741\n",
            "Epoch 211/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3865 - val_categorical_accuracy: 0.5741\n",
            "Epoch 212/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3817 - val_categorical_accuracy: 0.5741\n",
            "Epoch 213/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3777 - val_categorical_accuracy: 0.5741\n",
            "Epoch 214/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3814 - val_categorical_accuracy: 0.5741\n",
            "Epoch 215/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3957 - val_categorical_accuracy: 0.5741\n",
            "Epoch 216/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.4070 - val_categorical_accuracy: 0.5370\n",
            "Epoch 217/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5018 - categorical_accuracy: 0.7220 - val_loss: 1.4058 - val_categorical_accuracy: 0.5370\n",
            "Epoch 218/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5011 - categorical_accuracy: 0.7199 - val_loss: 1.3945 - val_categorical_accuracy: 0.5741\n",
            "Epoch 219/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3890 - val_categorical_accuracy: 0.5741\n",
            "Epoch 220/500\n",
            "482/482 [==============================] - 0s 73us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3842 - val_categorical_accuracy: 0.5741\n",
            "Epoch 221/500\n",
            "482/482 [==============================] - 0s 67us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 222/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3856 - val_categorical_accuracy: 0.5741\n",
            "Epoch 223/500\n",
            "482/482 [==============================] - 0s 59us/step - loss: 0.4998 - categorical_accuracy: 0.7199 - val_loss: 1.3883 - val_categorical_accuracy: 0.5741\n",
            "Epoch 224/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3826 - val_categorical_accuracy: 0.5741\n",
            "Epoch 225/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3829 - val_categorical_accuracy: 0.5741\n",
            "Epoch 226/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3859 - val_categorical_accuracy: 0.5741\n",
            "Epoch 227/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3868 - val_categorical_accuracy: 0.5741\n",
            "Epoch 228/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4995 - categorical_accuracy: 0.7220 - val_loss: 1.3775 - val_categorical_accuracy: 0.5926\n",
            "Epoch 229/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5017 - categorical_accuracy: 0.7220 - val_loss: 1.3749 - val_categorical_accuracy: 0.5926\n",
            "Epoch 230/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5012 - categorical_accuracy: 0.7220 - val_loss: 1.3782 - val_categorical_accuracy: 0.5926\n",
            "Epoch 231/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7282 - val_loss: 1.3867 - val_categorical_accuracy: 0.5741\n",
            "Epoch 232/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.4016 - val_categorical_accuracy: 0.5741\n",
            "Epoch 233/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5030 - categorical_accuracy: 0.7178 - val_loss: 1.4053 - val_categorical_accuracy: 0.5370\n",
            "Epoch 234/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5016 - categorical_accuracy: 0.7220 - val_loss: 1.3918 - val_categorical_accuracy: 0.5741\n",
            "Epoch 235/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3795 - val_categorical_accuracy: 0.5741\n",
            "Epoch 236/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3753 - val_categorical_accuracy: 0.5741\n",
            "Epoch 237/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3805 - val_categorical_accuracy: 0.5741\n",
            "Epoch 238/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3791 - val_categorical_accuracy: 0.5741\n",
            "Epoch 239/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3837 - val_categorical_accuracy: 0.5741\n",
            "Epoch 240/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.5002 - categorical_accuracy: 0.7261 - val_loss: 1.3974 - val_categorical_accuracy: 0.5741\n",
            "Epoch 241/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5005 - categorical_accuracy: 0.7241 - val_loss: 1.4038 - val_categorical_accuracy: 0.5741\n",
            "Epoch 242/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.5010 - categorical_accuracy: 0.7261 - val_loss: 1.4035 - val_categorical_accuracy: 0.5741\n",
            "Epoch 243/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.3938 - val_categorical_accuracy: 0.5741\n",
            "Epoch 244/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3924 - val_categorical_accuracy: 0.5741\n",
            "Epoch 245/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.4998 - categorical_accuracy: 0.7199 - val_loss: 1.3902 - val_categorical_accuracy: 0.5741\n",
            "Epoch 246/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3951 - val_categorical_accuracy: 0.5741\n",
            "Epoch 247/500\n",
            "482/482 [==============================] - 0s 56us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3911 - val_categorical_accuracy: 0.5741\n",
            "Epoch 248/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3828 - val_categorical_accuracy: 0.5370\n",
            "Epoch 249/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4998 - categorical_accuracy: 0.7199 - val_loss: 1.3781 - val_categorical_accuracy: 0.5556\n",
            "Epoch 250/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.5011 - categorical_accuracy: 0.7220 - val_loss: 1.3759 - val_categorical_accuracy: 0.5556\n",
            "Epoch 251/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.5010 - categorical_accuracy: 0.7220 - val_loss: 1.3806 - val_categorical_accuracy: 0.5556\n",
            "Epoch 252/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5008 - categorical_accuracy: 0.7261 - val_loss: 1.3940 - val_categorical_accuracy: 0.5741\n",
            "Epoch 253/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3967 - val_categorical_accuracy: 0.5741\n",
            "Epoch 254/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3926 - val_categorical_accuracy: 0.5741\n",
            "Epoch 255/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3909 - val_categorical_accuracy: 0.5741\n",
            "Epoch 256/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3845 - val_categorical_accuracy: 0.5370\n",
            "Epoch 257/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3840 - val_categorical_accuracy: 0.5370\n",
            "Epoch 258/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3914 - val_categorical_accuracy: 0.5741\n",
            "Epoch 259/500\n",
            "482/482 [==============================] - 0s 69us/step - loss: 0.4995 - categorical_accuracy: 0.7220 - val_loss: 1.3934 - val_categorical_accuracy: 0.5741\n",
            "Epoch 260/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3972 - val_categorical_accuracy: 0.5741\n",
            "Epoch 261/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3932 - val_categorical_accuracy: 0.5741\n",
            "Epoch 262/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3843 - val_categorical_accuracy: 0.5741\n",
            "Epoch 263/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3760 - val_categorical_accuracy: 0.5926\n",
            "Epoch 264/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3806 - val_categorical_accuracy: 0.5741\n",
            "Epoch 265/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4996 - categorical_accuracy: 0.7261 - val_loss: 1.3915 - val_categorical_accuracy: 0.5741\n",
            "Epoch 266/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5010 - categorical_accuracy: 0.7241 - val_loss: 1.4025 - val_categorical_accuracy: 0.5741\n",
            "Epoch 267/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.4004 - val_categorical_accuracy: 0.5741\n",
            "Epoch 268/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3937 - val_categorical_accuracy: 0.5741\n",
            "Epoch 269/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4993 - categorical_accuracy: 0.7261 - val_loss: 1.3836 - val_categorical_accuracy: 0.5370\n",
            "Epoch 270/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4994 - categorical_accuracy: 0.7261 - val_loss: 1.3787 - val_categorical_accuracy: 0.5556\n",
            "Epoch 271/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.3791 - val_categorical_accuracy: 0.5556\n",
            "Epoch 272/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4992 - categorical_accuracy: 0.7220 - val_loss: 1.3904 - val_categorical_accuracy: 0.5741\n",
            "Epoch 273/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.5011 - categorical_accuracy: 0.7220 - val_loss: 1.4037 - val_categorical_accuracy: 0.5741\n",
            "Epoch 274/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.4013 - val_categorical_accuracy: 0.5741\n",
            "Epoch 275/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3922 - val_categorical_accuracy: 0.5741\n",
            "Epoch 276/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3802 - val_categorical_accuracy: 0.5556\n",
            "Epoch 277/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.3762 - val_categorical_accuracy: 0.5556\n",
            "Epoch 278/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5020 - categorical_accuracy: 0.7220 - val_loss: 1.3784 - val_categorical_accuracy: 0.5556\n",
            "Epoch 279/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3880 - val_categorical_accuracy: 0.5926\n",
            "Epoch 280/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4998 - categorical_accuracy: 0.7178 - val_loss: 1.3943 - val_categorical_accuracy: 0.5741\n",
            "Epoch 281/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3968 - val_categorical_accuracy: 0.5741\n",
            "Epoch 282/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5008 - categorical_accuracy: 0.7220 - val_loss: 1.3990 - val_categorical_accuracy: 0.5741\n",
            "Epoch 283/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3959 - val_categorical_accuracy: 0.5741\n",
            "Epoch 284/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4998 - categorical_accuracy: 0.7261 - val_loss: 1.3881 - val_categorical_accuracy: 0.5926\n",
            "Epoch 285/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3842 - val_categorical_accuracy: 0.5556\n",
            "Epoch 286/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5005 - categorical_accuracy: 0.7241 - val_loss: 1.3818 - val_categorical_accuracy: 0.5556\n",
            "Epoch 287/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3889 - val_categorical_accuracy: 0.5741\n",
            "Epoch 288/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4997 - categorical_accuracy: 0.7220 - val_loss: 1.3948 - val_categorical_accuracy: 0.5741\n",
            "Epoch 289/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3875 - val_categorical_accuracy: 0.5741\n",
            "Epoch 290/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4996 - categorical_accuracy: 0.7199 - val_loss: 1.3787 - val_categorical_accuracy: 0.5926\n",
            "Epoch 291/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3753 - val_categorical_accuracy: 0.5926\n",
            "Epoch 292/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3758 - val_categorical_accuracy: 0.5926\n",
            "Epoch 293/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 294/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4999 - categorical_accuracy: 0.7261 - val_loss: 1.3986 - val_categorical_accuracy: 0.5741\n",
            "Epoch 295/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.4078 - val_categorical_accuracy: 0.5741\n",
            "Epoch 296/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5019 - categorical_accuracy: 0.7220 - val_loss: 1.4065 - val_categorical_accuracy: 0.5741\n",
            "Epoch 297/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5012 - categorical_accuracy: 0.7241 - val_loss: 1.3919 - val_categorical_accuracy: 0.5741\n",
            "Epoch 298/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4992 - categorical_accuracy: 0.7220 - val_loss: 1.3851 - val_categorical_accuracy: 0.5741\n",
            "Epoch 299/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3800 - val_categorical_accuracy: 0.5741\n",
            "Epoch 300/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3817 - val_categorical_accuracy: 0.5741\n",
            "Epoch 301/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4998 - categorical_accuracy: 0.7199 - val_loss: 1.3808 - val_categorical_accuracy: 0.5741\n",
            "Epoch 302/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5004 - categorical_accuracy: 0.7178 - val_loss: 1.3746 - val_categorical_accuracy: 0.5926\n",
            "Epoch 303/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5005 - categorical_accuracy: 0.7220 - val_loss: 1.3814 - val_categorical_accuracy: 0.5926\n",
            "Epoch 304/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4996 - categorical_accuracy: 0.7199 - val_loss: 1.3922 - val_categorical_accuracy: 0.5741\n",
            "Epoch 305/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3939 - val_categorical_accuracy: 0.5741\n",
            "Epoch 306/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3958 - val_categorical_accuracy: 0.5741\n",
            "Epoch 307/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3937 - val_categorical_accuracy: 0.5741\n",
            "Epoch 308/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3966 - val_categorical_accuracy: 0.5741\n",
            "Epoch 309/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.4038 - val_categorical_accuracy: 0.5741\n",
            "Epoch 310/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5007 - categorical_accuracy: 0.7241 - val_loss: 1.4018 - val_categorical_accuracy: 0.5741\n",
            "Epoch 311/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5000 - categorical_accuracy: 0.7199 - val_loss: 1.3903 - val_categorical_accuracy: 0.5741\n",
            "Epoch 312/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5003 - categorical_accuracy: 0.7199 - val_loss: 1.3854 - val_categorical_accuracy: 0.5926\n",
            "Epoch 313/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5008 - categorical_accuracy: 0.7137 - val_loss: 1.3837 - val_categorical_accuracy: 0.5926\n",
            "Epoch 314/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5011 - categorical_accuracy: 0.7220 - val_loss: 1.3803 - val_categorical_accuracy: 0.5926\n",
            "Epoch 315/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5014 - categorical_accuracy: 0.7220 - val_loss: 1.3809 - val_categorical_accuracy: 0.5926\n",
            "Epoch 316/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5002 - categorical_accuracy: 0.7199 - val_loss: 1.3913 - val_categorical_accuracy: 0.5741\n",
            "Epoch 317/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.4997 - categorical_accuracy: 0.7220 - val_loss: 1.4061 - val_categorical_accuracy: 0.5741\n",
            "Epoch 318/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5005 - categorical_accuracy: 0.7241 - val_loss: 1.4062 - val_categorical_accuracy: 0.5741\n",
            "Epoch 319/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3974 - val_categorical_accuracy: 0.5741\n",
            "Epoch 320/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5001 - categorical_accuracy: 0.7282 - val_loss: 1.3839 - val_categorical_accuracy: 0.5556\n",
            "Epoch 321/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3841 - val_categorical_accuracy: 0.5370\n",
            "Epoch 322/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3893 - val_categorical_accuracy: 0.5741\n",
            "Epoch 323/500\n",
            "482/482 [==============================] - 0s 71us/step - loss: 0.4994 - categorical_accuracy: 0.7220 - val_loss: 1.3980 - val_categorical_accuracy: 0.5741\n",
            "Epoch 324/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.4995 - categorical_accuracy: 0.7282 - val_loss: 1.4063 - val_categorical_accuracy: 0.5741\n",
            "Epoch 325/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.5010 - categorical_accuracy: 0.7241 - val_loss: 1.4115 - val_categorical_accuracy: 0.5741\n",
            "Epoch 326/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.5009 - categorical_accuracy: 0.7241 - val_loss: 1.4040 - val_categorical_accuracy: 0.5741\n",
            "Epoch 327/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.4992 - categorical_accuracy: 0.7220 - val_loss: 1.3902 - val_categorical_accuracy: 0.5741\n",
            "Epoch 328/500\n",
            "482/482 [==============================] - 0s 25us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3874 - val_categorical_accuracy: 0.5370\n",
            "Epoch 329/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3921 - val_categorical_accuracy: 0.5741\n",
            "Epoch 330/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.4993 - categorical_accuracy: 0.7220 - val_loss: 1.4008 - val_categorical_accuracy: 0.5741\n",
            "Epoch 331/500\n",
            "482/482 [==============================] - 0s 52us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.4036 - val_categorical_accuracy: 0.5741\n",
            "Epoch 332/500\n",
            "482/482 [==============================] - 0s 65us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.4010 - val_categorical_accuracy: 0.5741\n",
            "Epoch 333/500\n",
            "482/482 [==============================] - 0s 45us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3965 - val_categorical_accuracy: 0.5741\n",
            "Epoch 334/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4996 - categorical_accuracy: 0.7303 - val_loss: 1.3914 - val_categorical_accuracy: 0.5926\n",
            "Epoch 335/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4999 - categorical_accuracy: 0.7261 - val_loss: 1.3937 - val_categorical_accuracy: 0.5741\n",
            "Epoch 336/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3915 - val_categorical_accuracy: 0.5741\n",
            "Epoch 337/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3930 - val_categorical_accuracy: 0.5741\n",
            "Epoch 338/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3928 - val_categorical_accuracy: 0.5741\n",
            "Epoch 339/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4996 - categorical_accuracy: 0.7199 - val_loss: 1.3986 - val_categorical_accuracy: 0.5741\n",
            "Epoch 340/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.4021 - val_categorical_accuracy: 0.5741\n",
            "Epoch 341/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.4012 - val_categorical_accuracy: 0.5370\n",
            "Epoch 342/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3873 - val_categorical_accuracy: 0.5741\n",
            "Epoch 343/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3779 - val_categorical_accuracy: 0.5556\n",
            "Epoch 344/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5001 - categorical_accuracy: 0.7220 - val_loss: 1.3746 - val_categorical_accuracy: 0.5556\n",
            "Epoch 345/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3790 - val_categorical_accuracy: 0.5556\n",
            "Epoch 346/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3826 - val_categorical_accuracy: 0.5741\n",
            "Epoch 347/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3847 - val_categorical_accuracy: 0.5741\n",
            "Epoch 348/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.5008 - categorical_accuracy: 0.7199 - val_loss: 1.3712 - val_categorical_accuracy: 0.5556\n",
            "Epoch 349/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5004 - categorical_accuracy: 0.7199 - val_loss: 1.3689 - val_categorical_accuracy: 0.5556\n",
            "Epoch 350/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5009 - categorical_accuracy: 0.7220 - val_loss: 1.3723 - val_categorical_accuracy: 0.5556\n",
            "Epoch 351/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3814 - val_categorical_accuracy: 0.5741\n",
            "Epoch 352/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4994 - categorical_accuracy: 0.7261 - val_loss: 1.3983 - val_categorical_accuracy: 0.5370\n",
            "Epoch 353/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5013 - categorical_accuracy: 0.7220 - val_loss: 1.4020 - val_categorical_accuracy: 0.5370\n",
            "Epoch 354/500\n",
            "482/482 [==============================] - 0s 47us/step - loss: 0.5004 - categorical_accuracy: 0.7199 - val_loss: 1.3842 - val_categorical_accuracy: 0.5741\n",
            "Epoch 355/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5000 - categorical_accuracy: 0.7199 - val_loss: 1.3733 - val_categorical_accuracy: 0.5556\n",
            "Epoch 356/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5011 - categorical_accuracy: 0.7220 - val_loss: 1.3748 - val_categorical_accuracy: 0.5556\n",
            "Epoch 357/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3810 - val_categorical_accuracy: 0.5926\n",
            "Epoch 358/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3867 - val_categorical_accuracy: 0.5741\n",
            "Epoch 359/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3884 - val_categorical_accuracy: 0.5741\n",
            "Epoch 360/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.4996 - categorical_accuracy: 0.7282 - val_loss: 1.3856 - val_categorical_accuracy: 0.5741\n",
            "Epoch 361/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3765 - val_categorical_accuracy: 0.5556\n",
            "Epoch 362/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5008 - categorical_accuracy: 0.7282 - val_loss: 1.3812 - val_categorical_accuracy: 0.5741\n",
            "Epoch 363/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3845 - val_categorical_accuracy: 0.5741\n",
            "Epoch 364/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3850 - val_categorical_accuracy: 0.5741\n",
            "Epoch 365/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5003 - categorical_accuracy: 0.7261 - val_loss: 1.3788 - val_categorical_accuracy: 0.5741\n",
            "Epoch 366/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3828 - val_categorical_accuracy: 0.5741\n",
            "Epoch 367/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4992 - categorical_accuracy: 0.7220 - val_loss: 1.3896 - val_categorical_accuracy: 0.5741\n",
            "Epoch 368/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.3966 - val_categorical_accuracy: 0.5741\n",
            "Epoch 369/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.5004 - categorical_accuracy: 0.7241 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 370/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5001 - categorical_accuracy: 0.7178 - val_loss: 1.3759 - val_categorical_accuracy: 0.5926\n",
            "Epoch 371/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5001 - categorical_accuracy: 0.7220 - val_loss: 1.3777 - val_categorical_accuracy: 0.5926\n",
            "Epoch 372/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4997 - categorical_accuracy: 0.7220 - val_loss: 1.3829 - val_categorical_accuracy: 0.5741\n",
            "Epoch 373/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4994 - categorical_accuracy: 0.7220 - val_loss: 1.3947 - val_categorical_accuracy: 0.5741\n",
            "Epoch 374/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.4003 - val_categorical_accuracy: 0.5741\n",
            "Epoch 375/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.3999 - val_categorical_accuracy: 0.5741\n",
            "Epoch 376/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4997 - categorical_accuracy: 0.7261 - val_loss: 1.3895 - val_categorical_accuracy: 0.5741\n",
            "Epoch 377/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4995 - categorical_accuracy: 0.7199 - val_loss: 1.3778 - val_categorical_accuracy: 0.5741\n",
            "Epoch 378/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3750 - val_categorical_accuracy: 0.5370\n",
            "Epoch 379/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5009 - categorical_accuracy: 0.7178 - val_loss: 1.3725 - val_categorical_accuracy: 0.5556\n",
            "Epoch 380/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5015 - categorical_accuracy: 0.7282 - val_loss: 1.3761 - val_categorical_accuracy: 0.5370\n",
            "Epoch 381/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3840 - val_categorical_accuracy: 0.5741\n",
            "Epoch 382/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3891 - val_categorical_accuracy: 0.5741\n",
            "Epoch 383/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3907 - val_categorical_accuracy: 0.5741\n",
            "Epoch 384/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3922 - val_categorical_accuracy: 0.5741\n",
            "Epoch 385/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3923 - val_categorical_accuracy: 0.5370\n",
            "Epoch 386/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.5002 - categorical_accuracy: 0.7261 - val_loss: 1.3890 - val_categorical_accuracy: 0.5370\n",
            "Epoch 387/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3883 - val_categorical_accuracy: 0.5370\n",
            "Epoch 388/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5003 - categorical_accuracy: 0.7241 - val_loss: 1.3890 - val_categorical_accuracy: 0.5370\n",
            "Epoch 389/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3991 - val_categorical_accuracy: 0.5741\n",
            "Epoch 390/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.4085 - val_categorical_accuracy: 0.5370\n",
            "Epoch 391/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5009 - categorical_accuracy: 0.7220 - val_loss: 1.4123 - val_categorical_accuracy: 0.5370\n",
            "Epoch 392/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.5018 - categorical_accuracy: 0.7220 - val_loss: 1.4095 - val_categorical_accuracy: 0.5370\n",
            "Epoch 393/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.5014 - categorical_accuracy: 0.7220 - val_loss: 1.3946 - val_categorical_accuracy: 0.5741\n",
            "Epoch 394/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3856 - val_categorical_accuracy: 0.5370\n",
            "Epoch 395/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3875 - val_categorical_accuracy: 0.5741\n",
            "Epoch 396/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3891 - val_categorical_accuracy: 0.5741\n",
            "Epoch 397/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3916 - val_categorical_accuracy: 0.5741\n",
            "Epoch 398/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3981 - val_categorical_accuracy: 0.5741\n",
            "Epoch 399/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3955 - val_categorical_accuracy: 0.5741\n",
            "Epoch 400/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3854 - val_categorical_accuracy: 0.5741\n",
            "Epoch 401/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5000 - categorical_accuracy: 0.7261 - val_loss: 1.3763 - val_categorical_accuracy: 0.5370\n",
            "Epoch 402/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.5005 - categorical_accuracy: 0.7241 - val_loss: 1.3762 - val_categorical_accuracy: 0.5370\n",
            "Epoch 403/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3847 - val_categorical_accuracy: 0.5741\n",
            "Epoch 404/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3939 - val_categorical_accuracy: 0.5741\n",
            "Epoch 405/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5000 - categorical_accuracy: 0.7220 - val_loss: 1.3951 - val_categorical_accuracy: 0.5741\n",
            "Epoch 406/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5006 - categorical_accuracy: 0.7220 - val_loss: 1.3838 - val_categorical_accuracy: 0.5741\n",
            "Epoch 407/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3783 - val_categorical_accuracy: 0.5741\n",
            "Epoch 408/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.3711 - val_categorical_accuracy: 0.5370\n",
            "Epoch 409/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3695 - val_categorical_accuracy: 0.5741\n",
            "Epoch 410/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3785 - val_categorical_accuracy: 0.5741\n",
            "Epoch 411/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4999 - categorical_accuracy: 0.7220 - val_loss: 1.3872 - val_categorical_accuracy: 0.5741\n",
            "Epoch 412/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4999 - categorical_accuracy: 0.7220 - val_loss: 1.3857 - val_categorical_accuracy: 0.5741\n",
            "Epoch 413/500\n",
            "482/482 [==============================] - 0s 46us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3884 - val_categorical_accuracy: 0.5741\n",
            "Epoch 414/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3867 - val_categorical_accuracy: 0.5741\n",
            "Epoch 415/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.5000 - categorical_accuracy: 0.7282 - val_loss: 1.3746 - val_categorical_accuracy: 0.5370\n",
            "Epoch 416/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3695 - val_categorical_accuracy: 0.5370\n",
            "Epoch 417/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.5003 - categorical_accuracy: 0.7261 - val_loss: 1.3700 - val_categorical_accuracy: 0.5370\n",
            "Epoch 418/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3767 - val_categorical_accuracy: 0.5741\n",
            "Epoch 419/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3749 - val_categorical_accuracy: 0.5741\n",
            "Epoch 420/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5001 - categorical_accuracy: 0.7220 - val_loss: 1.3703 - val_categorical_accuracy: 0.5741\n",
            "Epoch 421/500\n",
            "482/482 [==============================] - 0s 77us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3712 - val_categorical_accuracy: 0.5741\n",
            "Epoch 422/500\n",
            "482/482 [==============================] - 0s 79us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3775 - val_categorical_accuracy: 0.5741\n",
            "Epoch 423/500\n",
            "482/482 [==============================] - 0s 69us/step - loss: 0.4995 - categorical_accuracy: 0.7199 - val_loss: 1.3865 - val_categorical_accuracy: 0.5741\n",
            "Epoch 424/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5015 - categorical_accuracy: 0.7241 - val_loss: 1.3926 - val_categorical_accuracy: 0.5741\n",
            "Epoch 425/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.5015 - categorical_accuracy: 0.7241 - val_loss: 1.3860 - val_categorical_accuracy: 0.5741\n",
            "Epoch 426/500\n",
            "482/482 [==============================] - 0s 63us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.3785 - val_categorical_accuracy: 0.5741\n",
            "Epoch 427/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3753 - val_categorical_accuracy: 0.5741\n",
            "Epoch 428/500\n",
            "482/482 [==============================] - 0s 57us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3677 - val_categorical_accuracy: 0.5741\n",
            "Epoch 429/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3637 - val_categorical_accuracy: 0.5741\n",
            "Epoch 430/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.5002 - categorical_accuracy: 0.7241 - val_loss: 1.3612 - val_categorical_accuracy: 0.5741\n",
            "Epoch 431/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.5001 - categorical_accuracy: 0.7241 - val_loss: 1.3630 - val_categorical_accuracy: 0.5741\n",
            "Epoch 432/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.3721 - val_categorical_accuracy: 0.5741\n",
            "Epoch 433/500\n",
            "482/482 [==============================] - 0s 33us/step - loss: 0.4996 - categorical_accuracy: 0.7199 - val_loss: 1.3723 - val_categorical_accuracy: 0.5741\n",
            "Epoch 434/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3798 - val_categorical_accuracy: 0.5741\n",
            "Epoch 435/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3839 - val_categorical_accuracy: 0.5741\n",
            "Epoch 436/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3787 - val_categorical_accuracy: 0.5741\n",
            "Epoch 437/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5003 - categorical_accuracy: 0.7220 - val_loss: 1.3750 - val_categorical_accuracy: 0.5741\n",
            "Epoch 438/500\n",
            "482/482 [==============================] - 0s 37us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3811 - val_categorical_accuracy: 0.5741\n",
            "Epoch 439/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3855 - val_categorical_accuracy: 0.5741\n",
            "Epoch 440/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3804 - val_categorical_accuracy: 0.5741\n",
            "Epoch 441/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3706 - val_categorical_accuracy: 0.5926\n",
            "Epoch 442/500\n",
            "482/482 [==============================] - 0s 61us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3703 - val_categorical_accuracy: 0.5556\n",
            "Epoch 443/500\n",
            "482/482 [==============================] - 0s 69us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3777 - val_categorical_accuracy: 0.5370\n",
            "Epoch 444/500\n",
            "482/482 [==============================] - 0s 60us/step - loss: 0.4991 - categorical_accuracy: 0.7199 - val_loss: 1.3896 - val_categorical_accuracy: 0.5741\n",
            "Epoch 445/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3985 - val_categorical_accuracy: 0.5741\n",
            "Epoch 446/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.5000 - categorical_accuracy: 0.7241 - val_loss: 1.4038 - val_categorical_accuracy: 0.5741\n",
            "Epoch 447/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.5009 - categorical_accuracy: 0.7241 - val_loss: 1.4061 - val_categorical_accuracy: 0.5741\n",
            "Epoch 448/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5007 - categorical_accuracy: 0.7261 - val_loss: 1.3957 - val_categorical_accuracy: 0.5741\n",
            "Epoch 449/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3878 - val_categorical_accuracy: 0.5741\n",
            "Epoch 450/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3834 - val_categorical_accuracy: 0.5370\n",
            "Epoch 451/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4999 - categorical_accuracy: 0.7241 - val_loss: 1.3882 - val_categorical_accuracy: 0.5370\n",
            "Epoch 452/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3915 - val_categorical_accuracy: 0.5741\n",
            "Epoch 453/500\n",
            "482/482 [==============================] - 0s 59us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3979 - val_categorical_accuracy: 0.5741\n",
            "Epoch 454/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4999 - categorical_accuracy: 0.7220 - val_loss: 1.4095 - val_categorical_accuracy: 0.5741\n",
            "Epoch 455/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.4097 - val_categorical_accuracy: 0.5741\n",
            "Epoch 456/500\n",
            "482/482 [==============================] - 0s 38us/step - loss: 0.5006 - categorical_accuracy: 0.7241 - val_loss: 1.4023 - val_categorical_accuracy: 0.5741\n",
            "Epoch 457/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.5002 - categorical_accuracy: 0.7220 - val_loss: 1.3885 - val_categorical_accuracy: 0.5741\n",
            "Epoch 458/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4995 - categorical_accuracy: 0.7261 - val_loss: 1.3838 - val_categorical_accuracy: 0.5370\n",
            "Epoch 459/500\n",
            "482/482 [==============================] - 0s 71us/step - loss: 0.4994 - categorical_accuracy: 0.7282 - val_loss: 1.3880 - val_categorical_accuracy: 0.5741\n",
            "Epoch 460/500\n",
            "482/482 [==============================] - 0s 42us/step - loss: 0.4991 - categorical_accuracy: 0.7261 - val_loss: 1.3911 - val_categorical_accuracy: 0.5741\n",
            "Epoch 461/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3967 - val_categorical_accuracy: 0.5741\n",
            "Epoch 462/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3958 - val_categorical_accuracy: 0.5741\n",
            "Epoch 463/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4994 - categorical_accuracy: 0.7241 - val_loss: 1.3944 - val_categorical_accuracy: 0.5741\n",
            "Epoch 464/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3875 - val_categorical_accuracy: 0.5741\n",
            "Epoch 465/500\n",
            "482/482 [==============================] - 0s 53us/step - loss: 0.4991 - categorical_accuracy: 0.7220 - val_loss: 1.3805 - val_categorical_accuracy: 0.5370\n",
            "Epoch 466/500\n",
            "482/482 [==============================] - 0s 40us/step - loss: 0.4990 - categorical_accuracy: 0.7220 - val_loss: 1.3749 - val_categorical_accuracy: 0.5556\n",
            "Epoch 467/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3707 - val_categorical_accuracy: 0.5556\n",
            "Epoch 468/500\n",
            "482/482 [==============================] - 0s 41us/step - loss: 0.5003 - categorical_accuracy: 0.7178 - val_loss: 1.3765 - val_categorical_accuracy: 0.5370\n",
            "Epoch 469/500\n",
            "482/482 [==============================] - 0s 39us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3829 - val_categorical_accuracy: 0.5370\n",
            "Epoch 470/500\n",
            "482/482 [==============================] - 0s 44us/step - loss: 0.4990 - categorical_accuracy: 0.7261 - val_loss: 1.3928 - val_categorical_accuracy: 0.5741\n",
            "Epoch 471/500\n",
            "482/482 [==============================] - 0s 31us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.4044 - val_categorical_accuracy: 0.5741\n",
            "Epoch 472/500\n",
            "482/482 [==============================] - 0s 36us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.4069 - val_categorical_accuracy: 0.5741\n",
            "Epoch 473/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.5008 - categorical_accuracy: 0.7241 - val_loss: 1.4033 - val_categorical_accuracy: 0.5741\n",
            "Epoch 474/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.5004 - categorical_accuracy: 0.7220 - val_loss: 1.3937 - val_categorical_accuracy: 0.5370\n",
            "Epoch 475/500\n",
            "482/482 [==============================] - 0s 27us/step - loss: 0.4991 - categorical_accuracy: 0.7158 - val_loss: 1.3926 - val_categorical_accuracy: 0.5370\n",
            "Epoch 476/500\n",
            "482/482 [==============================] - 0s 35us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3902 - val_categorical_accuracy: 0.5370\n",
            "Epoch 477/500\n",
            "482/482 [==============================] - 0s 58us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3905 - val_categorical_accuracy: 0.5370\n",
            "Epoch 478/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4995 - categorical_accuracy: 0.7220 - val_loss: 1.3832 - val_categorical_accuracy: 0.5556\n",
            "Epoch 479/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4996 - categorical_accuracy: 0.7220 - val_loss: 1.3834 - val_categorical_accuracy: 0.5556\n",
            "Epoch 480/500\n",
            "482/482 [==============================] - 0s 55us/step - loss: 0.4998 - categorical_accuracy: 0.7241 - val_loss: 1.3917 - val_categorical_accuracy: 0.5741\n",
            "Epoch 481/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3924 - val_categorical_accuracy: 0.5741\n",
            "Epoch 482/500\n",
            "482/482 [==============================] - 0s 70us/step - loss: 0.4994 - categorical_accuracy: 0.7199 - val_loss: 1.3812 - val_categorical_accuracy: 0.5556\n",
            "Epoch 483/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4995 - categorical_accuracy: 0.7220 - val_loss: 1.3788 - val_categorical_accuracy: 0.5556\n",
            "Epoch 484/500\n",
            "482/482 [==============================] - 0s 61us/step - loss: 0.4998 - categorical_accuracy: 0.7220 - val_loss: 1.3765 - val_categorical_accuracy: 0.5556\n",
            "Epoch 485/500\n",
            "482/482 [==============================] - 0s 51us/step - loss: 0.5001 - categorical_accuracy: 0.7178 - val_loss: 1.3821 - val_categorical_accuracy: 0.5926\n",
            "Epoch 486/500\n",
            "482/482 [==============================] - 0s 34us/step - loss: 0.4991 - categorical_accuracy: 0.7220 - val_loss: 1.3873 - val_categorical_accuracy: 0.5741\n",
            "Epoch 487/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3960 - val_categorical_accuracy: 0.5741\n",
            "Epoch 488/500\n",
            "482/482 [==============================] - 0s 29us/step - loss: 0.4997 - categorical_accuracy: 0.7241 - val_loss: 1.3938 - val_categorical_accuracy: 0.5741\n",
            "Epoch 489/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3908 - val_categorical_accuracy: 0.5741\n",
            "Epoch 490/500\n",
            "482/482 [==============================] - 0s 32us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3882 - val_categorical_accuracy: 0.5741\n",
            "Epoch 491/500\n",
            "482/482 [==============================] - 0s 28us/step - loss: 0.4995 - categorical_accuracy: 0.7241 - val_loss: 1.3841 - val_categorical_accuracy: 0.5741\n",
            "Epoch 492/500\n",
            "482/482 [==============================] - 0s 30us/step - loss: 0.4991 - categorical_accuracy: 0.7241 - val_loss: 1.3866 - val_categorical_accuracy: 0.5741\n",
            "Epoch 493/500\n",
            "482/482 [==============================] - 0s 48us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3863 - val_categorical_accuracy: 0.5741\n",
            "Epoch 494/500\n",
            "482/482 [==============================] - 0s 73us/step - loss: 0.4997 - categorical_accuracy: 0.7261 - val_loss: 1.3918 - val_categorical_accuracy: 0.5741\n",
            "Epoch 495/500\n",
            "482/482 [==============================] - 0s 50us/step - loss: 0.4993 - categorical_accuracy: 0.7241 - val_loss: 1.3855 - val_categorical_accuracy: 0.5741\n",
            "Epoch 496/500\n",
            "482/482 [==============================] - 0s 68us/step - loss: 0.4988 - categorical_accuracy: 0.7199 - val_loss: 1.3741 - val_categorical_accuracy: 0.5370\n",
            "Epoch 497/500\n",
            "482/482 [==============================] - 0s 49us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3737 - val_categorical_accuracy: 0.5370\n",
            "Epoch 498/500\n",
            "482/482 [==============================] - 0s 43us/step - loss: 0.4992 - categorical_accuracy: 0.7199 - val_loss: 1.3784 - val_categorical_accuracy: 0.5741\n",
            "Epoch 499/500\n",
            "482/482 [==============================] - 0s 54us/step - loss: 0.4992 - categorical_accuracy: 0.7241 - val_loss: 1.3849 - val_categorical_accuracy: 0.5741\n",
            "Epoch 500/500\n",
            "482/482 [==============================] - 0s 110us/step - loss: 0.4996 - categorical_accuracy: 0.7241 - val_loss: 1.3832 - val_categorical_accuracy: 0.5741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLnxpNkCxFZ1",
        "colab_type": "code",
        "outputId": "e9af7151-d04f-4a0f-d021-779b2242e6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test our model against the hidden one\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) # just by running a deep learning model with the goddamn ages and gender, wtf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 526us/step\n",
            "Score: 0.77, Accuracy: 0.750000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysTchue5QU2i",
        "colab_type": "code",
        "outputId": "af5c0532-dc0b-4204-d7c5-af89af625a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "# save\n",
        "plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_acc.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "# save\n",
        "plt.savefig('all_mprage_grappa/z_tests/keep_models/as_model_plot_val.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAsAQC+SBQCgF8kCANDrAJVthy9WflN0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKRRERCSmUBARkZhCQUREYv8fuFgJk9z2pcQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Esb6vfqURV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save our best model\n",
        "# pickle.dump( as_model, open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"wb\" ) )\n",
        "# load model\n",
        "# as_model = pickle.load( open( \"all_mprage_grappa/z_tests/keep_models/as_model.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBO4xI1FbIR3",
        "colab_type": "text"
      },
      "source": [
        "**Testing our model with an independent batch of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARhPhXtQWUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load our pickle batch of data\n",
        "with open('all_mprage_grappa/processed_brains_aug/dbatch4.pkl', 'rb') as f: # also 'total_slices_all.pkl' ## RENAMED 5 TO 7, TESTING IT\n",
        "  total_slices_test, total_slices_info_test = pickle.load(f) # stored_batches/total_slices_batch5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-x9fN2IfVqy",
        "colab_type": "code",
        "outputId": "99cdb3da-3c8e-446f-d208-c28ee0aa8d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(total_slices_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 160, 160, 160, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYjNw77Lt7nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the set an array\n",
        "total_slices_test = np.array(total_slices_test)\n",
        "\n",
        "# independent test\n",
        "y_test, sex_test, ages_test = get_classification(total_slices_info_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_mFSZqfvBCK",
        "colab_type": "code",
        "outputId": "a27a3991-2fc6-4fa2-967e-b32012a61726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test our model against the independent set\n",
        "score, acc = as_model.evaluate([sex_test, ages_test], y_test)\n",
        "\n",
        "print (\"Score: %.2f, Accuracy: %f\" % (score, acc)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 2ms/step\n",
            "Score: 0.73, Accuracy: 0.586207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNCAXUfgbYIs",
        "colab_type": "text"
      },
      "source": [
        "**Make predictions with our as model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kQ5Mrw5wsVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "as_train_preds = as_model.predict([sex_train, ages_train]) # this is what we will feed into next model\n",
        "as_test_preds = as_model.predict([sex_test, ages_test]) # this is what we will use when testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA2N4vQ7yVEv",
        "colab_type": "text"
      },
      "source": [
        "**Next part, get predictions from model on ages and sex and predictions from images, feed into another deep NN to try to predict the output, then test accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3v84hoBy5AE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALTERNATIVELY: load our cnn_models\n",
        "cnn_model = load_model('all_mprage_grappa/stored_models/model4x/model40_aug_v0.h5') # take 040, best performing one so far"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHzkS1kxKWX",
        "colab_type": "code",
        "outputId": "091324da-2e06-44ff-bd5b-305c0d1f56d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#load batch slices, predict and aggregate scores\n",
        "cnn_train_preds = []\n",
        "\n",
        "# load the pickle (train with 0 to 3)\n",
        "print (training_batch_f)\n",
        "for tbf in training_batch_f:\n",
        "  with open('all_mprage_grappa/processed_brains_aug/'+tbf, 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f) \n",
        "    # predict and aggregate\n",
        "    total_slices = np.array(total_slices)\n",
        "    \n",
        "    # split and predict, too many to handle otherwise and will crash\n",
        "    sub_arrays = np.array_split(total_slices, 50)\n",
        "    \n",
        "    for i in range(len(sub_arrays)):\n",
        "      \n",
        "      sub_array = sub_arrays[i]\n",
        "      cnn_prediction = cnn_model.predict(sub_array)\n",
        "      cnn_train_preds.extend(cnn_prediction)\n",
        "    \n",
        "print (np.shape(cnn_train_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dbatch0.pkl', 'dbatch1.pkl', 'dbatch2.pkl']\n",
            "(300, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU5fMHzOjWmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# also get the test predictions\n",
        "cnn_test_preds = cnn_model.predict(total_slices_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0UzJCQti-pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save our preds (cause it takes ages to build)\n",
        "# pickle.dump( cnn_train_preds, open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"wb\" ) )\n",
        "# pickle.dump( cnn_test_preds, open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"wb\" ) )\n",
        "## load model\n",
        "cnn_train_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_training_preds.pkl\", \"rb\" ) )\n",
        "cnn_test_preds = pickle.load( open( \"all_mprage_grappa/z_tests/cnn_test_preds.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaXsBh1NzDc",
        "colab_type": "text"
      },
      "source": [
        "**Now make deep learning framework where we load our cnn train preds, our as train preds and aggregate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYsnoPcwwue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_binary(cat_array):\n",
        "  '''Function to convert categorical back to binary values'''\n",
        "  binary_output_array = []\n",
        "  for i in range(len(cat_array)):\n",
        "    binary_output_array.append(np.argmax(cat_array[i]))\n",
        "    \n",
        "  binary_output_array = np.array(binary_output_array)\n",
        "  return binary_output_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3NgFPpkMq17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create MLP models\n",
        "mlp_cnn = create_mlp(2, regress=False) # value of 1 for binary, 2 for categorical\n",
        "mlp_as = create_mlp(2, regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combined_input_two = concatenate([mlp_cnn.output, mlp_as.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "z = Dense(8, activation=\"relu\")(combined_input_two)\n",
        "z = Dropout(0.2)(z)\n",
        "z = Dense(2, activation=\"sigmoid\")(z)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "combined_model = Model(inputs=[mlp_cnn.input, mlp_as.input], outputs=z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKsn3mJSYNR",
        "colab_type": "code",
        "outputId": "4d5772e9-4ce4-4a04-be19-074c904a4384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6787
        }
      },
      "source": [
        "# compile our model\n",
        "combined_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3 / 200), \n",
        "              metrics = ['categorical_accuracy']) # decay in Adam..\n",
        "\n",
        "# train the model\n",
        "print(\"training model...\")\n",
        "combined_model.fit([cnn_train_preds, as_train_preds], y_train, validation_split=0.1, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model...\n",
            "Train on 270 samples, validate on 30 samples\n",
            "Epoch 1/200\n",
            "270/270 [==============================] - 2s 6ms/step - loss: 0.6758 - categorical_accuracy: 0.5815 - val_loss: 0.6712 - val_categorical_accuracy: 0.6000\n",
            "Epoch 2/200\n",
            "270/270 [==============================] - 0s 112us/step - loss: 0.6749 - categorical_accuracy: 0.5889 - val_loss: 0.6674 - val_categorical_accuracy: 0.6000\n",
            "Epoch 3/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.6697 - categorical_accuracy: 0.5556 - val_loss: 0.6638 - val_categorical_accuracy: 0.6000\n",
            "Epoch 4/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.6678 - categorical_accuracy: 0.5667 - val_loss: 0.6595 - val_categorical_accuracy: 0.6000\n",
            "Epoch 5/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.6624 - categorical_accuracy: 0.5778 - val_loss: 0.6552 - val_categorical_accuracy: 0.6000\n",
            "Epoch 6/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.6567 - categorical_accuracy: 0.5667 - val_loss: 0.6505 - val_categorical_accuracy: 0.6000\n",
            "Epoch 7/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.6600 - categorical_accuracy: 0.5667 - val_loss: 0.6455 - val_categorical_accuracy: 0.6000\n",
            "Epoch 8/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.6550 - categorical_accuracy: 0.5741 - val_loss: 0.6403 - val_categorical_accuracy: 0.6000\n",
            "Epoch 9/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.6444 - categorical_accuracy: 0.5852 - val_loss: 0.6349 - val_categorical_accuracy: 0.6000\n",
            "Epoch 10/200\n",
            "270/270 [==============================] - 0s 96us/step - loss: 0.6399 - categorical_accuracy: 0.5704 - val_loss: 0.6294 - val_categorical_accuracy: 0.6000\n",
            "Epoch 11/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.6368 - categorical_accuracy: 0.5704 - val_loss: 0.6231 - val_categorical_accuracy: 0.6000\n",
            "Epoch 12/200\n",
            "270/270 [==============================] - 0s 79us/step - loss: 0.6377 - categorical_accuracy: 0.5741 - val_loss: 0.6164 - val_categorical_accuracy: 0.6000\n",
            "Epoch 13/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.6238 - categorical_accuracy: 0.6000 - val_loss: 0.6081 - val_categorical_accuracy: 0.6000\n",
            "Epoch 14/200\n",
            "270/270 [==============================] - 0s 90us/step - loss: 0.6141 - categorical_accuracy: 0.5963 - val_loss: 0.5979 - val_categorical_accuracy: 0.6000\n",
            "Epoch 15/200\n",
            "270/270 [==============================] - 0s 104us/step - loss: 0.6064 - categorical_accuracy: 0.6111 - val_loss: 0.5870 - val_categorical_accuracy: 0.6000\n",
            "Epoch 16/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.5937 - categorical_accuracy: 0.6222 - val_loss: 0.5745 - val_categorical_accuracy: 0.6000\n",
            "Epoch 17/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.5815 - categorical_accuracy: 0.6222 - val_loss: 0.5615 - val_categorical_accuracy: 0.6000\n",
            "Epoch 18/200\n",
            "270/270 [==============================] - 0s 83us/step - loss: 0.5742 - categorical_accuracy: 0.6111 - val_loss: 0.5477 - val_categorical_accuracy: 0.6667\n",
            "Epoch 19/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.5539 - categorical_accuracy: 0.6444 - val_loss: 0.5324 - val_categorical_accuracy: 0.6667\n",
            "Epoch 20/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.5443 - categorical_accuracy: 0.7000 - val_loss: 0.5167 - val_categorical_accuracy: 0.7000\n",
            "Epoch 21/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.5295 - categorical_accuracy: 0.7630 - val_loss: 0.4989 - val_categorical_accuracy: 0.8333\n",
            "Epoch 22/200\n",
            "270/270 [==============================] - 0s 88us/step - loss: 0.5073 - categorical_accuracy: 0.8037 - val_loss: 0.4806 - val_categorical_accuracy: 0.9333\n",
            "Epoch 23/200\n",
            "270/270 [==============================] - 0s 89us/step - loss: 0.4864 - categorical_accuracy: 0.8407 - val_loss: 0.4609 - val_categorical_accuracy: 0.9667\n",
            "Epoch 24/200\n",
            "270/270 [==============================] - 0s 97us/step - loss: 0.4725 - categorical_accuracy: 0.8296 - val_loss: 0.4418 - val_categorical_accuracy: 0.9667\n",
            "Epoch 25/200\n",
            "270/270 [==============================] - 0s 92us/step - loss: 0.4442 - categorical_accuracy: 0.9222 - val_loss: 0.4218 - val_categorical_accuracy: 0.9667\n",
            "Epoch 26/200\n",
            "270/270 [==============================] - 0s 86us/step - loss: 0.4429 - categorical_accuracy: 0.9074 - val_loss: 0.4021 - val_categorical_accuracy: 0.9667\n",
            "Epoch 27/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.4170 - categorical_accuracy: 0.9185 - val_loss: 0.3841 - val_categorical_accuracy: 0.9667\n",
            "Epoch 28/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.4021 - categorical_accuracy: 0.9074 - val_loss: 0.3682 - val_categorical_accuracy: 0.9667\n",
            "Epoch 29/200\n",
            "270/270 [==============================] - 0s 87us/step - loss: 0.3916 - categorical_accuracy: 0.9333 - val_loss: 0.3531 - val_categorical_accuracy: 0.9667\n",
            "Epoch 30/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.3698 - categorical_accuracy: 0.9333 - val_loss: 0.3390 - val_categorical_accuracy: 0.9667\n",
            "Epoch 31/200\n",
            "270/270 [==============================] - 0s 103us/step - loss: 0.3573 - categorical_accuracy: 0.9333 - val_loss: 0.3233 - val_categorical_accuracy: 0.9667\n",
            "Epoch 32/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.3588 - categorical_accuracy: 0.9259 - val_loss: 0.3105 - val_categorical_accuracy: 0.9667\n",
            "Epoch 33/200\n",
            "270/270 [==============================] - 0s 88us/step - loss: 0.3361 - categorical_accuracy: 0.9370 - val_loss: 0.2984 - val_categorical_accuracy: 0.9667\n",
            "Epoch 34/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.3224 - categorical_accuracy: 0.9333 - val_loss: 0.2862 - val_categorical_accuracy: 0.9667\n",
            "Epoch 35/200\n",
            "270/270 [==============================] - 0s 90us/step - loss: 0.3279 - categorical_accuracy: 0.9148 - val_loss: 0.2745 - val_categorical_accuracy: 0.9667\n",
            "Epoch 36/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.2855 - categorical_accuracy: 0.9444 - val_loss: 0.2631 - val_categorical_accuracy: 0.9667\n",
            "Epoch 37/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.2747 - categorical_accuracy: 0.9407 - val_loss: 0.2520 - val_categorical_accuracy: 0.9667\n",
            "Epoch 38/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.2634 - categorical_accuracy: 0.9519 - val_loss: 0.2421 - val_categorical_accuracy: 0.9667\n",
            "Epoch 39/200\n",
            "270/270 [==============================] - 0s 98us/step - loss: 0.2725 - categorical_accuracy: 0.9481 - val_loss: 0.2333 - val_categorical_accuracy: 0.9667\n",
            "Epoch 40/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.2686 - categorical_accuracy: 0.9556 - val_loss: 0.2248 - val_categorical_accuracy: 0.9667\n",
            "Epoch 41/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.2372 - categorical_accuracy: 0.9556 - val_loss: 0.2174 - val_categorical_accuracy: 0.9667\n",
            "Epoch 42/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.2434 - categorical_accuracy: 0.9556 - val_loss: 0.2102 - val_categorical_accuracy: 0.9667\n",
            "Epoch 43/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.2313 - categorical_accuracy: 0.9556 - val_loss: 0.2038 - val_categorical_accuracy: 0.9667\n",
            "Epoch 44/200\n",
            "270/270 [==============================] - 0s 71us/step - loss: 0.2311 - categorical_accuracy: 0.9519 - val_loss: 0.1979 - val_categorical_accuracy: 0.9667\n",
            "Epoch 45/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.2089 - categorical_accuracy: 0.9519 - val_loss: 0.1927 - val_categorical_accuracy: 0.9667\n",
            "Epoch 46/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1941 - categorical_accuracy: 0.9556 - val_loss: 0.1873 - val_categorical_accuracy: 0.9667\n",
            "Epoch 47/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.2233 - categorical_accuracy: 0.9333 - val_loss: 0.1825 - val_categorical_accuracy: 0.9667\n",
            "Epoch 48/200\n",
            "270/270 [==============================] - 0s 71us/step - loss: 0.1902 - categorical_accuracy: 0.9519 - val_loss: 0.1786 - val_categorical_accuracy: 0.9667\n",
            "Epoch 49/200\n",
            "270/270 [==============================] - 0s 86us/step - loss: 0.1868 - categorical_accuracy: 0.9481 - val_loss: 0.1756 - val_categorical_accuracy: 0.9667\n",
            "Epoch 50/200\n",
            "270/270 [==============================] - 0s 71us/step - loss: 0.1977 - categorical_accuracy: 0.9519 - val_loss: 0.1739 - val_categorical_accuracy: 0.9667\n",
            "Epoch 51/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1979 - categorical_accuracy: 0.9481 - val_loss: 0.1703 - val_categorical_accuracy: 0.9667\n",
            "Epoch 52/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1871 - categorical_accuracy: 0.9667 - val_loss: 0.1676 - val_categorical_accuracy: 0.9667\n",
            "Epoch 53/200\n",
            "270/270 [==============================] - 0s 64us/step - loss: 0.1778 - categorical_accuracy: 0.9481 - val_loss: 0.1654 - val_categorical_accuracy: 0.9667\n",
            "Epoch 54/200\n",
            "270/270 [==============================] - 0s 79us/step - loss: 0.1832 - categorical_accuracy: 0.9630 - val_loss: 0.1639 - val_categorical_accuracy: 0.9667\n",
            "Epoch 55/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1689 - categorical_accuracy: 0.9519 - val_loss: 0.1641 - val_categorical_accuracy: 0.9667\n",
            "Epoch 56/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1778 - categorical_accuracy: 0.9556 - val_loss: 0.1634 - val_categorical_accuracy: 0.9667\n",
            "Epoch 57/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1739 - categorical_accuracy: 0.9593 - val_loss: 0.1615 - val_categorical_accuracy: 0.9667\n",
            "Epoch 58/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1647 - categorical_accuracy: 0.9593 - val_loss: 0.1591 - val_categorical_accuracy: 0.9667\n",
            "Epoch 59/200\n",
            "270/270 [==============================] - 0s 98us/step - loss: 0.1743 - categorical_accuracy: 0.9333 - val_loss: 0.1583 - val_categorical_accuracy: 0.9667\n",
            "Epoch 60/200\n",
            "270/270 [==============================] - 0s 67us/step - loss: 0.1681 - categorical_accuracy: 0.9667 - val_loss: 0.1583 - val_categorical_accuracy: 0.9667\n",
            "Epoch 61/200\n",
            "270/270 [==============================] - 0s 63us/step - loss: 0.1708 - categorical_accuracy: 0.9593 - val_loss: 0.1589 - val_categorical_accuracy: 0.9667\n",
            "Epoch 62/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1652 - categorical_accuracy: 0.9593 - val_loss: 0.1592 - val_categorical_accuracy: 0.9667\n",
            "Epoch 63/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1681 - categorical_accuracy: 0.9556 - val_loss: 0.1591 - val_categorical_accuracy: 0.9667\n",
            "Epoch 64/200\n",
            "270/270 [==============================] - 0s 79us/step - loss: 0.1503 - categorical_accuracy: 0.9593 - val_loss: 0.1564 - val_categorical_accuracy: 0.9667\n",
            "Epoch 65/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1650 - categorical_accuracy: 0.9667 - val_loss: 0.1564 - val_categorical_accuracy: 0.9667\n",
            "Epoch 66/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1849 - categorical_accuracy: 0.9519 - val_loss: 0.1567 - val_categorical_accuracy: 0.9667\n",
            "Epoch 67/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1383 - categorical_accuracy: 0.9667 - val_loss: 0.1571 - val_categorical_accuracy: 0.9667\n",
            "Epoch 68/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1494 - categorical_accuracy: 0.9630 - val_loss: 0.1568 - val_categorical_accuracy: 0.9667\n",
            "Epoch 69/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.1308 - categorical_accuracy: 0.9667 - val_loss: 0.1556 - val_categorical_accuracy: 0.9667\n",
            "Epoch 70/200\n",
            "270/270 [==============================] - 0s 97us/step - loss: 0.1767 - categorical_accuracy: 0.9481 - val_loss: 0.1547 - val_categorical_accuracy: 0.9667\n",
            "Epoch 71/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1349 - categorical_accuracy: 0.9704 - val_loss: 0.1551 - val_categorical_accuracy: 0.9667\n",
            "Epoch 72/200\n",
            "270/270 [==============================] - 0s 92us/step - loss: 0.1548 - categorical_accuracy: 0.9556 - val_loss: 0.1571 - val_categorical_accuracy: 0.9667\n",
            "Epoch 73/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1565 - categorical_accuracy: 0.9519 - val_loss: 0.1570 - val_categorical_accuracy: 0.9667\n",
            "Epoch 74/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1378 - categorical_accuracy: 0.9667 - val_loss: 0.1564 - val_categorical_accuracy: 0.9667\n",
            "Epoch 75/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1511 - categorical_accuracy: 0.9519 - val_loss: 0.1569 - val_categorical_accuracy: 0.9667\n",
            "Epoch 76/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1544 - categorical_accuracy: 0.9667 - val_loss: 0.1565 - val_categorical_accuracy: 0.9667\n",
            "Epoch 77/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1184 - categorical_accuracy: 0.9704 - val_loss: 0.1558 - val_categorical_accuracy: 0.9667\n",
            "Epoch 78/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1430 - categorical_accuracy: 0.9593 - val_loss: 0.1552 - val_categorical_accuracy: 0.9667\n",
            "Epoch 79/200\n",
            "270/270 [==============================] - 0s 79us/step - loss: 0.1215 - categorical_accuracy: 0.9667 - val_loss: 0.1550 - val_categorical_accuracy: 0.9667\n",
            "Epoch 80/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1584 - categorical_accuracy: 0.9667 - val_loss: 0.1550 - val_categorical_accuracy: 0.9667\n",
            "Epoch 81/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1401 - categorical_accuracy: 0.9593 - val_loss: 0.1560 - val_categorical_accuracy: 0.9667\n",
            "Epoch 82/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1438 - categorical_accuracy: 0.9667 - val_loss: 0.1570 - val_categorical_accuracy: 0.9667\n",
            "Epoch 83/200\n",
            "270/270 [==============================] - 0s 66us/step - loss: 0.1397 - categorical_accuracy: 0.9667 - val_loss: 0.1564 - val_categorical_accuracy: 0.9667\n",
            "Epoch 84/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1312 - categorical_accuracy: 0.9704 - val_loss: 0.1565 - val_categorical_accuracy: 0.9667\n",
            "Epoch 85/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.1765 - categorical_accuracy: 0.9481 - val_loss: 0.1559 - val_categorical_accuracy: 0.9667\n",
            "Epoch 86/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1615 - categorical_accuracy: 0.9630 - val_loss: 0.1566 - val_categorical_accuracy: 0.9667\n",
            "Epoch 87/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1600 - categorical_accuracy: 0.9593 - val_loss: 0.1572 - val_categorical_accuracy: 0.9667\n",
            "Epoch 88/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1505 - categorical_accuracy: 0.9556 - val_loss: 0.1575 - val_categorical_accuracy: 0.9667\n",
            "Epoch 89/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1445 - categorical_accuracy: 0.9667 - val_loss: 0.1572 - val_categorical_accuracy: 0.9667\n",
            "Epoch 90/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1398 - categorical_accuracy: 0.9630 - val_loss: 0.1572 - val_categorical_accuracy: 0.9333\n",
            "Epoch 91/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1519 - categorical_accuracy: 0.9556 - val_loss: 0.1586 - val_categorical_accuracy: 0.9333\n",
            "Epoch 92/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1586 - categorical_accuracy: 0.9593 - val_loss: 0.1577 - val_categorical_accuracy: 0.9667\n",
            "Epoch 93/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.1406 - categorical_accuracy: 0.9519 - val_loss: 0.1583 - val_categorical_accuracy: 0.9667\n",
            "Epoch 94/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1397 - categorical_accuracy: 0.9630 - val_loss: 0.1584 - val_categorical_accuracy: 0.9667\n",
            "Epoch 95/200\n",
            "270/270 [==============================] - 0s 67us/step - loss: 0.1319 - categorical_accuracy: 0.9704 - val_loss: 0.1590 - val_categorical_accuracy: 0.9667\n",
            "Epoch 96/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1441 - categorical_accuracy: 0.9593 - val_loss: 0.1588 - val_categorical_accuracy: 0.9667\n",
            "Epoch 97/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1242 - categorical_accuracy: 0.9630 - val_loss: 0.1588 - val_categorical_accuracy: 0.9667\n",
            "Epoch 98/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1488 - categorical_accuracy: 0.9519 - val_loss: 0.1593 - val_categorical_accuracy: 0.9667\n",
            "Epoch 99/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1499 - categorical_accuracy: 0.9593 - val_loss: 0.1592 - val_categorical_accuracy: 0.9333\n",
            "Epoch 100/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1537 - categorical_accuracy: 0.9481 - val_loss: 0.1594 - val_categorical_accuracy: 0.9667\n",
            "Epoch 101/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1387 - categorical_accuracy: 0.9593 - val_loss: 0.1597 - val_categorical_accuracy: 0.9667\n",
            "Epoch 102/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1342 - categorical_accuracy: 0.9630 - val_loss: 0.1601 - val_categorical_accuracy: 0.9667\n",
            "Epoch 103/200\n",
            "270/270 [==============================] - 0s 86us/step - loss: 0.1491 - categorical_accuracy: 0.9519 - val_loss: 0.1598 - val_categorical_accuracy: 0.9667\n",
            "Epoch 104/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1450 - categorical_accuracy: 0.9519 - val_loss: 0.1601 - val_categorical_accuracy: 0.9667\n",
            "Epoch 105/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1505 - categorical_accuracy: 0.9556 - val_loss: 0.1603 - val_categorical_accuracy: 0.9333\n",
            "Epoch 106/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1716 - categorical_accuracy: 0.9407 - val_loss: 0.1607 - val_categorical_accuracy: 0.9667\n",
            "Epoch 107/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1469 - categorical_accuracy: 0.9519 - val_loss: 0.1610 - val_categorical_accuracy: 0.9667\n",
            "Epoch 108/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1622 - categorical_accuracy: 0.9481 - val_loss: 0.1609 - val_categorical_accuracy: 0.9667\n",
            "Epoch 109/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1594 - categorical_accuracy: 0.9593 - val_loss: 0.1612 - val_categorical_accuracy: 0.9667\n",
            "Epoch 110/200\n",
            "270/270 [==============================] - 0s 80us/step - loss: 0.1501 - categorical_accuracy: 0.9556 - val_loss: 0.1606 - val_categorical_accuracy: 0.9667\n",
            "Epoch 111/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1125 - categorical_accuracy: 0.9667 - val_loss: 0.1609 - val_categorical_accuracy: 0.9667\n",
            "Epoch 112/200\n",
            "270/270 [==============================] - 0s 93us/step - loss: 0.1626 - categorical_accuracy: 0.9407 - val_loss: 0.1619 - val_categorical_accuracy: 0.9667\n",
            "Epoch 113/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1170 - categorical_accuracy: 0.9481 - val_loss: 0.1608 - val_categorical_accuracy: 0.9667\n",
            "Epoch 114/200\n",
            "270/270 [==============================] - 0s 87us/step - loss: 0.1505 - categorical_accuracy: 0.9556 - val_loss: 0.1603 - val_categorical_accuracy: 0.9333\n",
            "Epoch 115/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1347 - categorical_accuracy: 0.9407 - val_loss: 0.1607 - val_categorical_accuracy: 0.9333\n",
            "Epoch 116/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1389 - categorical_accuracy: 0.9630 - val_loss: 0.1611 - val_categorical_accuracy: 0.9333\n",
            "Epoch 117/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1304 - categorical_accuracy: 0.9444 - val_loss: 0.1608 - val_categorical_accuracy: 0.9333\n",
            "Epoch 118/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1214 - categorical_accuracy: 0.9630 - val_loss: 0.1610 - val_categorical_accuracy: 0.9333\n",
            "Epoch 119/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1561 - categorical_accuracy: 0.9407 - val_loss: 0.1612 - val_categorical_accuracy: 0.9333\n",
            "Epoch 120/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1248 - categorical_accuracy: 0.9593 - val_loss: 0.1614 - val_categorical_accuracy: 0.9333\n",
            "Epoch 121/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.1459 - categorical_accuracy: 0.9519 - val_loss: 0.1613 - val_categorical_accuracy: 0.9333\n",
            "Epoch 122/200\n",
            "270/270 [==============================] - 0s 80us/step - loss: 0.1284 - categorical_accuracy: 0.9704 - val_loss: 0.1613 - val_categorical_accuracy: 0.9333\n",
            "Epoch 123/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1086 - categorical_accuracy: 0.9630 - val_loss: 0.1620 - val_categorical_accuracy: 0.9333\n",
            "Epoch 124/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.1236 - categorical_accuracy: 0.9556 - val_loss: 0.1614 - val_categorical_accuracy: 0.9333\n",
            "Epoch 125/200\n",
            "270/270 [==============================] - 0s 96us/step - loss: 0.1619 - categorical_accuracy: 0.9333 - val_loss: 0.1616 - val_categorical_accuracy: 0.9333\n",
            "Epoch 126/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1345 - categorical_accuracy: 0.9556 - val_loss: 0.1619 - val_categorical_accuracy: 0.9333\n",
            "Epoch 127/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1608 - categorical_accuracy: 0.9630 - val_loss: 0.1625 - val_categorical_accuracy: 0.9333\n",
            "Epoch 128/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1488 - categorical_accuracy: 0.9481 - val_loss: 0.1628 - val_categorical_accuracy: 0.9667\n",
            "Epoch 129/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1605 - categorical_accuracy: 0.9519 - val_loss: 0.1626 - val_categorical_accuracy: 0.9667\n",
            "Epoch 130/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1522 - categorical_accuracy: 0.9481 - val_loss: 0.1623 - val_categorical_accuracy: 0.9333\n",
            "Epoch 131/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1458 - categorical_accuracy: 0.9481 - val_loss: 0.1622 - val_categorical_accuracy: 0.9333\n",
            "Epoch 132/200\n",
            "270/270 [==============================] - 0s 80us/step - loss: 0.1434 - categorical_accuracy: 0.9630 - val_loss: 0.1623 - val_categorical_accuracy: 0.9333\n",
            "Epoch 133/200\n",
            "270/270 [==============================] - 0s 94us/step - loss: 0.1415 - categorical_accuracy: 0.9556 - val_loss: 0.1627 - val_categorical_accuracy: 0.9333\n",
            "Epoch 134/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1506 - categorical_accuracy: 0.9519 - val_loss: 0.1626 - val_categorical_accuracy: 0.9333\n",
            "Epoch 135/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1518 - categorical_accuracy: 0.9519 - val_loss: 0.1632 - val_categorical_accuracy: 0.9333\n",
            "Epoch 136/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1393 - categorical_accuracy: 0.9556 - val_loss: 0.1630 - val_categorical_accuracy: 0.9333\n",
            "Epoch 137/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1127 - categorical_accuracy: 0.9704 - val_loss: 0.1629 - val_categorical_accuracy: 0.9333\n",
            "Epoch 138/200\n",
            "270/270 [==============================] - 0s 80us/step - loss: 0.1394 - categorical_accuracy: 0.9519 - val_loss: 0.1631 - val_categorical_accuracy: 0.9333\n",
            "Epoch 139/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1400 - categorical_accuracy: 0.9556 - val_loss: 0.1632 - val_categorical_accuracy: 0.9333\n",
            "Epoch 140/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1559 - categorical_accuracy: 0.9444 - val_loss: 0.1644 - val_categorical_accuracy: 0.9333\n",
            "Epoch 141/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1350 - categorical_accuracy: 0.9519 - val_loss: 0.1646 - val_categorical_accuracy: 0.9333\n",
            "Epoch 142/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.1501 - categorical_accuracy: 0.9370 - val_loss: 0.1640 - val_categorical_accuracy: 0.9333\n",
            "Epoch 143/200\n",
            "270/270 [==============================] - 0s 85us/step - loss: 0.1310 - categorical_accuracy: 0.9370 - val_loss: 0.1639 - val_categorical_accuracy: 0.9333\n",
            "Epoch 144/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1105 - categorical_accuracy: 0.9519 - val_loss: 0.1650 - val_categorical_accuracy: 0.9333\n",
            "Epoch 145/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1289 - categorical_accuracy: 0.9667 - val_loss: 0.1650 - val_categorical_accuracy: 0.9333\n",
            "Epoch 146/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1326 - categorical_accuracy: 0.9556 - val_loss: 0.1645 - val_categorical_accuracy: 0.9333\n",
            "Epoch 147/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1171 - categorical_accuracy: 0.9556 - val_loss: 0.1646 - val_categorical_accuracy: 0.9333\n",
            "Epoch 148/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.1385 - categorical_accuracy: 0.9519 - val_loss: 0.1650 - val_categorical_accuracy: 0.9333\n",
            "Epoch 149/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1316 - categorical_accuracy: 0.9519 - val_loss: 0.1653 - val_categorical_accuracy: 0.9333\n",
            "Epoch 150/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1290 - categorical_accuracy: 0.9593 - val_loss: 0.1656 - val_categorical_accuracy: 0.9333\n",
            "Epoch 151/200\n",
            "270/270 [==============================] - 0s 66us/step - loss: 0.1494 - categorical_accuracy: 0.9481 - val_loss: 0.1653 - val_categorical_accuracy: 0.9333\n",
            "Epoch 152/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1276 - categorical_accuracy: 0.9593 - val_loss: 0.1654 - val_categorical_accuracy: 0.9333\n",
            "Epoch 153/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1275 - categorical_accuracy: 0.9556 - val_loss: 0.1658 - val_categorical_accuracy: 0.9333\n",
            "Epoch 154/200\n",
            "270/270 [==============================] - 0s 66us/step - loss: 0.1563 - categorical_accuracy: 0.9556 - val_loss: 0.1663 - val_categorical_accuracy: 0.9333\n",
            "Epoch 155/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1529 - categorical_accuracy: 0.9370 - val_loss: 0.1663 - val_categorical_accuracy: 0.9333\n",
            "Epoch 156/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1346 - categorical_accuracy: 0.9630 - val_loss: 0.1665 - val_categorical_accuracy: 0.9333\n",
            "Epoch 157/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1414 - categorical_accuracy: 0.9444 - val_loss: 0.1666 - val_categorical_accuracy: 0.9333\n",
            "Epoch 158/200\n",
            "270/270 [==============================] - 0s 97us/step - loss: 0.1415 - categorical_accuracy: 0.9519 - val_loss: 0.1669 - val_categorical_accuracy: 0.9333\n",
            "Epoch 159/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1363 - categorical_accuracy: 0.9519 - val_loss: 0.1670 - val_categorical_accuracy: 0.9333\n",
            "Epoch 160/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1351 - categorical_accuracy: 0.9519 - val_loss: 0.1673 - val_categorical_accuracy: 0.9333\n",
            "Epoch 161/200\n",
            "270/270 [==============================] - 0s 66us/step - loss: 0.1283 - categorical_accuracy: 0.9630 - val_loss: 0.1673 - val_categorical_accuracy: 0.9333\n",
            "Epoch 162/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1289 - categorical_accuracy: 0.9519 - val_loss: 0.1674 - val_categorical_accuracy: 0.9333\n",
            "Epoch 163/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1595 - categorical_accuracy: 0.9407 - val_loss: 0.1677 - val_categorical_accuracy: 0.9333\n",
            "Epoch 164/200\n",
            "270/270 [==============================] - 0s 77us/step - loss: 0.1286 - categorical_accuracy: 0.9556 - val_loss: 0.1677 - val_categorical_accuracy: 0.9333\n",
            "Epoch 165/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1660 - categorical_accuracy: 0.9481 - val_loss: 0.1679 - val_categorical_accuracy: 0.9333\n",
            "Epoch 166/200\n",
            "270/270 [==============================] - 0s 76us/step - loss: 0.1560 - categorical_accuracy: 0.9519 - val_loss: 0.1681 - val_categorical_accuracy: 0.9333\n",
            "Epoch 167/200\n",
            "270/270 [==============================] - 0s 78us/step - loss: 0.1334 - categorical_accuracy: 0.9593 - val_loss: 0.1679 - val_categorical_accuracy: 0.9333\n",
            "Epoch 168/200\n",
            "270/270 [==============================] - 0s 71us/step - loss: 0.1544 - categorical_accuracy: 0.9556 - val_loss: 0.1678 - val_categorical_accuracy: 0.9333\n",
            "Epoch 169/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1620 - categorical_accuracy: 0.9333 - val_loss: 0.1684 - val_categorical_accuracy: 0.9333\n",
            "Epoch 170/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1350 - categorical_accuracy: 0.9556 - val_loss: 0.1689 - val_categorical_accuracy: 0.9333\n",
            "Epoch 171/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1240 - categorical_accuracy: 0.9519 - val_loss: 0.1685 - val_categorical_accuracy: 0.9333\n",
            "Epoch 172/200\n",
            "270/270 [==============================] - 0s 87us/step - loss: 0.1531 - categorical_accuracy: 0.9370 - val_loss: 0.1682 - val_categorical_accuracy: 0.9333\n",
            "Epoch 173/200\n",
            "270/270 [==============================] - 0s 90us/step - loss: 0.1233 - categorical_accuracy: 0.9481 - val_loss: 0.1684 - val_categorical_accuracy: 0.9333\n",
            "Epoch 174/200\n",
            "270/270 [==============================] - 0s 86us/step - loss: 0.1445 - categorical_accuracy: 0.9593 - val_loss: 0.1688 - val_categorical_accuracy: 0.9333\n",
            "Epoch 175/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1294 - categorical_accuracy: 0.9370 - val_loss: 0.1692 - val_categorical_accuracy: 0.9333\n",
            "Epoch 176/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1455 - categorical_accuracy: 0.9519 - val_loss: 0.1697 - val_categorical_accuracy: 0.9333\n",
            "Epoch 177/200\n",
            "270/270 [==============================] - 0s 65us/step - loss: 0.1525 - categorical_accuracy: 0.9444 - val_loss: 0.1692 - val_categorical_accuracy: 0.9333\n",
            "Epoch 178/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1267 - categorical_accuracy: 0.9667 - val_loss: 0.1691 - val_categorical_accuracy: 0.9333\n",
            "Epoch 179/200\n",
            "270/270 [==============================] - 0s 82us/step - loss: 0.1388 - categorical_accuracy: 0.9481 - val_loss: 0.1691 - val_categorical_accuracy: 0.9333\n",
            "Epoch 180/200\n",
            "270/270 [==============================] - 0s 80us/step - loss: 0.1371 - categorical_accuracy: 0.9667 - val_loss: 0.1709 - val_categorical_accuracy: 0.9333\n",
            "Epoch 181/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1397 - categorical_accuracy: 0.9593 - val_loss: 0.1724 - val_categorical_accuracy: 0.9333\n",
            "Epoch 182/200\n",
            "270/270 [==============================] - 0s 70us/step - loss: 0.1195 - categorical_accuracy: 0.9481 - val_loss: 0.1721 - val_categorical_accuracy: 0.9333\n",
            "Epoch 183/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1440 - categorical_accuracy: 0.9481 - val_loss: 0.1718 - val_categorical_accuracy: 0.9333\n",
            "Epoch 184/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1588 - categorical_accuracy: 0.9296 - val_loss: 0.1709 - val_categorical_accuracy: 0.9333\n",
            "Epoch 185/200\n",
            "270/270 [==============================] - 0s 71us/step - loss: 0.1489 - categorical_accuracy: 0.9481 - val_loss: 0.1700 - val_categorical_accuracy: 0.9333\n",
            "Epoch 186/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1314 - categorical_accuracy: 0.9556 - val_loss: 0.1702 - val_categorical_accuracy: 0.9333\n",
            "Epoch 187/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1101 - categorical_accuracy: 0.9704 - val_loss: 0.1708 - val_categorical_accuracy: 0.9333\n",
            "Epoch 188/200\n",
            "270/270 [==============================] - 0s 73us/step - loss: 0.1208 - categorical_accuracy: 0.9741 - val_loss: 0.1715 - val_categorical_accuracy: 0.9333\n",
            "Epoch 189/200\n",
            "270/270 [==============================] - 0s 81us/step - loss: 0.1315 - categorical_accuracy: 0.9444 - val_loss: 0.1717 - val_categorical_accuracy: 0.9333\n",
            "Epoch 190/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1405 - categorical_accuracy: 0.9481 - val_loss: 0.1713 - val_categorical_accuracy: 0.9333\n",
            "Epoch 191/200\n",
            "270/270 [==============================] - 0s 69us/step - loss: 0.1384 - categorical_accuracy: 0.9556 - val_loss: 0.1718 - val_categorical_accuracy: 0.9333\n",
            "Epoch 192/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1381 - categorical_accuracy: 0.9630 - val_loss: 0.1723 - val_categorical_accuracy: 0.9333\n",
            "Epoch 193/200\n",
            "270/270 [==============================] - 0s 68us/step - loss: 0.1108 - categorical_accuracy: 0.9667 - val_loss: 0.1722 - val_categorical_accuracy: 0.9333\n",
            "Epoch 194/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1532 - categorical_accuracy: 0.9556 - val_loss: 0.1719 - val_categorical_accuracy: 0.9333\n",
            "Epoch 195/200\n",
            "270/270 [==============================] - 0s 74us/step - loss: 0.1462 - categorical_accuracy: 0.9556 - val_loss: 0.1717 - val_categorical_accuracy: 0.9333\n",
            "Epoch 196/200\n",
            "270/270 [==============================] - 0s 75us/step - loss: 0.1419 - categorical_accuracy: 0.9556 - val_loss: 0.1719 - val_categorical_accuracy: 0.9333\n",
            "Epoch 197/200\n",
            "270/270 [==============================] - 0s 67us/step - loss: 0.1385 - categorical_accuracy: 0.9407 - val_loss: 0.1722 - val_categorical_accuracy: 0.9333\n",
            "Epoch 198/200\n",
            "270/270 [==============================] - 0s 72us/step - loss: 0.1164 - categorical_accuracy: 0.9519 - val_loss: 0.1734 - val_categorical_accuracy: 0.9333\n",
            "Epoch 199/200\n",
            "270/270 [==============================] - 0s 86us/step - loss: 0.1223 - categorical_accuracy: 0.9630 - val_loss: 0.1730 - val_categorical_accuracy: 0.9333\n",
            "Epoch 200/200\n",
            "270/270 [==============================] - 0s 84us/step - loss: 0.1436 - categorical_accuracy: 0.9667 - val_loss: 0.1739 - val_categorical_accuracy: 0.9333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc41f849550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx-CqpxPUSPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle.dump( combined_model, open( \"all_mprage_grappa/z_tests/combined_model_binary.pkl\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENTzWL9pTTYR",
        "colab_type": "text"
      },
      "source": [
        "**Load last one and test it out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us7wRSU2S4rc",
        "colab_type": "code",
        "outputId": "8d75d78b-e181-4356-89c1-ec52ae33a421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get predictions of cnn_model on test data\n",
        "# get predictions of as_model on test data\n",
        "# input these predictions into our combined_model\n",
        "# evaluate these against the actual y-value, pray it is not overfitted..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsGbpSiXWamH",
        "colab_type": "code",
        "outputId": "3ad747ba-8610-40f1-8f6e-1ed4c99f23e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "cm_score, cm_acc = combined_model.evaluate([cnn_test_preds, as_preds_test], y_test)\n",
        "\n",
        "print (\"Combined model score: %.2f, and combined model accuracy: %.2f\" % (cm_score, cm_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 228us/step\n",
            "Combined model score: 1.20, and combined model accuracy: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtf-HdCoJI0",
        "colab_type": "text"
      },
      "source": [
        "**Try: XGboosting the training predicted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZDlLxAzqkaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyBt5S9aw_Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert everything to binary\n",
        "cnn_train_preds_binary = to_binary(cnn_train_preds)\n",
        "as_train_preds_binary = to_binary(as_preds_train)\n",
        "y_train_binary = to_binary(y_train)\n",
        "\n",
        "cnn_test_preds_binary = to_binary(cnn_preds_test)\n",
        "as_test_preds_binary = to_binary(as_preds_test)\n",
        "y_test_binary = to_binary(y_values_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vxZoNjxyo_",
        "colab_type": "code",
        "outputId": "81633373-da04-419a-cb91-5b0ddd20e91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(y_train_binary) # no worky"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(620,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umVR6Q_esX_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make into a matrix and input this into our space\n",
        "\n",
        "combined_zipped_train = list(zip(cnn_train_preds_binary, as_train_preds_binary))\n",
        "combined_zipped_train = np.array(combined_zipped_train)\n",
        "\n",
        "# fit model with training data\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(combined_zipped_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrOAKOxjrZYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_zipped_test = list(zip(cnn_preds_test, as_preds_test))\n",
        "combined_zipped_test = np.array(combined_zipped_test)\n",
        "\n",
        "y_values_predictions = []\n",
        "for i in range(len(combined_zipped_test)):\n",
        "  cz_test_csr = csr_matrix(combined_zipped_test[i])\n",
        "  xgb_preds = xgb_model.predict(cz_test_csr)\n",
        "  y_values_predictions.append(xgb_preds)\n",
        "  \n",
        "y_values_predictions = np.array(y_values_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDhTUzOvcJG",
        "colab_type": "code",
        "outputId": "e2c7e58f-b952-46d8-a175-9bde48e3b6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(y_values_test, y_values_predictions, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIUzf6sUW73T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_train_preds # train with this as an input\n",
        "as_preds_train # another input\n",
        "y_train # ideal output\n",
        "\n",
        "#test\n",
        "cnn_preds_test\n",
        "as_preds_test\n",
        "y_values_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8O6PliZPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXxbK0vbB3B",
        "colab_type": "text"
      },
      "source": [
        "To try: reverse classification on each model and feed these in, see how our model holds up. Then do the same for our test and compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srl7AdZxbIaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
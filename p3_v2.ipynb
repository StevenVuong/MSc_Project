{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p3_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/p3_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqsFNk0abH5",
        "colab_type": "text"
      },
      "source": [
        "**This notebook aims to modularise some of the input functions in P3 to suit the databse**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ejr96oJaYyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcV-vanjaa8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "ae7a9354-1eb5-43bd-9818-a9896f242d62"
      },
      "source": [
        "# mount google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where the data is\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln4-nPp3bBzp",
        "colab_type": "text"
      },
      "source": [
        "**Load our dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwpIQNqFa2H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_df = pd.read_pickle('processed_patient_df_TRY2.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rbFYCN1bExv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "fd33dac5-4ed4-4f00-a0ba-deada50209bd"
      },
      "source": [
        "# establish control and pd df's\n",
        "df_control = patient_df[patient_df.Group == 0] \n",
        "df_pd = patient_df[patient_df.Group == 1] \n",
        "\n",
        "patient_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Data ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Group</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Visit</th>\n",
              "      <th>Modality</th>\n",
              "      <th>Description</th>\n",
              "      <th>Type</th>\n",
              "      <th>Acq Date</th>\n",
              "      <th>Format</th>\n",
              "      <th>Downloaded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1130198</td>\n",
              "      <td>75422</td>\n",
              "      <td>0</td>\n",
              "      <td>M</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>11/13/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>5/07/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1130191</td>\n",
              "      <td>75414</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>Sag MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>12/13/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1125041</td>\n",
              "      <td>74375</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE_GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>9/06/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1003469</td>\n",
              "      <td>72138</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>2/19/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1119693</td>\n",
              "      <td>71935</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>66</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>4/03/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Image Data ID  Subject  Group Sex  ...      Type    Acq Date Format Downloaded\n",
              "0        1130198    75422      0   M  ...  Original  11/13/2018    DCM  5/07/2019\n",
              "2        1130191    75414      0   F  ...  Original  12/13/2018    DCM  4/24/2019\n",
              "3        1125041    74375      0   F  ...  Original   9/06/2018    DCM  4/24/2019\n",
              "4        1003469    72138      0   F  ...  Original   2/19/2018    DCM  4/24/2019\n",
              "5        1119693    71935      1   M  ...  Original   4/03/2018    DCM  4/24/2019\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ilYDMpHb1hG",
        "colab_type": "text"
      },
      "source": [
        "**Load our file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSLclggbwHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for each set of training data in our pickle\n",
        "# load\n",
        "# process\n",
        "# train our model in this\n",
        "# 100 Test, 15% of 600 Validation, 85% of 600 Training. 700 Slices in total\n",
        "# Save our model, tweak and evaluate etc.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BajrkVMoeRFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets our y-values and converts to keras, one hot encoded outputs\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMILzi2f25i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_model():\n",
        "\n",
        "  # compile our model\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Convolution3D(filters=8, kernel_size=2, padding='same', input_shape=(200,200,160,1), kernel_regularizer=regularizers.l2(0.05),use_bias = True)) # padding on first one only?\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  model.add(MaxPooling3D(pool_size=2)) # pool_size=2\n",
        "  ## model.add(BatchNormalization(momentum=0.99)) # something to consider next level, moving mean and variance\n",
        "\n",
        "  model.add(Convolution3D(filters=16, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01)) \n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=32, kernel_size=3))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=64, kernel_size=3))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=128, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Convolution3D(filters=256, kernel_size=2))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(MaxPooling3D(pool_size=2))\n",
        "  ## model.add(BatchNormalization(momentum=0.99))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  # model.add(Dropout(0.45)) # add dropout to prevent overfitting\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  # model.add(Dropout(0.2))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']\n",
        "\n",
        "  # experiment with literally everything?... Random Search with optimisers\n",
        "            \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9amGVRb4Ct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c0cab558-2d87-4217-edb6-517ea003d144"
      },
      "source": [
        "# use 6 files to train, 7th we test our model on\n",
        "total_slices_train = os.listdir('stored_batches')[:6]\n",
        "total_slices_test = os.listdir('stored_batches')[6]\n",
        "\n",
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "\n",
        "# load and fit our model for our instances\n",
        "for tsf in total_slices_train:\n",
        "  pkl_path = os.listdir('stored_batches/'+tsf)\n",
        "  \n",
        "  # load pickle file\n",
        "  with open('total_slices_all.pkl', 'rb') as f:\n",
        "    total_slices, total_slices_info = pickle.load(f)\n",
        "    \n",
        "  # process y-values\n",
        "  y_values = get_y_values(total_slices_info)\n",
        "  \n",
        "  # fit our model ## can play with batch size\n",
        "  model.fit(x=total_slices, y=y_values, batch_size=5, epochs=1, verbose=1,\n",
        "          validation_split=0.15, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['total_slices_batch0.pkl',\n",
              " 'total_slices_batch1.pkl',\n",
              " 'total_slices_batch2.pkl',\n",
              " 'total_slices_batch3.pkl',\n",
              " 'total_slices_batch4.pkl',\n",
              " 'total_slices_batch5.pkl',\n",
              " 'total_slices_batch6.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}
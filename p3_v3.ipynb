{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p3_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenVuong/MSc_Project/blob/master/p3_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpFNZMnps6BR",
        "colab_type": "text"
      },
      "source": [
        "**This notebook aims to modularise some of the input functions in P3 to suit the databse**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OlDI_tKs8Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# other imports to handle files\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# deep learning imports\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution3D, MaxPooling3D, Convolution1D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.utils import np_utils, generic_utils, to_categorical\n",
        "from keras.layers import LeakyReLU\n",
        "from keras import regularizers\n",
        "\n",
        "# to split our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# to mount our drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrFa1HXSs9JO",
        "colab_type": "code",
        "outputId": "4a699d9a-f29f-44bf-93a6-15b6bff4faf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# mount google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# go to where the data is\n",
        "print (os.listdir())\n",
        "os.chdir('gdrive/My Drive/msc_project')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p8YYPoGtCbi",
        "colab_type": "text"
      },
      "source": [
        "**Load our dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52fJRiLHs-VF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_df = pd.read_pickle('processed_patient_df_TRY2.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOjHqucntEtD",
        "colab_type": "code",
        "outputId": "476aa795-0b78-42fd-8862-9fa2db0cac2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# establish control and pd df's\n",
        "df_control = patient_df[patient_df.Group == 0] \n",
        "df_pd = patient_df[patient_df.Group == 1] \n",
        "\n",
        "patient_df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Data ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Group</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Visit</th>\n",
              "      <th>Modality</th>\n",
              "      <th>Description</th>\n",
              "      <th>Type</th>\n",
              "      <th>Acq Date</th>\n",
              "      <th>Format</th>\n",
              "      <th>Downloaded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1130198</td>\n",
              "      <td>75422</td>\n",
              "      <td>0</td>\n",
              "      <td>M</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>11/13/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>5/07/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1130191</td>\n",
              "      <td>75414</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>Sag MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>12/13/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1125041</td>\n",
              "      <td>74375</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE_GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>9/06/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1003469</td>\n",
              "      <td>72138</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>2/19/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1119693</td>\n",
              "      <td>71935</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>66</td>\n",
              "      <td>1</td>\n",
              "      <td>MRI</td>\n",
              "      <td>MPRAGE GRAPPA</td>\n",
              "      <td>Original</td>\n",
              "      <td>4/03/2018</td>\n",
              "      <td>DCM</td>\n",
              "      <td>4/24/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Image Data ID  Subject  Group Sex  ...      Type    Acq Date Format Downloaded\n",
              "0        1130198    75422      0   M  ...  Original  11/13/2018    DCM  5/07/2019\n",
              "2        1130191    75414      0   F  ...  Original  12/13/2018    DCM  4/24/2019\n",
              "3        1125041    74375      0   F  ...  Original   9/06/2018    DCM  4/24/2019\n",
              "4        1003469    72138      0   F  ...  Original   2/19/2018    DCM  4/24/2019\n",
              "5        1119693    71935      1   M  ...  Original   4/03/2018    DCM  4/24/2019\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw2oOqvEtHGS",
        "colab_type": "text"
      },
      "source": [
        "**Load our file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcadXFs_tF1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for each set of training data in our pickle\n",
        "# load\n",
        "# process\n",
        "# train our model in this\n",
        "# 100 Test, 15% of 600 Validation, 85% of 600 Training. 700 Slices in total\n",
        "# Save our model, tweak and evaluate etc.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-ZJ9UM_tIeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets our y-values and converts to keras, one hot encoded outputs\n",
        "def get_y_values(total_slices_info):\n",
        "  \n",
        "  y_values = [s[2] for s in total_slices_info]\n",
        "\n",
        "  y_values = np.array(to_categorical(y_values, 2))\n",
        "  \n",
        "  return y_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGsctcuEUpKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_model():\n",
        "\n",
        "  # compile our model\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Convolution3D(filters=32, kernel_size=3, padding='same', input_shape=(200,200,160,1)))# ,kernel_regularizer=regularizers.l2(0.02))) # padding on first one only?\n",
        "  model.add(LeakyReLU(alpha=0.01)) # set to 0.01\n",
        "  ## model.add(BatchNormalization()) \n",
        "  model.add(MaxPooling3D(pool_size=3)) # pool_size=2\n",
        "  \n",
        "  model.add(Convolution3D(filters=64, kernel_size=3)) #,kernel_regularizer=regularizers.l2(0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.01)) \n",
        "  ## model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=4))\n",
        "\n",
        "  model.add(Convolution3D(filters=128, kernel_size=3))# ,kernel_regularizer=regularizers.l2(0.02)))\n",
        "  model.add(LeakyReLU())\n",
        "  ## model.add(BatchNormalization())\n",
        "  model.add(MaxPooling3D(pool_size=4))\n",
        "\n",
        "  model.add(Flatten())\n",
        "   \n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  ## model.add(Dropout(0.35)) # add dropout to prevent overfitting\n",
        "\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy',metrics = ['accuracy']) # metrics=['categorical_accuracy']\n",
        "\n",
        "  # experiment with literally everything?... Random Search with optimisers\n",
        "            \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cW8qDz2FUJ5",
        "colab_type": "code",
        "outputId": "e4454bb5-4cc2-4d5f-e872-f57b8495892f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_61 (Conv3D)           (None, 200, 200, 160, 32) 896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)   (None, 200, 200, 160, 32) 0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_61 (MaxPooling (None, 66, 66, 53, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_62 (Conv3D)           (None, 64, 64, 51, 64)    55360     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_77 (LeakyReLU)   (None, 64, 64, 51, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_62 (MaxPooling (None, 16, 16, 12, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_63 (Conv3D)           (None, 14, 14, 10, 128)   221312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_78 (LeakyReLU)   (None, 14, 14, 10, 128)   0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_63 (MaxPooling (None, 3, 3, 2, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_79 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 1,458,754\n",
            "Trainable params: 1,458,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSkzMMGtwcbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(hist):\n",
        "  ''' Function to get our metrics from history and score as inputs'''\n",
        "\n",
        "  # actually obtain our metrics\n",
        "  val_loss = hist.history['val_loss'][0]\n",
        "  val_acc = hist.history['val_acc'][0]\n",
        "  train_loss = hist.history['loss'][0]\n",
        "  train_acc = hist.history['acc'][0]\n",
        "\n",
        "  # put everything into one array\n",
        "  return [val_loss, val_acc, train_loss, train_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0oIxZjLtLBr",
        "colab_type": "code",
        "outputId": "214e1739-03b5-41d0-eef9-d656dc22cefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2705
        }
      },
      "source": [
        "# train on all, perhaps have different conditions for when we reach our last one\n",
        "total_slices_train = os.listdir('stored_batches')[:6] # 600 exemplars, then test on 100\n",
        "\n",
        "# Initialise Model!\n",
        "model = initialise_model()\n",
        "\n",
        "# Load Model (If not this, then must initialise)\n",
        "# model = load_model('my_model_1.h5')\n",
        "\n",
        "num_loopz = 1 # number of repeats we want\n",
        "num_iterations = 10 # number of times we want to loop it\n",
        "\n",
        "for kk in range(num_loopz):\n",
        "  \n",
        "  file_name = ('12_v'+str(kk)) ###filename, what we are changing\n",
        "  for iteration in range(num_iterations):\n",
        "\n",
        "    # load and fit our model for our instances\n",
        "    for tsf in total_slices_train:\n",
        "      pkl_path = 'stored_batches/'+tsf\n",
        "\n",
        "      # load pickle file\n",
        "      with open(pkl_path, 'rb') as f:\n",
        "        total_slices, total_slices_info = pickle.load(f)\n",
        "\n",
        "      # convert to numpy array\n",
        "      total_slices = np.array(total_slices)\n",
        "      # process y-values\n",
        "      y_values = get_y_values(total_slices_info)\n",
        "\n",
        "      # Run our model with validation of 15%\n",
        "      hist = model.fit(x=total_slices, y=y_values, batch_size=1, epochs=1, verbose=1, validation_split=0.15, shuffle=True)\n",
        "      # get metrics\n",
        "      metrics = get_metrics(hist)\n",
        "\n",
        "      # write to csv (want to append instead of overwrite)\n",
        "      with open('stored_metrics_v2/metrics'+file_name+'.csv', 'a') as csvFile:\n",
        "          writer = csv.writer(csvFile)\n",
        "          writer.writerow(metrics)\n",
        "      csvFile.close()\n",
        "\n",
        "      print (\"Iteration: %d, batch %s\" % (iteration, tsf[-5]))\n",
        "\n",
        "  # save the model as reference, incase we need the brain heatmap\n",
        "  model.save('stored_models_v2/model'+file_name+'.h5', overwrite=True)  # saves as a hd5 file"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 33s 354ms/step - loss: 5.9522 - acc: 0.6129 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 0, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 344ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 0, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 342ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 0, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 343ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 0, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 344ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 1, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 344ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 1, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 1, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 342ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 1, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 344ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 2, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 344ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 2, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 2, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 343ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 2, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 345ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 3, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 344ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 3, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 3, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 342ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 3, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 344ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 4, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 344ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 4, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 346ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 4, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 341ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 4, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 344ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 5, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 343ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 5, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 5, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 342ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 5, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 343ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 6, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 343ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 6, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 343ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 6, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 341ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 6, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 343ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 7, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 29s 343ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 7, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 7, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 340ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 7, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 343ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 8, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 30s 343ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 8, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 343ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 8, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 341ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 8, batch 3\n",
            "Train on 93 samples, validate on 17 samples\n",
            "Epoch 1/1\n",
            "93/93 [==============================] - 32s 343ms/step - loss: 5.5460 - acc: 0.6559 - val_loss: 3.7925 - val_acc: 0.7647\n",
            "Iteration: 9, batch 0\n",
            "Train on 86 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "86/86 [==============================] - 29s 343ms/step - loss: 8.2465 - acc: 0.4884 - val_loss: 2.0148 - val_acc: 0.8750\n",
            "Iteration: 9, batch 1\n",
            "Train on 85 samples, validate on 15 samples\n",
            "Epoch 1/1\n",
            "85/85 [==============================] - 29s 344ms/step - loss: 6.4472 - acc: 0.6000 - val_loss: 4.2982 - val_acc: 0.7333\n",
            "Iteration: 9, batch 2\n",
            "Train on 90 samples, validate on 16 samples\n",
            "Epoch 1/1\n",
            "90/90 [==============================] - 31s 341ms/step - loss: 4.2982 - acc: 0.7333 - val_loss: 4.0295 - val_acc: 0.7500\n",
            "Iteration: 9, batch 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqhWrgku96SD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}